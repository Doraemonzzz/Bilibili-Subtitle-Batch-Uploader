1
00:01:17,340 --> 00:01:22,110
Pancho's comfort sciences engineering and that's our project is the father of

2
00:01:22,110 --> 00:01:31,010
and that's our project is the father of I scale it allows realization

3
00:01:31,010 --> 00:01:31,020


4
00:01:31,020 --> 00:01:47,480
what with this content and Babylon

5
00:01:47,480 --> 00:01:47,490


6
00:01:47,490 --> 00:01:54,600
our oxygens are collocated physically

7
00:01:54,600 --> 00:01:54,610


8
00:01:54,610 --> 00:02:04,390
and they were happy

9
00:02:04,390 --> 00:02:04,400


10
00:02:04,400 --> 00:02:17,209
it's mostly we go to the sender and Senate in 2014

11
00:02:17,209 --> 00:02:21,260
we go to the sender and Senate in 2014 and we go stock unit director for my

12
00:02:21,260 --> 00:02:25,910
and we go stock unit director for my because many different still salvation

13
00:02:25,910 --> 00:02:31,310
because many different still salvation so only one feel alive today and so

14
00:02:31,310 --> 00:02:33,890
so only one feel alive today and so there is no countries or implementation

15
00:02:33,890 --> 00:02:37,220
there is no countries or implementation in 2014 but a remarkable achievements

16
00:02:37,220 --> 00:02:38,960
in 2014 but a remarkable achievements can be provided in international

17
00:02:38,960 --> 00:02:41,870
can be provided in international measures where there are enormous

18
00:02:41,870 --> 00:02:46,010
measures where there are enormous efforts both from industry and

19
00:02:46,010 --> 00:02:49,280
efforts both from industry and they invested to this field so a lot of

20
00:02:49,280 --> 00:02:50,960
they invested to this field so a lot of learnings and else you're gonna answer

21
00:02:50,960 --> 00:02:53,990
learnings and else you're gonna answer on experience

22
00:02:53,990 --> 00:02:59,330
on experience oh really

23
00:02:59,330 --> 00:03:02,310
oh really so my whole thing to follow him he'll

24
00:03:02,310 --> 00:03:13,150
so my whole thing to follow him he'll understand

25
00:03:13,150 --> 00:03:13,160


26
00:03:13,160 --> 00:03:30,470
with

27
00:03:30,470 --> 00:03:30,480


28
00:03:30,480 --> 00:03:35,830
[Music]

29
00:03:35,830 --> 00:03:35,840


30
00:03:35,840 --> 00:03:56,780
they also procedure

31
00:03:56,780 --> 00:03:56,790


32
00:03:56,790 --> 00:04:06,480
the resident incentive

33
00:04:06,480 --> 00:04:06,490


34
00:04:06,490 --> 00:04:11,830
adventures it's actually been trying to make some

35
00:04:11,830 --> 00:04:16,349
it's actually been trying to make some sort of internship

36
00:04:16,349 --> 00:04:16,359


37
00:04:16,359 --> 00:04:22,050
just emoticon sentences can be described in this block and these pauses between

38
00:04:22,050 --> 00:04:28,650
in this block and these pauses between world

39
00:04:28,650 --> 00:04:28,660


40
00:04:28,660 --> 00:04:35,710
[Music]

41
00:04:35,710 --> 00:04:35,720


42
00:04:35,720 --> 00:04:38,930
[Music]

43
00:04:38,930 --> 00:04:38,940


44
00:04:38,940 --> 00:04:45,020
and

45
00:04:45,020 --> 00:04:45,030


46
00:04:45,030 --> 00:04:56,090
the son of two halibut you only knew they wanted to

47
00:04:56,090 --> 00:04:56,100


48
00:04:56,100 --> 00:04:59,429
Oh

49
00:04:59,429 --> 00:04:59,439


50
00:04:59,439 --> 00:05:07,190
there's a high single formula up here

51
00:05:07,190 --> 00:05:07,200


52
00:05:07,200 --> 00:06:13,690
[Music]

53
00:06:13,690 --> 00:06:13,700


54
00:06:13,700 --> 00:06:21,119
lost function

55
00:06:21,119 --> 00:06:21,129


56
00:06:21,129 --> 00:06:30,920
and finally

57
00:06:30,920 --> 00:06:30,930


58
00:06:30,930 --> 00:06:41,200
[Music]

59
00:06:41,200 --> 00:06:41,210


60
00:06:41,210 --> 00:06:44,410
and

61
00:06:44,410 --> 00:06:44,420


62
00:06:44,420 --> 00:06:58,830
there are some occasions

63
00:06:58,830 --> 00:06:58,840


64
00:06:58,840 --> 00:07:01,989
[Music]

65
00:07:01,989 --> 00:07:01,999


66
00:07:01,999 --> 00:07:08,839
in today's top five we were too complicated model simple very hot and we

67
00:07:08,839 --> 00:07:14,050
complicated model simple very hot and we can

68
00:07:14,050 --> 00:07:14,060


69
00:07:14,060 --> 00:07:18,040
[Music]

70
00:07:18,040 --> 00:07:18,050


71
00:07:18,050 --> 00:07:32,580
the intuitive is equal to is still simple

72
00:07:32,580 --> 00:07:32,590


73
00:07:32,590 --> 00:07:36,340
too much

74
00:07:36,340 --> 00:07:36,350


75
00:07:36,350 --> 00:07:38,989
no or

76
00:07:38,989 --> 00:07:38,999


77
00:07:38,999 --> 00:07:45,570
and using the form of my

78
00:07:45,570 --> 00:07:45,580


79
00:07:45,580 --> 00:07:48,870
[Music]

80
00:07:48,870 --> 00:07:48,880


81
00:07:48,880 --> 00:07:58,879
one apparently

82
00:07:58,879 --> 00:07:58,889


83
00:07:58,889 --> 00:08:13,420
and

84
00:08:13,420 --> 00:08:13,430


85
00:08:13,430 --> 00:08:18,350
[Music] they'll want to ignore

86
00:08:18,350 --> 00:08:21,439
they'll want to ignore oh please for geography

87
00:08:21,439 --> 00:08:32,790
oh please for geography Oh

88
00:08:32,790 --> 00:08:32,800


89
00:08:32,800 --> 00:08:41,810
electric never wear em because they like to come by

90
00:08:41,810 --> 00:08:41,820


91
00:08:41,820 --> 00:08:52,380
if you'll edit my size

92
00:08:52,380 --> 00:08:52,390


93
00:08:52,390 --> 00:09:01,490
in fact a buffalo in film Fantasia extra

94
00:09:01,490 --> 00:09:01,500


95
00:09:01,500 --> 00:09:11,860
so with all those

96
00:09:11,860 --> 00:09:11,870


97
00:09:11,870 --> 00:09:23,040
and four different color

98
00:09:23,040 --> 00:09:23,050


99
00:09:23,050 --> 00:09:31,190
but if there's no only feel like a people like Bill told the dream home

100
00:09:31,190 --> 00:09:55,200
people like Bill told the dream home life's real castle

101
00:09:55,200 --> 00:09:55,210


102
00:09:55,210 --> 00:10:06,699
his life can be beautiful maybe for other guests

103
00:10:06,699 --> 00:10:06,709


104
00:10:06,709 --> 00:10:24,480
it's been exciting to interact with other common sense

105
00:10:24,480 --> 00:10:24,490


106
00:10:24,490 --> 00:10:29,080
everything

107
00:10:29,080 --> 00:10:29,090


108
00:10:29,090 --> 00:10:38,100
are you all common

109
00:10:38,100 --> 00:10:38,110


110
00:10:38,110 --> 00:10:46,680
because when the bill was right there

111
00:10:46,680 --> 00:10:46,690


112
00:10:46,690 --> 00:10:52,420
and eat natural color of waffle work all over the stage that

113
00:10:52,420 --> 00:10:53,960
over the stage that [Music]

114
00:10:53,960 --> 00:10:58,290
[Music] unlike most all of it

115
00:10:58,290 --> 00:11:00,510
unlike most all of it [Music]

116
00:11:00,510 --> 00:11:05,260
[Music] Oh

117
00:11:05,260 --> 00:11:05,270


118
00:11:05,270 --> 00:11:10,200
with ocean alignment

119
00:11:10,200 --> 00:11:10,210


120
00:11:10,210 --> 00:11:26,160
hi

121
00:11:26,160 --> 00:11:26,170


122
00:11:26,170 --> 00:11:45,690
Oh

123
00:11:45,690 --> 00:11:45,700


124
00:11:45,700 --> 00:11:52,329
in my face this is a crazy scientist or teacher telling without question so much

125
00:11:52,329 --> 00:11:56,019
teacher telling without question so much all those tickets are a year of conquer

126
00:11:56,019 --> 00:12:00,009
all those tickets are a year of conquer Europe this fortune different features

127
00:12:00,009 --> 00:12:02,860
Europe this fortune different features and opportunity of all penalties amazing

128
00:12:02,860 --> 00:12:04,600
and opportunity of all penalties amazing creatures

129
00:12:04,600 --> 00:12:06,490
creatures all the clothing falling in touch with

130
00:12:06,490 --> 00:12:09,930
all the clothing falling in touch with both Amy and

131
00:12:09,930 --> 00:12:09,940


132
00:12:09,940 --> 00:12:14,830
and those kids are along on 25,000 college of dentistry there

133
00:12:14,830 --> 00:12:18,610
college of dentistry there that's slightly on faceted and the very

134
00:12:18,610 --> 00:12:21,250
that's slightly on faceted and the very people that are provided by this box

135
00:12:21,250 --> 00:12:23,740
people that are provided by this box it's also a binary package equivalent

136
00:12:23,740 --> 00:12:27,850
it's also a binary package equivalent test only like Oh 100 samples and the

137
00:12:27,850 --> 00:12:35,880
test only like Oh 100 samples and the relative truth of 1810 was 14

138
00:12:35,880 --> 00:12:35,890


139
00:12:35,890 --> 00:12:45,000
and the yeah stance against it and the incidental constants of 619

140
00:12:45,000 --> 00:12:47,639
and the incidental constants of 619 position we completed the first one you

141
00:12:47,639 --> 00:12:56,660
position we completed the first one you have my sin

142
00:12:56,660 --> 00:12:56,670


143
00:12:56,670 --> 00:13:01,760
Oh

144
00:13:01,760 --> 00:13:01,770


145
00:13:01,770 --> 00:13:06,980
over people

146
00:13:06,980 --> 00:13:06,990


147
00:13:06,990 --> 00:13:18,350
without fulfilling to image size and

148
00:13:18,350 --> 00:13:18,360


149
00:13:18,360 --> 00:13:22,750
we kept our accessories with the iterator and

150
00:13:22,750 --> 00:13:25,920
iterator and it's the most function value and this is

151
00:13:25,920 --> 00:13:38,590
it's the most function value and this is only one religion and

152
00:13:38,590 --> 00:13:38,600


153
00:13:38,600 --> 00:13:48,850
and

154
00:13:48,850 --> 00:13:48,860


155
00:13:48,860 --> 00:13:59,080
so the accumulation and spinning and

156
00:13:59,080 --> 00:13:59,090


157
00:13:59,090 --> 00:14:08,220
with all the gold

158
00:14:08,220 --> 00:14:08,230


159
00:14:08,230 --> 00:14:12,900
[Music]

160
00:14:12,900 --> 00:14:12,910


161
00:14:12,910 --> 00:14:16,320
here

162
00:14:16,320 --> 00:14:16,330


163
00:14:16,330 --> 00:14:32,280
and this Apple

164
00:14:32,280 --> 00:14:32,290


165
00:14:32,290 --> 00:14:39,300
[Music]

166
00:14:39,300 --> 00:14:39,310


167
00:14:39,310 --> 00:14:56,050
[Music]

168
00:14:56,050 --> 00:14:56,060


169
00:14:56,060 --> 00:15:05,700
we have fun disabilities that's disgusting

170
00:15:05,700 --> 00:15:05,710


171
00:15:05,710 --> 00:15:19,460
for example

172
00:15:19,460 --> 00:15:19,470


173
00:15:19,470 --> 00:15:22,850
[Music]

174
00:15:22,850 --> 00:15:22,860


175
00:15:22,860 --> 00:15:28,980
we love

176
00:15:28,980 --> 00:15:28,990


177
00:15:28,990 --> 00:15:34,990
the opinion of my knowledge through your basic thinking

178
00:15:34,990 --> 00:15:35,000


179
00:15:35,000 --> 00:15:50,040
yeah

180
00:15:50,040 --> 00:15:50,050


181
00:15:50,050 --> 00:15:53,620
didn't see

182
00:15:53,620 --> 00:15:53,630


183
00:15:53,630 --> 00:16:01,530
we can see the times of my children go through

184
00:16:01,530 --> 00:16:01,540


185
00:16:01,540 --> 00:16:04,040
1:44 Oh

186
00:16:04,040 --> 00:16:09,210
Oh to

187
00:16:09,210 --> 00:16:09,220


188
00:16:09,220 --> 00:16:13,650
and if you like a little like you're investigating their family huh because

189
00:16:13,650 --> 00:16:17,970
investigating their family huh because every

190
00:16:17,970 --> 00:16:17,980


191
00:16:17,980 --> 00:16:22,760
and yeah feels like

192
00:16:22,760 --> 00:16:22,770


193
00:16:22,770 --> 00:16:30,990
[Music]

194
00:16:30,990 --> 00:16:31,000


195
00:16:31,000 --> 00:16:37,100
listen to these more elaboration

196
00:16:37,100 --> 00:16:37,110


197
00:16:37,110 --> 00:16:46,930
think about choice agent standard within the assignment over the freaking place

198
00:16:46,930 --> 00:16:46,940


199
00:16:46,940 --> 00:17:07,870
exactly we learn more [Music]

200
00:17:07,870 --> 00:17:07,880


201
00:17:07,880 --> 00:17:19,610
the solution is smaller and

202
00:17:19,610 --> 00:17:19,620


203
00:17:19,620 --> 00:17:26,900
is the exact problem

204
00:17:26,900 --> 00:17:26,910


205
00:17:26,910 --> 00:17:32,580
[Music] a lot of artists

206
00:17:32,580 --> 00:17:32,590


207
00:17:32,590 --> 00:17:46,600
beautiful or

208
00:17:46,600 --> 00:17:46,610


209
00:17:46,610 --> 00:18:01,850
Oh

210
00:18:01,850 --> 00:18:01,860


211
00:18:01,860 --> 00:18:10,490
almost

212
00:18:10,490 --> 00:18:10,500


213
00:18:10,500 --> 00:18:39,920
[Applause]

214
00:18:39,920 --> 00:18:39,930


215
00:18:39,930 --> 00:18:46,610
Oh agency yeah I suppose me feel fancy because you're ever

216
00:18:46,610 --> 00:18:50,770
because you're ever the beginning stage of commotion on a

217
00:18:50,770 --> 00:18:54,230
the beginning stage of commotion on a real I resisted by the leaders for

218
00:18:54,230 --> 00:18:57,920
real I resisted by the leaders for passive occasions like some conflicts

219
00:18:57,920 --> 00:19:00,410
passive occasions like some conflicts attracted each other today my image mesh

220
00:19:00,410 --> 00:19:04,160
attracted each other today my image mesh also attracted by

221
00:19:04,160 --> 00:19:04,170


222
00:19:04,170 --> 00:19:11,390
in my example to classify a trip there are typically a lot of South America

223
00:19:11,390 --> 00:19:13,520
are typically a lot of South America have different features of action and

224
00:19:13,520 --> 00:19:15,980
have different features of action and the tendencies to talk about whether

225
00:19:15,980 --> 00:21:16,500
the tendencies to talk about whether this is good or not

226
00:21:16,500 --> 00:21:16,510


227
00:21:16,510 --> 00:21:25,020
all right hello everyone says I'm Nick Geneva and I want to presenting on

228
00:21:25,020 --> 00:21:28,930
Geneva and I want to presenting on comprising graphical models with my

229
00:21:28,930 --> 00:21:31,060
comprising graphical models with my original title was neural networks but

230
00:21:31,060 --> 00:21:32,800
original title was neural networks but we're going to generalize it to machine

231
00:21:32,800 --> 00:21:38,910
we're going to generalize it to machine learning for structural representation

232
00:21:38,910 --> 00:21:38,920


233
00:21:38,920 --> 00:21:45,070
all right so originally this I guess this presentation is kind of a story of

234
00:21:45,070 --> 00:21:48,070
this presentation is kind of a story of how I was originally going to focus on

235
00:21:48,070 --> 00:21:50,140
how I was originally going to focus on this paper up here this 2016 there's

236
00:21:50,140 --> 00:21:52,240
this paper up here this 2016 there's paper then basically got lost in one of

237
00:21:52,240 --> 00:21:54,940
paper then basically got lost in one of the references which actually ended up

238
00:21:54,940 --> 00:21:56,560
the references which actually ended up proposing that did I found kind of

239
00:21:56,560 --> 00:21:57,910
proposing that did I found kind of interesting and hard to understand

240
00:21:57,910 --> 00:22:00,940
interesting and hard to understand because it motivated a lot of what kind

241
00:22:00,940 --> 00:22:02,350
because it motivated a lot of what kind of ideas that are happening up in this

242
00:22:02,350 --> 00:22:07,270
of ideas that are happening up in this one so yeah all right starting off when

243
00:22:07,270 --> 00:22:09,040
one so yeah all right starting off when we're building a model where the model

244
00:22:09,040 --> 00:22:10,540
we're building a model where the model to represent data a model to generate

245
00:22:10,540 --> 00:22:13,150
to represent data a model to generate day of whatever you want generally you

246
00:22:13,150 --> 00:22:14,740
day of whatever you want generally you may have two different goals you may

247
00:22:14,740 --> 00:22:17,350
may have two different goals you may want to have a flexible approach so

248
00:22:17,350 --> 00:22:19,260
want to have a flexible approach so let's say you have some non parametric

249
00:22:19,260 --> 00:22:21,520
let's say you have some non parametric representation of some complex high

250
00:22:21,520 --> 00:22:23,590
representation of some complex high dimensional data or you may want to

251
00:22:23,590 --> 00:22:25,360
dimensional data or you may want to structure our approach and this may be

252
00:22:25,360 --> 00:22:27,520
structure our approach and this may be to find a structure representation of

253
00:22:27,520 --> 00:22:29,110
to find a structure representation of data that can then be generalized

254
00:22:29,110 --> 00:22:32,080
data that can then be generalized perhaps so in terms of platform model

255
00:22:32,080 --> 00:22:33,670
perhaps so in terms of platform model that's when I say flexible models I mean

256
00:22:33,670 --> 00:22:35,410
that's when I say flexible models I mean like I said I'm talking about these

257
00:22:35,410 --> 00:22:37,210
like I said I'm talking about these black box methods so machine learning

258
00:22:37,210 --> 00:22:38,770
black box methods so machine learning you're looking for buzzword like neural

259
00:22:38,770 --> 00:22:40,530
you're looking for buzzword like neural networks convolutional Network

260
00:22:40,530 --> 00:22:44,080
networks convolutional Network autoencoders Gans support vector

261
00:22:44,080 --> 00:22:47,920
autoencoders Gans support vector machines aa website structure model

262
00:22:47,920 --> 00:22:50,740
machines aa website structure model course PGM class so it's all about

263
00:22:50,740 --> 00:22:52,300
course PGM class so it's all about probabilistic graphical models stuff

264
00:22:52,300 --> 00:22:54,730
probabilistic graphical models stuff where we actually have values that means

265
00:22:54,730 --> 00:22:56,260
where we actually have values that means something rather than just random

266
00:22:56,260 --> 00:22:58,160
something rather than just random weights and stuff that they're just kind

267
00:22:58,160 --> 00:23:02,960
weights and stuff that they're just kind matching up a function so the question

268
00:23:02,960 --> 00:23:05,990
matching up a function so the question is what do we want what happens if we

269
00:23:05,990 --> 00:23:08,030
is what do we want what happens if we want both A's flexible yet also

270
00:23:08,030 --> 00:23:10,190
want both A's flexible yet also structured approach so this means

271
00:23:10,190 --> 00:23:12,860
structured approach so this means essentially like if I want like let's

272
00:23:12,860 --> 00:23:14,120
essentially like if I want like let's say like I have some high dimensional

273
00:23:14,120 --> 00:23:16,130
say like I have some high dimensional data that I need to map or something

274
00:23:16,130 --> 00:23:17,870
data that I need to map or something with some flexible method but I also

275
00:23:17,870 --> 00:23:20,090
with some flexible method but I also want some sort of values I can then

276
00:23:20,090 --> 00:23:21,350
want some sort of values I can then relate to maybe physical dynamics

277
00:23:21,350 --> 00:23:23,420
relate to maybe physical dynamics there's something about the actual prom

278
00:23:23,420 --> 00:23:26,540
there's something about the actual prom because of that price so just the

279
00:23:26,540 --> 00:23:28,610
because of that price so just the problem to kind of motivate this as an

280
00:23:28,610 --> 00:23:31,640
problem to kind of motivate this as an example so let's suppose that we have

281
00:23:31,640 --> 00:23:33,860
example so let's suppose that we have this mountain that's inside of this

282
00:23:33,860 --> 00:23:36,110
this mountain that's inside of this spherical container moving around and

283
00:23:36,110 --> 00:23:38,240
spherical container moving around and our input data is going to be some

284
00:23:38,240 --> 00:23:40,160
our input data is going to be some height so basically our input is this

285
00:23:40,160 --> 00:23:43,610
height so basically our input is this image and from that when our output

286
00:23:43,610 --> 00:23:46,130
image and from that when our output debase be the interest in trouble

287
00:23:46,130 --> 00:23:47,930
debase be the interest in trouble structure of essentially the mouse's

288
00:23:47,930 --> 00:23:51,200
structure of essentially the mouse's actual behavior so these may be the

289
00:23:51,200 --> 00:23:54,800
actual behavior so these may be the current of dance edits in or movement no

290
00:23:54,800 --> 00:23:56,690
current of dance edits in or movement no I'm not so as they clearly have this

291
00:23:56,690 --> 00:23:59,210
I'm not so as they clearly have this high dimensional image coming in so we

292
00:23:59,210 --> 00:24:00,830
high dimensional image coming in so we kind of want a flexible model to learn

293
00:24:00,830 --> 00:24:03,800
kind of want a flexible model to learn that but then our output we want to have

294
00:24:03,800 --> 00:24:05,750
that but then our output we want to have some sort of probabilistic inference on

295
00:24:05,750 --> 00:24:07,700
some sort of probabilistic inference on what's probably of the mouse's actions

296
00:24:07,700 --> 00:24:09,470
what's probably of the mouse's actions sitting up on its hind legs or if it's

297
00:24:09,470 --> 00:24:11,000
sitting up on its hind legs or if it's scratching its ear and stuff like that

298
00:24:11,000 --> 00:24:14,090
scratching its ear and stuff like that those are very structured output on so

299
00:24:14,090 --> 00:24:16,940
those are very structured output on so basically one kind of a combination and

300
00:24:16,940 --> 00:24:19,730
basically one kind of a combination and that's kind of a whole main idea of this

301
00:24:19,730 --> 00:24:22,340
that's kind of a whole main idea of this paper and kind of cold I think growing

302
00:24:22,340 --> 00:24:24,650
paper and kind of cold I think growing field is essentially can we combine

303
00:24:24,650 --> 00:24:27,110
field is essentially can we combine these to build stronger modeling tools

304
00:24:27,110 --> 00:24:29,240
these to build stronger modeling tools that can really get the best of both

305
00:24:29,240 --> 00:24:33,770
that can really get the best of both worlds is essentially it so for this we

306
00:24:33,770 --> 00:24:35,690
worlds is essentially it so for this we won't be looking at might we be looking

307
00:24:35,690 --> 00:24:36,940
won't be looking at might we be looking at something a little bit more dull

308
00:24:36,940 --> 00:24:40,970
at something a little bit more dull which is a natural clustering frog so

309
00:24:40,970 --> 00:24:43,430
which is a natural clustering frog so these are just an example it's a really

310
00:24:43,430 --> 00:24:45,050
these are just an example it's a really great toy problem on one we'll be

311
00:24:45,050 --> 00:24:47,870
great toy problem on one we'll be looking at is spiral problems maybe in a

312
00:24:47,870 --> 00:24:51,320
looking at is spiral problems maybe in a spiral and the reason for that is one

313
00:24:51,320 --> 00:24:53,000
spiral and the reason for that is one it's great they're both within two

314
00:24:53,000 --> 00:24:56,750
it's great they're both within two also it's these clusters are very

315
00:24:56,750 --> 00:24:58,790
also it's these clusters are very discrete clusters that we want to you

316
00:24:58,790 --> 00:25:00,380
discrete clusters that we want to you know categorize though each one of these

317
00:25:00,380 --> 00:25:01,730
know categorize though each one of these you want to just find is a different

318
00:25:01,730 --> 00:25:04,730
you want to just find is a different class however they are still there

319
00:25:04,730 --> 00:25:08,690
class however they are still there non-college so there and this is

320
00:25:08,690 --> 00:25:10,220
non-college so there and this is unsupervised learning problem so they

321
00:25:10,220 --> 00:25:13,010
unsupervised learning problem so they sleep you don't have any labels so

322
00:25:13,010 --> 00:25:15,530
sleep you don't have any labels so before we get into the actual paper room

323
00:25:15,530 --> 00:25:18,500
before we get into the actual paper room visit our old friend Gaussian mixture

324
00:25:18,500 --> 00:25:23,180
visit our old friend Gaussian mixture models to go over so this is the

325
00:25:23,180 --> 00:25:26,060
models to go over so this is the non-bayesian formulation I find this and

326
00:25:26,060 --> 00:25:29,360
non-bayesian formulation I find this and Bishop just to review if you're like me

327
00:25:29,360 --> 00:25:31,670
Bishop just to review if you're like me and have a memory of a goldfish we tend

328
00:25:31,670 --> 00:25:35,300
and have a memory of a goldfish we tend to forget this stuff however the idea of

329
00:25:35,300 --> 00:25:37,810
to forget this stuff however the idea of Ecology mixture model is that we have a

330
00:25:37,810 --> 00:25:40,310
Ecology mixture model is that we have a latent variable Z which is binary

331
00:25:40,310 --> 00:25:43,430
latent variable Z which is binary variable which represents which class of

332
00:25:43,430 --> 00:25:45,140
variable which represents which class of which cause mixture we're pulling from

333
00:25:45,140 --> 00:25:47,300
which cause mixture we're pulling from and we have for each causing make sure

334
00:25:47,300 --> 00:25:49,130
and we have for each causing make sure we have respective mean and it also

335
00:25:49,130 --> 00:25:52,160
we have respective mean and it also precision we can increase complexity a

336
00:25:52,160 --> 00:25:53,750
precision we can increase complexity a little bit and make it kind of more

337
00:25:53,750 --> 00:25:55,640
little bit and make it kind of more Bayesian correlation by introducing some

338
00:25:55,640 --> 00:25:57,230
Bayesian correlation by introducing some priors onto our precision and all

339
00:25:57,230 --> 00:26:00,140
priors onto our precision and all certainty which is T nope here these are

340
00:26:00,140 --> 00:26:01,130
certainty which is T nope here these are our high performers that we're

341
00:26:01,130 --> 00:26:04,400
our high performers that we're introducing so this is just some

342
00:26:04,400 --> 00:26:06,800
introducing so this is just some reminders again if you want to know more

343
00:26:06,800 --> 00:26:12,080
reminders again if you want to know more much rating so just refresh your memory

344
00:26:12,080 --> 00:26:14,630
much rating so just refresh your memory so again our joint distribution it's

345
00:26:14,630 --> 00:26:18,130
so again our joint distribution it's going to be a the multiplication of

346
00:26:18,130 --> 00:26:20,840
going to be a the multiplication of essentially like including our priors or

347
00:26:20,840 --> 00:26:22,370
essentially like including our priors or not the likelihood takes the form of

348
00:26:22,370 --> 00:26:28,360
not the likelihood takes the form of course our cause in C is binary

349
00:26:28,360 --> 00:26:28,370


350
00:26:28,370 --> 00:26:32,630
yeah just mixing coefficients we're just gonna go kind of fast through this - I

351
00:26:32,630 --> 00:26:33,920
gonna go kind of fast through this - I have greens on afterwards

352
00:26:33,920 --> 00:26:36,200
have greens on afterwards yet only used a anything here alright so

353
00:26:36,200 --> 00:26:39,140
yet only used a anything here alright so just to refresh your memory for actually

354
00:26:39,140 --> 00:26:42,440
just to refresh your memory for actually on conducting for tuning this problem

355
00:26:42,440 --> 00:26:42,960
on conducting for tuning this problem are legit

356
00:26:42,960 --> 00:26:44,970
are legit in France and whatnot it's day like

357
00:26:44,970 --> 00:26:47,120
in France and whatnot it's day like someone to enum algorithms with this

358
00:26:47,120 --> 00:26:49,260
someone to enum algorithms with this Bayesian that they've yet to take kind

359
00:26:49,260 --> 00:26:50,909
Bayesian that they've yet to take kind of a variational approach which involves

360
00:26:50,909 --> 00:26:52,529
of a variational approach which involves maximizing lower bound we all know that

361
00:26:52,529 --> 00:26:55,169
maximizing lower bound we all know that and it's an alternative estimate we came

362
00:26:55,169 --> 00:26:57,419
and it's an alternative estimate we came Kong back and forth between essential

363
00:26:57,419 --> 00:26:59,250
Kong back and forth between essential updating responsibilities computing the

364
00:26:59,250 --> 00:27:01,490
updating responsibilities computing the relevant statistics related to those

365
00:27:01,490 --> 00:27:03,779
relevant statistics related to those responsibilities and also updating the

366
00:27:03,779 --> 00:27:05,730
responsibilities and also updating the hyper parameters space you have to jump

367
00:27:05,730 --> 00:27:07,260
hyper parameters space you have to jump back and forth between these two then

368
00:27:07,260 --> 00:27:10,169
back and forth between these two then converge the actual solution in

369
00:27:10,169 --> 00:27:12,090
converge the actual solution in particular mission we don't know now the

370
00:27:12,090 --> 00:27:14,159
particular mission we don't know now the problem with this problem of interest is

371
00:27:14,159 --> 00:27:17,250
problem with this problem of interest is basically we can't actually describe

372
00:27:17,250 --> 00:27:20,549
basically we can't actually describe these regions very efficiently and like

373
00:27:20,549 --> 00:27:22,890
these regions very efficiently and like a hint said that before we chose us prom

374
00:27:22,890 --> 00:27:24,750
a hint said that before we chose us prom specifically because these are indeed

375
00:27:24,750 --> 00:27:27,210
specifically because these are indeed there they don't really fit college so

376
00:27:27,210 --> 00:27:28,590
there they don't really fit college so essentially even though we're able to

377
00:27:28,590 --> 00:27:31,470
essentially even though we're able to fit the data through mixture we don't

378
00:27:31,470 --> 00:27:33,480
fit the data through mixture we don't really get the five discrete classes

379
00:27:33,480 --> 00:27:37,919
really get the five discrete classes that we actually originally wanted so

380
00:27:37,919 --> 00:27:38,909
that we actually originally wanted so let's look at something completely

381
00:27:38,909 --> 00:27:40,950
let's look at something completely different now let's say all right so

382
00:27:40,950 --> 00:27:43,140
different now let's say all right so this was the structured result so now

383
00:27:43,140 --> 00:27:46,620
this was the structured result so now let's go to entirely flexible is all an

384
00:27:46,620 --> 00:27:48,299
let's go to entirely flexible is all an idea here is that now instead of

385
00:27:48,299 --> 00:27:49,919
idea here is that now instead of essentially having you know this mixture

386
00:27:49,919 --> 00:27:51,270
essentially having you know this mixture guardians were going to basically be

387
00:27:51,270 --> 00:27:55,760
guardians were going to basically be using a narrow network since you predict

388
00:27:55,760 --> 00:28:01,620
using a narrow network since you predict to predict a mean and also a procedure

389
00:28:01,620 --> 00:28:01,630


390
00:28:01,630 --> 00:28:06,570
so this is now our likelihood and you see that our our actual mean and also

391
00:28:06,570 --> 00:28:09,210
see that our our actual mean and also our precision is now so wait biases of

392
00:28:09,210 --> 00:28:11,760
our precision is now so wait biases of the neural network now this is actually

393
00:28:11,760 --> 00:28:13,080
the neural network now this is actually a function that's basically being

394
00:28:13,080 --> 00:28:17,159
a function that's basically being produced here now of course the issue

395
00:28:17,159 --> 00:28:18,480
produced here now of course the issue with this is obvious

396
00:28:18,480 --> 00:28:21,029
with this is obvious we've lost all the sort of categorizing

397
00:28:21,029 --> 00:28:23,340
we've lost all the sort of categorizing is essentially it sure we can fit this

398
00:28:23,340 --> 00:28:25,770
is essentially it sure we can fit this very nicely we get very nice densities

399
00:28:25,770 --> 00:28:28,560
very nicely we get very nice densities where the actual data is but in the way

400
00:28:28,560 --> 00:28:30,180
where the actual data is but in the way of classifying the different actual

401
00:28:30,180 --> 00:28:33,289
of classifying the different actual clusters Bravo

402
00:28:33,289 --> 00:28:37,710
clusters Bravo alright so now about which papers to

403
00:28:37,710 --> 00:28:43,020
alright so now about which papers to this later paper back in 2012 and I'm

404
00:28:43,020 --> 00:28:45,299
this later paper back in 2012 and I'm talking about a method which is kind of

405
00:28:45,299 --> 00:28:47,520
talking about a method which is kind of general approach car rat the mixture

406
00:28:47,520 --> 00:28:51,630
general approach car rat the mixture models and the core idea is essentially

407
00:28:51,630 --> 00:28:54,390
models and the core idea is essentially in its name so we're going to take a

408
00:28:54,390 --> 00:28:56,760
in its name so we're going to take a mixture model so on the left and

409
00:28:56,760 --> 00:28:58,500
mixture model so on the left and basically wrap it and some sort of

410
00:28:58,500 --> 00:29:01,320
basically wrap it and some sort of function that transforms basically the

411
00:29:01,320 --> 00:29:02,970
function that transforms basically the the latent variable was returned

412
00:29:02,970 --> 00:29:05,010
the latent variable was returned determined for the mixture model into a

413
00:29:05,010 --> 00:29:07,169
determined for the mixture model into a non Gaussian and spacing on garden

414
00:29:07,169 --> 00:29:10,320
non Gaussian and spacing on garden animals essentially so the idea is that

415
00:29:10,320 --> 00:29:13,049
animals essentially so the idea is that instead of using them as the direct

416
00:29:13,049 --> 00:29:14,940
instead of using them as the direct output along map it through some sort of

417
00:29:14,940 --> 00:29:16,950
output along map it through some sort of function do they get a much more complex

418
00:29:16,950 --> 00:29:20,159
function do they get a much more complex output yet will also retain the

419
00:29:20,159 --> 00:29:24,520
output yet will also retain the structured representation

420
00:29:24,520 --> 00:29:24,530


421
00:29:24,530 --> 00:29:29,530
so here's what we're looking at on here so basically everything most of it has

422
00:29:29,530 --> 00:29:31,930
so basically everything most of it has remained the same I'll talk about this

423
00:29:31,930 --> 00:29:34,120
remained the same I'll talk about this and break down more detail but only if

424
00:29:34,120 --> 00:29:36,460
and break down more detail but only if now at this arbitrary function over here

425
00:29:36,460 --> 00:29:39,640
now at this arbitrary function over here and for this and the paper we're gonna

426
00:29:39,640 --> 00:29:41,500
and for this and the paper we're gonna be following it it's gonna be a Gaussian

427
00:29:41,500 --> 00:29:45,160
be following it it's gonna be a Gaussian process so Reagan is down on the left

428
00:29:45,160 --> 00:29:47,230
process so Reagan is down on the left this is actually precisely the Bayesian

429
00:29:47,230 --> 00:29:49,870
this is actually precisely the Bayesian on Gaussian mixture model that we had

430
00:29:49,870 --> 00:29:50,530
on Gaussian mixture model that we had before

431
00:29:50,530 --> 00:29:53,800
before exactly actually except the key

432
00:29:53,800 --> 00:29:55,570
exactly actually except the key difference is that now the outputs are

433
00:29:55,570 --> 00:29:57,130
difference is that now the outputs are not the actual observations they're not

434
00:29:57,130 --> 00:29:59,020
not the actual observations they're not a set of latent variables so these don't

435
00:29:59,020 --> 00:30:01,480
a set of latent variables so these don't actually have really true meaning

436
00:30:01,480 --> 00:30:02,890
actually have really true meaning they're not what we're seeing isn't it's

437
00:30:02,890 --> 00:30:05,410
they're not what we're seeing isn't it's not what our actual data I also want to

438
00:30:05,410 --> 00:30:08,050
not what our actual data I also want to note here that I personally I predefined

439
00:30:08,050 --> 00:30:12,010
note here that I personally I predefined a set of mixture components k however

440
00:30:12,010 --> 00:30:14,710
a set of mixture components k however you can indeed have a infinite set which

441
00:30:14,710 --> 00:30:16,690
you can indeed have a infinite set which is what they do in the paper through a

442
00:30:16,690 --> 00:30:18,670
is what they do in the paper through a Derrick wave process and give sampling

443
00:30:18,670 --> 00:30:23,169
Derrick wave process and give sampling of that

444
00:30:23,169 --> 00:30:23,179


445
00:30:23,179 --> 00:30:29,200
now on the right side basically this is now going to be our Gaussian process

446
00:30:29,200 --> 00:30:31,029
now going to be our Gaussian process here so basically it's going to be

447
00:30:31,029 --> 00:30:33,249
here so basically it's going to be mapping these latent variables here to a

448
00:30:33,249 --> 00:30:38,109
mapping these latent variables here to a non ecology manifold is the idea and I

449
00:30:38,109 --> 00:30:40,209
non ecology manifold is the idea and I stay here that this is a Gaussian

450
00:30:40,209 --> 00:30:43,749
stay here that this is a Gaussian process however on this is not actually

451
00:30:43,749 --> 00:30:46,659
process however on this is not actually inclusive so you can extend this idea to

452
00:30:46,659 --> 00:30:48,729
inclusive so you can extend this idea to others or a methods however you want

453
00:30:48,729 --> 00:30:49,869
others or a methods however you want actually one of the map how you wanted

454
00:30:49,869 --> 00:30:52,629
actually one of the map how you wanted to find this function um it's just what

455
00:30:52,629 --> 00:30:53,769
to find this function um it's just what we launched we're doing this particular

456
00:30:53,769 --> 00:30:57,190
we launched we're doing this particular paper um I do know here that they're

457
00:30:57,190 --> 00:30:59,680
paper um I do know here that they're basically so this right here we have

458
00:30:59,680 --> 00:31:02,200
basically so this right here we have inputs our latent variables on that the

459
00:31:02,200 --> 00:31:06,159
inputs our latent variables on that the in mm process and this is precisely a

460
00:31:06,159 --> 00:31:08,709
in mm process and this is precisely a Gaussian process lake imperative model

461
00:31:08,709 --> 00:31:11,709
Gaussian process lake imperative model in fact if we ignore this closed section

462
00:31:11,709 --> 00:31:14,529
in fact if we ignore this closed section out here and we just take these guys as

463
00:31:14,529 --> 00:31:18,330
out here and we just take these guys as basically fixed it's precisely a gauntly

464
00:31:18,330 --> 00:31:21,729
basically fixed it's precisely a gauntly so in essence you can view this as a

465
00:31:21,729 --> 00:31:26,769
so in essence you can view this as a more generalized version of GPL so if

466
00:31:26,769 --> 00:31:28,779
more generalized version of GPL so if you like me and you're not too well

467
00:31:28,779 --> 00:31:30,940
you like me and you're not too well first on gauzy and upon latent variable

468
00:31:30,940 --> 00:31:33,129
first on gauzy and upon latent variable models we're gonna go over it so the

469
00:31:33,129 --> 00:31:35,139
models we're gonna go over it so the idea the the key principle is that

470
00:31:35,139 --> 00:31:38,619
idea the the key principle is that instead of having circle represent your

471
00:31:38,619 --> 00:31:41,039
instead of having circle represent your likelihood through a German Gaussian and

472
00:31:41,039 --> 00:31:44,379
likelihood through a German Gaussian and your actual covariance matrix is going

473
00:31:44,379 --> 00:31:46,690
your actual covariance matrix is going to be defined through a kernel function

474
00:31:46,690 --> 00:31:49,239
to be defined through a kernel function down here now the key thing that makes

475
00:31:49,239 --> 00:31:51,279
down here now the key thing that makes this innovation very models is that this

476
00:31:51,279 --> 00:31:52,659
this innovation very models is that this kernel function is governed through

477
00:31:52,659 --> 00:31:54,820
kernel function is governed through latent variables so these variables are

478
00:31:54,820 --> 00:31:57,489
latent variables so these variables are related to your actually do get related

479
00:31:57,489 --> 00:31:59,999
related to your actually do get related but there's been leading variables

480
00:31:59,999 --> 00:32:05,289
but there's been leading variables basically it now of course we need to

481
00:32:05,289 --> 00:32:07,060
basically it now of course we need to optimize these latent variables so we

482
00:32:07,060 --> 00:32:08,710
optimize these latent variables so we needed to tune this model so you can get

483
00:32:08,710 --> 00:32:11,019
needed to tune this model so you can get desired result on that is done basically

484
00:32:11,019 --> 00:32:12,430
desired result on that is done basically if ingredient based operations

485
00:32:12,430 --> 00:32:16,690
if ingredient based operations optimization and yeah so I just quickly

486
00:32:16,690 --> 00:32:18,789
optimization and yeah so I just quickly list some gradients here you can see

487
00:32:18,789 --> 00:32:21,340
list some gradients here you can see that basically we want a cost function

488
00:32:21,340 --> 00:32:22,930
that basically we want a cost function or you know what I like over here like a

489
00:32:22,930 --> 00:32:24,220
or you know what I like over here like a note of observation is getting laid

490
00:32:24,220 --> 00:32:26,080
note of observation is getting laid variables and also the data is gonna

491
00:32:26,080 --> 00:32:29,940
variables and also the data is gonna represent our parameters and the GPM and

492
00:32:29,940 --> 00:32:32,259
represent our parameters and the GPM and essentially this using chain rule you

493
00:32:32,259 --> 00:32:34,029
essentially this using chain rule you can see that basically this combined

494
00:32:34,029 --> 00:32:36,220
can see that basically this combined with this will give you the derivative

495
00:32:36,220 --> 00:32:39,700
with this will give you the derivative of the of your cost function so your

496
00:32:39,700 --> 00:32:42,279
of the of your cost function so your wall of your likelihood with respect to

497
00:32:42,279 --> 00:32:48,520
wall of your likelihood with respect to your actual latent variable be consumed

498
00:32:48,520 --> 00:32:48,530


499
00:32:48,530 --> 00:32:52,760
alright so now moving into the actual paper now so we're gonna break this down

500
00:32:52,760 --> 00:32:54,920
paper now so we're gonna break this down and look into three parts of this actual

501
00:32:54,920 --> 00:32:57,230
and look into three parts of this actual of this rap mixture model I want to talk

502
00:32:57,230 --> 00:32:59,000
of this rap mixture model I want to talk about a generation this is a generative

503
00:32:59,000 --> 00:33:01,220
about a generation this is a generative model I don't talk about the training of

504
00:33:01,220 --> 00:33:03,230
model I don't talk about the training of it they don't talk about on briefly

505
00:33:03,230 --> 00:33:06,320
it they don't talk about on briefly prediction so it's thermal energy ppl

506
00:33:06,320 --> 00:33:07,310
prediction so it's thermal energy ppl Fiona song

507
00:33:07,310 --> 00:33:09,140
Fiona song essentially this is a generative model

508
00:33:09,140 --> 00:33:11,900
essentially this is a generative model so we can generate realizations from

509
00:33:11,900 --> 00:33:16,010
so we can generate realizations from this PGM basic generator instead of

510
00:33:16,010 --> 00:33:17,570
this PGM basic generator instead of latent variables and feed it through our

511
00:33:17,570 --> 00:33:20,020
latent variables and feed it through our Gaussian process on to non Gaussian

512
00:33:20,020 --> 00:33:25,490
Gaussian process on to non Gaussian manifolds idea so the idea is I surmise

513
00:33:25,490 --> 00:33:26,990
manifolds idea so the idea is I surmise it's in three steps though for each

514
00:33:26,990 --> 00:33:30,200
it's in three steps though for each component for volume to I remember going

515
00:33:30,200 --> 00:33:34,100
component for volume to I remember going back to our PGM here so the first step

516
00:33:34,100 --> 00:33:36,290
back to our PGM here so the first step is that we want to draw precision and it

517
00:33:36,290 --> 00:33:38,480
is that we want to draw precision and it also mean for each component right from

518
00:33:38,480 --> 00:33:41,420
also mean for each component right from our priors who's here

519
00:33:41,420 --> 00:33:43,400
our priors who's here somewhere as we always define though

520
00:33:43,400 --> 00:33:45,170
somewhere as we always define though there's a Wishart distribution and the

521
00:33:45,170 --> 00:33:47,590
there's a Wishart distribution and the social pathology and distribution here

522
00:33:47,590 --> 00:33:51,260
social pathology and distribution here we can then draw a function so this is

523
00:33:51,260 --> 00:33:53,390
we can then draw a function so this is basically if you have priors over your

524
00:33:53,390 --> 00:33:55,670
basically if you have priors over your actual parameters and you're calling

525
00:33:55,670 --> 00:33:58,250
actual parameters and you're calling processed me I should draw those

526
00:33:58,250 --> 00:34:00,980
processed me I should draw those parameters for your GP and that foreign

527
00:34:00,980 --> 00:34:02,600
parameters for your GP and that foreign lady here function that's a long map

528
00:34:02,600 --> 00:34:08,450
lady here function that's a long map user space and for for each desired

529
00:34:08,450 --> 00:34:11,090
user space and for for each desired observation that you want you can then

530
00:34:11,090 --> 00:34:14,510
observation that you want you can then draw a assignment so you just choose

531
00:34:14,510 --> 00:34:16,490
draw a assignment so you just choose which class you want and from that draw

532
00:34:16,490 --> 00:34:18,770
which class you want and from that draw a set of latent variables from your T

533
00:34:18,770 --> 00:34:21,500
a set of latent variables from your T GM's this is from their cause in and you

534
00:34:21,500 --> 00:34:23,600
GM's this is from their cause in and you seek those X's through your dogs and

535
00:34:23,600 --> 00:34:26,399
seek those X's through your dogs and possible generator why

536
00:34:26,399 --> 00:34:29,740
possible generator why now that's nice but we still have to

537
00:34:29,740 --> 00:34:31,270
now that's nice but we still have to worry about how do we actually get this

538
00:34:31,270 --> 00:34:35,409
worry about how do we actually get this model to mark how do we trade it so we

539
00:34:35,409 --> 00:34:37,540
model to mark how do we trade it so we have two systems on or over further than

540
00:34:37,540 --> 00:34:39,399
have two systems on or over further than two systems in this you have the PGM and

541
00:34:39,399 --> 00:34:41,860
two systems in this you have the PGM and also the GP and we're going to be

542
00:34:41,860 --> 00:34:44,830
also the GP and we're going to be optimizing those simultaneously and

543
00:34:44,830 --> 00:34:47,680
optimizing those simultaneously and basically what you want to maximize are

544
00:34:47,680 --> 00:34:49,240
basically what you want to maximize are displayed here like there's a billion

545
00:34:49,240 --> 00:34:51,659
displayed here like there's a billion variables so this is different

546
00:34:51,659 --> 00:34:54,430
variables so this is different originally than our GP procedure but

547
00:34:54,430 --> 00:34:56,260
originally than our GP procedure but however if we expand it out we see that

548
00:34:56,260 --> 00:34:59,530
however if we expand it out we see that actually his and this is a kind of core

549
00:34:59,530 --> 00:35:02,740
actually his and this is a kind of core idea here is that essentially we have we

550
00:35:02,740 --> 00:35:05,170
idea here is that essentially we have we break it apart yes you have a loop

551
00:35:05,170 --> 00:35:07,030
break it apart yes you have a loop electrically that was from our Gaussian

552
00:35:07,030 --> 00:35:09,340
electrically that was from our Gaussian process but then we have essentially

553
00:35:09,340 --> 00:35:11,620
process but then we have essentially this you can consider it like a prior is

554
00:35:11,620 --> 00:35:13,150
this you can consider it like a prior is basically it so we have this complex

555
00:35:13,150 --> 00:35:16,020
basically it so we have this complex prior that's getting tagged on to our

556
00:35:16,020 --> 00:35:18,910
prior that's getting tagged on to our loss function which is then going to

557
00:35:18,910 --> 00:35:21,070
loss function which is then going to improve its performance and this is

558
00:35:21,070 --> 00:35:24,100
improve its performance and this is basically this idea right here is the

559
00:35:24,100 --> 00:35:26,110
basically this idea right here is the same concept that is used actually also

560
00:35:26,110 --> 00:35:28,390
same concept that is used actually also in the 2000 sticking paper that I was

561
00:35:28,390 --> 00:35:31,300
in the 2000 sticking paper that I was supposed to really focus on but it's the

562
00:35:31,300 --> 00:35:32,980
supposed to really focus on but it's the same with the same idea essentially

563
00:35:32,980 --> 00:35:36,450
same with the same idea essentially they're tacking on a more complex more

564
00:35:36,450 --> 00:35:40,810
they're tacking on a more complex more descriptive prior on to their loss

565
00:35:40,810 --> 00:35:41,680
descriptive prior on to their loss function

566
00:35:41,680 --> 00:35:43,930
function which basically allows them to do a lot

567
00:35:43,930 --> 00:35:47,840
which basically allows them to do a lot who were stuff

568
00:35:47,840 --> 00:35:47,850


569
00:35:47,850 --> 00:35:53,680
yes so to kind of evolve this on in this

570
00:35:53,680 --> 00:35:56,319
so to kind of evolve this on in this paper they use a hybrid Monte Carlo or

571
00:35:56,319 --> 00:35:58,779
paper they use a hybrid Monte Carlo or Hamiltonian Monte Carlo debate please

572
00:35:58,779 --> 00:36:03,190
Hamiltonian Monte Carlo debate please sample I say kind of move because I need

573
00:36:03,190 --> 00:36:07,480
sample I say kind of move because I need coming from a background in terms of

574
00:36:07,480 --> 00:36:09,670
coming from a background in terms of research I think gradient but it's

575
00:36:09,670 --> 00:36:11,500
research I think gradient but it's sampling so one sample a set of

576
00:36:11,500 --> 00:36:15,339
sampling so one sample a set of particles from basically this posterior

577
00:36:15,339 --> 00:36:18,339
particles from basically this posterior here so the idea there is essentially

578
00:36:18,339 --> 00:36:20,289
here so the idea there is essentially we're going to have an additional set of

579
00:36:20,289 --> 00:36:24,760
we're going to have an additional set of latent variables if you recall HMC

580
00:36:24,760 --> 00:36:26,500
latent variables if you recall HMC basically we're then going to propagate

581
00:36:26,500 --> 00:36:29,230
basically we're then going to propagate those using hamiltonian dynamics so to

582
00:36:29,230 --> 00:36:32,170
those using hamiltonian dynamics so to do that you meet the gradient the

583
00:36:32,170 --> 00:36:35,559
do that you meet the gradient the gradient here is actually so if we take

584
00:36:35,559 --> 00:36:37,930
gradient here is actually so if we take the log of the rights the logs put it

585
00:36:37,930 --> 00:36:40,380
the log of the rights the logs put it across then you have this sum of these

586
00:36:40,380 --> 00:36:43,510
across then you have this sum of these so we call that the gradient here is

587
00:36:43,510 --> 00:36:46,029
so we call that the gradient here is exactly comes from our GP so that's we

588
00:36:46,029 --> 00:36:48,430
exactly comes from our GP so that's we already have these here and then the

589
00:36:48,430 --> 00:36:51,579
already have these here and then the derivative of our prior is analytical

590
00:36:51,579 --> 00:36:54,099
derivative of our prior is analytical because it's a PGM and we can control we

591
00:36:54,099 --> 00:36:54,840
because it's a PGM and we can control we can say that

592
00:36:54,840 --> 00:36:57,360
can say that nice distributions here that allows us

593
00:36:57,360 --> 00:36:59,490
nice distributions here that allows us you know find this attractable

594
00:36:59,490 --> 00:37:04,539
you know find this attractable actual long solution

595
00:37:04,539 --> 00:37:04,549


596
00:37:04,549 --> 00:37:08,949
so yeah so base B once once the particles are sampled with HMC so

597
00:37:08,949 --> 00:37:11,410
particles are sampled with HMC so essentially we have a set then propagate

598
00:37:11,410 --> 00:37:13,569
essentially we have a set then propagate a little bit through his set amounts to

599
00:37:13,569 --> 00:37:16,269
a little bit through his set amounts to that so like 25 or whatever we then

600
00:37:16,269 --> 00:37:18,400
that so like 25 or whatever we then update the PCM so basically this new set

601
00:37:18,400 --> 00:37:20,799
update the PCM so basically this new set of particles within which are latent

602
00:37:20,799 --> 00:37:23,650
of particles within which are latent variables act as observations for the

603
00:37:23,650 --> 00:37:26,829
variables act as observations for the PGM and in a sense she then we take over

604
00:37:26,829 --> 00:37:29,979
PGM and in a sense she then we take over and we do the standard mixture of

605
00:37:29,979 --> 00:37:33,009
and we do the standard mixture of garbage in optimization process you know

606
00:37:33,009 --> 00:37:34,809
garbage in optimization process you know the variational approach or if you're

607
00:37:34,809 --> 00:37:36,880
the variational approach or if you're not so in the Bayesian and DM approach

608
00:37:36,880 --> 00:37:39,130
not so in the Bayesian and DM approach at an optimized peak and then you go

609
00:37:39,130 --> 00:37:41,410
at an optimized peak and then you go back you return to the document process

610
00:37:41,410 --> 00:37:45,429
back you return to the document process you optimize and basically on you repeat

611
00:37:45,429 --> 00:37:49,410
you optimize and basically on you repeat you jump back and forth so in summary

612
00:37:49,410 --> 00:37:51,969
you jump back and forth so in summary essentially we have a set of training

613
00:37:51,969 --> 00:37:55,539
essentially we have a set of training observations Y we have an initial set of

614
00:37:55,539 --> 00:37:57,789
observations Y we have an initial set of latent variable is packed however

615
00:37:57,789 --> 00:37:59,559
latent variable is packed however looking for each training parameters

616
00:37:59,559 --> 00:38:02,049
looking for each training parameters here so theta again is our Gaussian

617
00:38:02,049 --> 00:38:06,219
here so theta again is our Gaussian process parameters so length scales or

618
00:38:06,219 --> 00:38:08,289
process parameters so length scales or whatever some additional noise terms and

619
00:38:08,289 --> 00:38:10,150
whatever some additional noise terms and then these are all part of our actual

620
00:38:10,150 --> 00:38:13,359
then these are all part of our actual PDF so the first step is that we're

621
00:38:13,359 --> 00:38:15,130
PDF so the first step is that we're going to calculate the gradient of our

622
00:38:15,130 --> 00:38:17,829
going to calculate the gradient of our potential here we're then little conduct

623
00:38:17,829 --> 00:38:19,599
potential here we're then little conduct agency so essentially we're going to

624
00:38:19,599 --> 00:38:21,249
agency so essentially we're going to take every single particle or I'll move

625
00:38:21,249 --> 00:38:22,569
take every single particle or I'll move it a little bit a little bit a little

626
00:38:22,569 --> 00:38:25,449
it a little bit a little bit a little bit toward the posterior and then that

627
00:38:25,449 --> 00:38:27,219
bit toward the posterior and then that sample right there after stead amount of

628
00:38:27,219 --> 00:38:29,140
sample right there after stead amount of steps will then act as a new set of

629
00:38:29,140 --> 00:38:30,789
steps will then act as a new set of waiting observations for the

630
00:38:30,789 --> 00:38:33,339
waiting observations for the probabilistic graphical models from here

631
00:38:33,339 --> 00:38:36,130
probabilistic graphical models from here we then conduct the inference on the PGM

632
00:38:36,130 --> 00:38:37,839
we then conduct the inference on the PGM we iterate back and forth between

633
00:38:37,839 --> 00:38:39,519
we iterate back and forth between computing the responsibilities and

634
00:38:39,519 --> 00:38:43,140
computing the responsibilities and actual parameters the hyper parameters

635
00:38:43,140 --> 00:38:45,420
actual parameters the hyper parameters and we update them and then down here I

636
00:38:45,420 --> 00:38:47,490
and we update them and then down here I did not mention this but essentially the

637
00:38:47,490 --> 00:38:50,430
did not mention this but essentially the the parameters and the Guardian process

638
00:38:50,430 --> 00:38:52,860
the parameters and the Guardian process are also updated through agency also so

639
00:38:52,860 --> 00:38:55,170
are also updated through agency also so basically they sample length scales and

640
00:38:55,170 --> 00:38:57,390
basically they sample length scales and noise terms and slowly propagate those

641
00:38:57,390 --> 00:38:59,040
noise terms and slowly propagate those as well and basically this entire

642
00:38:59,040 --> 00:39:00,930
as well and basically this entire process and then it looped over and over

643
00:39:00,930 --> 00:39:03,000
process and then it looped over and over and over until you team convergent or

644
00:39:03,000 --> 00:39:06,710
and over until you team convergent or your epoch has been reached

645
00:39:06,710 --> 00:39:09,000
your epoch has been reached alright yes this is a quick on

646
00:39:09,000 --> 00:39:12,990
alright yes this is a quick on discussion regarding the predictive

647
00:39:12,990 --> 00:39:14,880
discussion regarding the predictive distribution so this is a little bit

648
00:39:14,880 --> 00:39:18,350
distribution so this is a little bit it's a little bit involved essentially

649
00:39:18,350 --> 00:39:21,530
it's a little bit involved essentially we have to do a Monte Carlo

650
00:39:21,530 --> 00:39:24,900
we have to do a Monte Carlo approximation unfortunately last and

651
00:39:24,900 --> 00:39:27,000
approximation unfortunately last and unfortunately but yeah and the main

652
00:39:27,000 --> 00:39:28,470
unfortunately but yeah and the main reason behind that is because we're

653
00:39:28,470 --> 00:39:30,450
reason behind that is because we're using monstro it's actually sampled

654
00:39:30,450 --> 00:39:33,720
using monstro it's actually sampled easily in variables and optimize it but

655
00:39:33,720 --> 00:39:36,780
easily in variables and optimize it but I will say here that on so breaking

656
00:39:36,780 --> 00:39:38,990
I will say here that on so breaking apart actual predictive distribution

657
00:39:38,990 --> 00:39:41,130
apart actual predictive distribution where Y stars weren't guaranteed

658
00:39:41,130 --> 00:39:42,900
where Y stars weren't guaranteed observation that were interested in and

659
00:39:42,900 --> 00:39:50,250
observation that were interested in and X star is going to be so if we kind of

660
00:39:50,250 --> 00:39:54,030
X star is going to be so if we kind of split this up and separate it we can see

661
00:39:54,030 --> 00:39:56,610
split this up and separate it we can see that this guy here this is actually done

662
00:39:56,610 --> 00:39:59,760
that this guy here this is actually done through agency so this is it done three

663
00:39:59,760 --> 00:40:01,320
through agency so this is it done three agencies this is also so this is

664
00:40:01,320 --> 00:40:03,690
agencies this is also so this is basically done by sampling we

665
00:40:03,690 --> 00:40:06,660
basically done by sampling we marginalized yeah so we compute this by

666
00:40:06,660 --> 00:40:09,210
marginalized yeah so we compute this by essentially and playing a bunch of new

667
00:40:09,210 --> 00:40:11,640
essentially and playing a bunch of new flaking variables from our PGM and

668
00:40:11,640 --> 00:40:13,320
flaking variables from our PGM and feeding them forward through the network

669
00:40:13,320 --> 00:40:17,460
feeding them forward through the network and this right here is actually the GP

670
00:40:17,460 --> 00:40:18,960
and this right here is actually the GP predictive distribution which there's

671
00:40:18,960 --> 00:40:21,480
predictive distribution which there's actually playing analytical for because

672
00:40:21,480 --> 00:40:26,150
actually playing analytical for because it's just bizarre

673
00:40:26,150 --> 00:40:26,160


674
00:40:26,160 --> 00:40:29,770
all right

675
00:40:29,770 --> 00:40:29,780


676
00:40:29,780 --> 00:40:38,109
still angry work I don't know which

677
00:40:38,109 --> 00:40:38,119


678
00:40:38,119 --> 00:40:47,430
I have no internet

679
00:40:47,430 --> 00:40:47,440


680
00:40:47,440 --> 00:40:53,410
okay all right well anyway I'll just I'll just kind of illustrate this here

681
00:40:53,410 --> 00:40:56,290
I'll just kind of illustrate this here so essentially this if you look on the

682
00:40:56,290 --> 00:40:58,540
so essentially this if you look on the slides basically this is a movie that

683
00:40:58,540 --> 00:41:00,640
slides basically this is a movie that illustrates this whole process so the

684
00:41:00,640 --> 00:41:03,940
illustrates this whole process so the idea is that on on the right we have a

685
00:41:03,940 --> 00:41:05,910
idea is that on on the right we have a set of observations and a spiral

686
00:41:05,910 --> 00:41:09,610
set of observations and a spiral basically these we through the GP on

687
00:41:09,610 --> 00:41:12,820
basically these we through the GP on these are translated to variables that

688
00:41:12,820 --> 00:41:16,630
these are translated to variables that we then propagate through HMC the PGM

689
00:41:16,630 --> 00:41:18,250
we then propagate through HMC the PGM then tries to map instead of mixtures

690
00:41:18,250 --> 00:41:21,340
then tries to map instead of mixtures onto that and then over here we then

691
00:41:21,340 --> 00:41:23,290
onto that and then over here we then basically sample a bunch from these and

692
00:41:23,290 --> 00:41:24,820
basically sample a bunch from these and then this represents the standpoint by

693
00:41:24,820 --> 00:41:27,610
then this represents the standpoint by the colors and there's also shows in the

694
00:41:27,610 --> 00:41:42,609
the colors and there's also shows in the paper of how

695
00:41:42,609 --> 00:41:42,619


696
00:41:42,619 --> 00:41:49,710
and also our breakfast any question

697
00:41:49,710 --> 00:41:49,720


698
00:41:49,720 --> 00:42:51,940
[Applause]

699
00:42:51,940 --> 00:42:51,950


700
00:42:51,950 --> 00:42:56,319
good afternoon everyone I'll be talking about the paper that has been recently

701
00:42:56,319 --> 00:42:59,020
about the paper that has been recently published in 2017 that is a learning in

702
00:42:59,020 --> 00:43:00,460
published in 2017 that is a learning in network structure of heterogeneous data

703
00:43:00,460 --> 00:43:02,530
network structure of heterogeneous data by your pairwise exponential macro

704
00:43:02,530 --> 00:43:04,809
by your pairwise exponential macro random field and in this we can observe

705
00:43:04,809 --> 00:43:07,030
random field and in this we can observe there are three main important keywords

706
00:43:07,030 --> 00:43:09,520
there are three main important keywords one is the learning the network

707
00:43:09,520 --> 00:43:11,859
one is the learning the network structure second one is a word data

708
00:43:11,859 --> 00:43:14,319
structure second one is a word data it's a heterogeneous in nature and the

709
00:43:14,319 --> 00:43:16,230
it's a heterogeneous in nature and the third one is what model we are going to

710
00:43:16,230 --> 00:43:18,970
third one is what model we are going to want to pair with exponential Markov

711
00:43:18,970 --> 00:43:21,760
want to pair with exponential Markov random field and this is the outline of

712
00:43:21,760 --> 00:43:25,150
random field and this is the outline of my this thing I mean talk to me first

713
00:43:25,150 --> 00:43:26,440
my this thing I mean talk to me first I'll be giving an introduction of water

714
00:43:26,440 --> 00:43:28,690
I'll be giving an introduction of water test next but the motivation of its of

715
00:43:28,690 --> 00:43:32,589
test next but the motivation of its of the work and then later the column

716
00:43:32,589 --> 00:43:35,500
the work and then later the column definition and then I'll be talking very

717
00:43:35,500 --> 00:43:40,240
definition and then I'll be talking very in detail about pmrs and later since

718
00:43:40,240 --> 00:43:43,900
in detail about pmrs and later since there is one term that is the partition

719
00:43:43,900 --> 00:43:45,870
there is one term that is the partition function which is not tractable so

720
00:43:45,870 --> 00:43:47,530
function which is not tractable so computationally so we go for

721
00:43:47,530 --> 00:43:49,000
computationally so we go for approximated or maximum like here

722
00:43:49,000 --> 00:43:50,319
approximated or maximum like here instead of going for exact maximum

723
00:43:50,319 --> 00:43:52,240
instead of going for exact maximum likelihood and then later I'll be

724
00:43:52,240 --> 00:43:55,030
likelihood and then later I'll be discussing about optimization technique

725
00:43:55,030 --> 00:43:56,799
discussing about optimization technique called or a DMM that is alternating

726
00:43:56,799 --> 00:43:58,630
called or a DMM that is alternating direction or method of power multiplex

727
00:43:58,630 --> 00:44:01,059
direction or method of power multiplex and then I will show a bleep result of

728
00:44:01,059 --> 00:44:03,690
and then I will show a bleep result of what is obtained and then the conclusion

729
00:44:03,690 --> 00:44:08,109
what is obtained and then the conclusion so what does and Markov random feel like

730
00:44:08,109 --> 00:44:09,700
so what does and Markov random feel like its importance of macro random field is

731
00:44:09,700 --> 00:44:11,349
its importance of macro random field is like its importance is fundamental tool

732
00:44:11,349 --> 00:44:14,530
like its importance is fundamental tool to many applications in machine learning

733
00:44:14,530 --> 00:44:18,690
to many applications in machine learning and also it is necessary for the samara

734
00:44:18,690 --> 00:44:21,010
and also it is necessary for the samara from all this a matter for heterogeneous

735
00:44:21,010 --> 00:44:23,260
from all this a matter for heterogeneous entities so if for example you have a

736
00:44:23,260 --> 00:44:27,520
entities so if for example you have a medical database and nature so that is

737
00:44:27,520 --> 00:44:30,099
medical database and nature so that is it has between the categorical and

738
00:44:30,099 --> 00:44:32,079
it has between the categorical and continuous variables so for example the

739
00:44:32,079 --> 00:44:34,500
continuous variables so for example the age gender the medical history of the

740
00:44:34,500 --> 00:44:40,150
age gender the medical history of the patient and the other interesting area

741
00:44:40,150 --> 00:44:44,680
patient and the other interesting area where it can be used for analyzing the

742
00:44:44,680 --> 00:44:48,160
where it can be used for analyzing the protein interaction

743
00:44:48,160 --> 00:44:48,170


744
00:44:48,170 --> 00:44:54,040
there is one major major field which implements this and that is in

745
00:44:54,040 --> 00:44:57,550
implements this and that is in computational biology which is the being

746
00:44:57,550 --> 00:44:58,660
computational biology which is the being unable to task for this is to

747
00:44:58,660 --> 00:45:02,200
unable to task for this is to infrastructure and what is happening we

748
00:45:02,200 --> 00:45:05,110
infrastructure and what is happening we now people and people who sees the

749
00:45:05,110 --> 00:45:09,030
now people and people who sees the Gaussian graphical models but in

750
00:45:09,030 --> 00:45:12,550
Gaussian graphical models but in nowadays with new technologies of this

751
00:45:12,550 --> 00:45:16,600
nowadays with new technologies of this DNA sequencing we can which produces the

752
00:45:16,600 --> 00:45:18,700
DNA sequencing we can which produces the data with or with nitrogen is activation

753
00:45:18,700 --> 00:45:21,670
data with or with nitrogen is activation and this growth gaussian graphical

754
00:45:21,670 --> 00:45:25,590
and this growth gaussian graphical models will not be able to extract

755
00:45:25,590 --> 00:45:27,540
models will not be able to extract generate the structure for this

756
00:45:27,540 --> 00:45:31,080
generate the structure for this heterogenous domain so because of this

757
00:45:31,080 --> 00:45:34,570
heterogenous domain so because of this we are going to outline the entire now

758
00:45:34,570 --> 00:45:36,790
we are going to outline the entire now in the sport questions we are going to

759
00:45:36,790 --> 00:45:39,700
in the sport questions we are going to see what there's a PMRF and then second

760
00:45:39,700 --> 00:45:41,280
see what there's a PMRF and then second we are going to of approximate the

761
00:45:41,280 --> 00:45:42,480
we are going to of approximate the [Applause]

762
00:45:42,480 --> 00:45:44,740
[Applause] formula to approximate in maximum

763
00:45:44,740 --> 00:45:47,860
formula to approximate in maximum likelihood problem and then develop

764
00:45:47,860 --> 00:45:50,890
likelihood problem and then develop Avant and then on the partition function

765
00:45:50,890 --> 00:45:53,320
Avant and then on the partition function in the later a scalable or a DML

766
00:45:53,320 --> 00:45:56,260
in the later a scalable or a DML optimization algorithm with the closed

767
00:45:56,260 --> 00:45:58,780
optimization algorithm with the closed form update and then you'll also know

768
00:45:58,780 --> 00:46:01,920
form update and then you'll also know that if then our estimator is a Faustian

769
00:46:01,920 --> 00:46:06,910
that if then our estimator is a Faustian so what is PMS going to diffuse of EMR

770
00:46:06,910 --> 00:46:10,090
so what is PMS going to diffuse of EMR so there are some definitions that we

771
00:46:10,090 --> 00:46:11,620
so there are some definitions that we are going to consider first we'll

772
00:46:11,620 --> 00:46:13,900
are going to consider first we'll consider that data is like an

773
00:46:13,900 --> 00:46:15,580
consider that data is like an independent multiband observation and

774
00:46:15,580 --> 00:46:18,550
independent multiband observation and then later these samples are iid from an

775
00:46:18,550 --> 00:46:22,000
then later these samples are iid from an exponential family of institution it's P

776
00:46:22,000 --> 00:46:24,040
exponential family of institution it's P of X comma theta and then later with

777
00:46:24,040 --> 00:46:26,230
of X comma theta and then later with represented by P no graphical model with

778
00:46:26,230 --> 00:46:29,650
represented by P no graphical model with the natural parameter theta and and we

779
00:46:29,650 --> 00:46:31,900
the natural parameter theta and and we use these samples to estimate the

780
00:46:31,900 --> 00:46:33,670
use these samples to estimate the underling distribution and this is the

781
00:46:33,670 --> 00:46:37,600
underling distribution and this is the in general like we have London PGM where

782
00:46:37,600 --> 00:46:40,060
in general like we have London PGM where V is my Nora vertices and E is my edge

783
00:46:40,060 --> 00:46:43,390
V is my Nora vertices and E is my edge and the length of this is P and also the

784
00:46:43,390 --> 00:46:44,710
and the length of this is P and also the structure is encoded with this

785
00:46:44,710 --> 00:46:51,300
structure is encoded with this exponential family parameter theta so

786
00:46:51,300 --> 00:46:54,220
exponential family parameter theta so PMRF pms is a subclass of multivariate

787
00:46:54,220 --> 00:46:55,190
PMRF pms is a subclass of multivariate exponent

788
00:46:55,190 --> 00:46:59,270
or exponential family that can explain explicitly relieve the marker structure

789
00:46:59,270 --> 00:47:01,910
explicitly relieve the marker structure across heterogeneous variables and here

790
00:47:01,910 --> 00:47:03,829
across heterogeneous variables and here this these are the few notations that we

791
00:47:03,829 --> 00:47:06,950
this these are the few notations that we will follow in this light falling slice

792
00:47:06,950 --> 00:47:08,720
will follow in this light falling slice the inner product is given by this

793
00:47:08,720 --> 00:47:11,960
the inner product is given by this trailer of a B and also the obvious upma

794
00:47:11,960 --> 00:47:14,000
trailer of a B and also the obvious upma contaminate the set of very effectors in

795
00:47:14,000 --> 00:47:17,870
contaminate the set of very effectors in this form okay so let's consider a

796
00:47:17,870 --> 00:47:21,799
this form okay so let's consider a random vector with X X 1 to X B and then

797
00:47:21,799 --> 00:47:23,420
random vector with X X 1 to X B and then later which are retro genus and within

798
00:47:23,420 --> 00:47:25,579
later which are retro genus and within this domain or extras a chronicle

799
00:47:25,579 --> 00:47:28,660
this domain or extras a chronicle product of just variables so what is it

800
00:47:28,660 --> 00:47:31,730
product of just variables so what is it if you have a matrix a which is a 1 a 2

801
00:47:31,730 --> 00:47:34,880
if you have a matrix a which is a 1 a 2 a 3 a 4 and B so we multiply each

802
00:47:34,880 --> 00:47:37,880
a 3 a 4 and B so we multiply each element of this a1 through this matrix B

803
00:47:37,880 --> 00:47:40,390
element of this a1 through this matrix B and a to this PHP so that's the

804
00:47:40,390 --> 00:47:43,190
and a to this PHP so that's the chronicle product here and then later or

805
00:47:43,190 --> 00:47:46,430
chronicle product here and then later or suppose we the conditional distribution

806
00:47:46,430 --> 00:47:48,589
suppose we the conditional distribution of each variable is X are given all my

807
00:47:48,589 --> 00:47:51,309
of each variable is X are given all my previous PP minus 1 variables are

808
00:47:51,309 --> 00:47:53,480
previous PP minus 1 variables are unknown to be the exponential

809
00:47:53,480 --> 00:47:57,890
unknown to be the exponential distribution in this domain is the sy

810
00:47:57,890 --> 00:48:01,609
distribution in this domain is the sy are Thea ok so yeah and then later the

811
00:48:01,609 --> 00:48:03,440
are Thea ok so yeah and then later the distribution is also specified with this

812
00:48:03,440 --> 00:48:07,700
distribution is also specified with this M R dimensional or potential with BR and

813
00:48:07,700 --> 00:48:11,180
M R dimensional or potential with BR and s BR into X Appa and my scale based

814
00:48:11,180 --> 00:48:13,309
s BR into X Appa and my scale based parameters TR into extra part and then

815
00:48:13,309 --> 00:48:16,220
parameters TR into extra part and then my let the K this is the way this is the

816
00:48:16,220 --> 00:48:18,319
my let the K this is the way this is the main Joint Distribution for this the

817
00:48:18,319 --> 00:48:22,280
main Joint Distribution for this the condition is that my X is 1 to XP you

818
00:48:22,280 --> 00:48:25,240
condition is that my X is 1 to XP you should follow this joint distribution

819
00:48:25,240 --> 00:48:29,180
should follow this joint distribution where a of theta is my partition

820
00:48:29,180 --> 00:48:35,000
where a of theta is my partition function and theta is my the no

821
00:48:35,000 --> 00:48:37,430
function and theta is my the no parameter sorry this X parameter and the

822
00:48:37,430 --> 00:48:41,480
parameter sorry this X parameter and the sweetest monitor is my parameter so yeah

823
00:48:41,480 --> 00:48:45,380
sweetest monitor is my parameter so yeah theta 1 to theta P so we're maldita is

824
00:48:45,380 --> 00:48:47,720
theta 1 to theta P so we're maldita is the node parameter and the big theta is

825
00:48:47,720 --> 00:48:50,000
the node parameter and the big theta is my order Kappa theta is my H parameter

826
00:48:50,000 --> 00:48:52,280
my order Kappa theta is my H parameter and your Peter make a lock partition

827
00:48:52,280 --> 00:48:54,579
and your Peter make a lock partition function which is expressed in this

828
00:48:54,579 --> 00:48:56,930
function which is expressed in this equation ok

829
00:48:56,930 --> 00:49:00,260
equation ok so now this is a small brief like like

830
00:49:00,260 --> 00:49:03,110
so now this is a small brief like like we're going to say we're going to define

831
00:49:03,110 --> 00:49:05,570
we're going to say we're going to define this P of X in terms of this exponential

832
00:49:05,570 --> 00:49:08,330
this P of X in terms of this exponential family expression so where B of X is

833
00:49:08,330 --> 00:49:09,830
family expression so where B of X is nothing but when sufficient statistic

834
00:49:09,830 --> 00:49:12,710
nothing but when sufficient statistic this is my base parameter and a of theta

835
00:49:12,710 --> 00:49:15,680
this is my base parameter and a of theta is my partition function and this inner

836
00:49:15,680 --> 00:49:20,770
is my partition function and this inner product I define it in this fashion okay

837
00:49:20,770 --> 00:49:20,780


838
00:49:20,780 --> 00:49:27,440
so the next important thing is what happens at one node so one a conditional

839
00:49:27,440 --> 00:49:29,690
happens at one node so one a conditional distribution what happens so a node

840
00:49:29,690 --> 00:49:33,100
distribution what happens so a node condition distribution given node X are

841
00:49:33,100 --> 00:49:36,380
condition distribution given node X are given all my known condition on may all

842
00:49:36,380 --> 00:49:38,600
given all my known condition on may all only previous notes that V of you'll

843
00:49:38,600 --> 00:49:39,440
only previous notes that V of you'll know that it's an exponential

844
00:49:39,440 --> 00:49:41,750
know that it's an exponential exponential family is given by this

845
00:49:41,750 --> 00:49:43,280
exponential family is given by this expression and I don't have a of beta

846
00:49:43,280 --> 00:49:44,690
expression and I don't have a of beta because I have given up proportional

847
00:49:44,690 --> 00:49:46,910
because I have given up proportional sign here where my sufficient statistics

848
00:49:46,910 --> 00:49:50,870
sign here where my sufficient statistics is just our these variables and these

849
00:49:50,870 --> 00:49:52,730
is just our these variables and these values and base parameter of these

850
00:49:52,730 --> 00:49:56,930
values and base parameter of these values okay the reason why because we

851
00:49:56,930 --> 00:49:59,270
values okay the reason why because we are both we are doing node by node or

852
00:49:59,270 --> 00:50:02,060
are both we are doing node by node or basis because of that I defined that the

853
00:50:02,060 --> 00:50:05,270
basis because of that I defined that the road condition institution okay next the

854
00:50:05,270 --> 00:50:07,520
road condition institution okay next the PMRF model can is the populace tribution

855
00:50:07,520 --> 00:50:09,170
PMRF model can is the populace tribution which which can ranging from a

856
00:50:09,170 --> 00:50:11,660
which which can ranging from a homogeneous to or mixed one and then

857
00:50:11,660 --> 00:50:13,520
homogeneous to or mixed one and then later from a node conditional

858
00:50:13,520 --> 00:50:15,290
later from a node conditional distribution in previous equation we can

859
00:50:15,290 --> 00:50:18,080
distribution in previous equation we can that we can design up

860
00:50:18,080 --> 00:50:20,630
that we can design up PMR for a node by node distance by

861
00:50:20,630 --> 00:50:23,000
PMR for a node by node distance by choosing a decide of my base parameter

862
00:50:23,000 --> 00:50:25,880
choosing a decide of my base parameter and my official status in the VR and see

863
00:50:25,880 --> 00:50:30,050
and my official status in the VR and see oh yeah putting still BR and my based

864
00:50:30,050 --> 00:50:34,370
oh yeah putting still BR and my based measure TR and note that this is a joint

865
00:50:34,370 --> 00:50:37,190
measure TR and note that this is a joint this is valid if I am able to obtain a

866
00:50:37,190 --> 00:50:40,520
this is valid if I am able to obtain a value for y of theta but my L theta is

867
00:50:40,520 --> 00:50:42,740
value for y of theta but my L theta is computationally intractable so the

868
00:50:42,740 --> 00:50:45,680
computationally intractable so the reason why we go for approximate log

869
00:50:45,680 --> 00:50:49,790
reason why we go for approximate log likelihood okay so for example I don't

870
00:50:49,790 --> 00:50:51,260
likelihood okay so for example I don't give an example of a universal

871
00:50:51,260 --> 00:50:54,189
give an example of a universal distribution where are my

872
00:50:54,189 --> 00:52:00,699
distribution where are my Sigma into the - and what happens if my

873
00:52:00,699 --> 00:52:04,519
Sigma into the - and what happens if my lambda is in a structured graph so and

874
00:52:04,519 --> 00:52:08,509
lambda is in a structured graph so and again in this we use a penalty so this

875
00:52:08,509 --> 00:52:10,819
again in this we use a penalty so this one have taken it from the mouse we book

876
00:52:10,819 --> 00:52:13,370
one have taken it from the mouse we book chapter 23 I believe so this is an

877
00:52:13,370 --> 00:52:16,669
chapter 23 I believe so this is an example so my lambda is very high so it

878
00:52:16,669 --> 00:52:18,919
example so my lambda is very high so it is very sparse in nature and for this my

879
00:52:18,919 --> 00:52:20,689
is very sparse in nature and for this my lambda is equal to 0 so it is very dense

880
00:52:20,689 --> 00:52:23,269
lambda is equal to 0 so it is very dense in nature so my lambda gives me a very

881
00:52:23,269 --> 00:52:28,969
in nature so my lambda gives me a very big spectrum of my specificity okay so

882
00:52:28,969 --> 00:52:30,829
big spectrum of my specificity okay so next a parent thing is the approximate

883
00:52:30,829 --> 00:52:36,319
next a parent thing is the approximate maximum like you know make you so we go

884
00:52:36,319 --> 00:52:37,999
maximum like you know make you so we go for a profit because a of theta as high

885
00:52:37,999 --> 00:52:39,949
for a profit because a of theta as high dimensional integral and it's typically

886
00:52:39,949 --> 00:52:44,259
dimensional integral and it's typically intractable and in order to do this we

887
00:52:44,259 --> 00:52:47,479
intractable and in order to do this we replace your theta with intractable or

888
00:52:47,479 --> 00:52:50,509
replace your theta with intractable or convex upper value of theta or for my

889
00:52:50,509 --> 00:52:54,979
convex upper value of theta or for my problem for proceeding for me to obtain

890
00:52:54,979 --> 00:52:57,859
problem for proceeding for me to obtain our value for U of T Tauri of theta so

891
00:52:57,859 --> 00:53:00,450
our value for U of T Tauri of theta so we need we need to recall all

892
00:53:00,450 --> 00:53:07,140
we need we need to recall all patience because from the I mean a

893
00:53:07,140 --> 00:53:11,010
patience because from the I mean a different paper so in order to have a

894
00:53:11,010 --> 00:53:12,030
different paper so in order to have a compact notation

895
00:53:12,030 --> 00:53:14,040
compact notation I just recall everything again so theta

896
00:53:14,040 --> 00:53:15,810
I just recall everything again so theta is my natural parameter and be your

897
00:53:15,810 --> 00:53:18,320
is my natural parameter and be your purchase my sufficient statistics and

898
00:53:18,320 --> 00:53:21,240
purchase my sufficient statistics and it's in this domain and also my inner

899
00:53:21,240 --> 00:53:22,680
it's in this domain and also my inner product is expressed in this fashion

900
00:53:22,680 --> 00:53:26,070
product is expressed in this fashion which we all remember few slides back

901
00:53:26,070 --> 00:53:28,140
which we all remember few slides back and also here the new thing is we're

902
00:53:28,140 --> 00:53:31,230
and also here the new thing is we're going to define the mean parameters in

903
00:53:31,230 --> 00:53:35,670
going to define the mean parameters in this fashion and this is for my for my

904
00:53:35,670 --> 00:53:38,160
this fashion and this is for my for my edge so main one is for my edge and my

905
00:53:38,160 --> 00:53:40,109
edge so main one is for my edge and my expectation of the VR of X is nothing

906
00:53:40,109 --> 00:53:43,050
expectation of the VR of X is nothing but my mu R and for the similarly for

907
00:53:43,050 --> 00:53:45,660
but my mu R and for the similarly for the S parameter and also one more

908
00:53:45,660 --> 00:53:46,920
the S parameter and also one more important thing is you are going to

909
00:53:46,920 --> 00:53:52,230
important thing is you are going to express it and the Google map the for

910
00:53:52,230 --> 00:53:55,079
express it and the Google map the for the node and the X parameter with the in

911
00:53:55,079 --> 00:54:00,839
the node and the X parameter with the in this matrix okay the reason I mentioned

912
00:54:00,839 --> 00:54:02,190
this matrix okay the reason I mentioned it because in this book and the

913
00:54:02,190 --> 00:54:02,880
it because in this book and the surrealist paper

914
00:54:02,880 --> 00:54:06,480
surrealist paper Jordan paper he gives an expression for

915
00:54:06,480 --> 00:54:08,760
Jordan paper he gives an expression for the upper bound the lock partition

916
00:54:08,760 --> 00:54:09,960
the upper bound the lock partition function of theta has the following

917
00:54:09,960 --> 00:54:13,050
function of theta has the following upper bound and here the same notations

918
00:54:13,050 --> 00:54:15,329
upper bound and here the same notations what a different report is used here in

919
00:54:15,329 --> 00:54:19,109
what a different report is used here in this paper and this is just a brief

920
00:54:19,109 --> 00:54:21,480
this paper and this is just a brief explanation of how he has obtained this

921
00:54:21,480 --> 00:54:23,250
explanation of how he has obtained this one so in particularly we obtain the

922
00:54:23,250 --> 00:54:25,109
one so in particularly we obtain the upper bound relationship between the

923
00:54:25,109 --> 00:54:26,670
upper bound relationship between the entropy H of X and the entropy of the

924
00:54:26,670 --> 00:54:29,609
entropy H of X and the entropy of the node potential given by this in addition

925
00:54:29,609 --> 00:54:32,010
node potential given by this in addition to the different choices of L R where R

926
00:54:32,010 --> 00:54:34,380
to the different choices of L R where R because I have in my Pino does this

927
00:54:34,380 --> 00:54:37,230
because I have in my Pino does this thing so it's goes from R 1 to P 4 or

928
00:54:37,230 --> 00:54:40,070
thing so it's goes from R 1 to P 4 or heterogeneous domains so I have this

929
00:54:40,070 --> 00:54:44,490
heterogeneous domains so I have this expression for a of theta now is by

930
00:54:44,490 --> 00:54:46,230
expression for a of theta now is by taking the dedication of this dual you

931
00:54:46,230 --> 00:54:47,520
taking the dedication of this dual you can convert this high dimension problem

932
00:54:47,520 --> 00:54:49,829
can convert this high dimension problem from the above theorem and that is

933
00:54:49,829 --> 00:54:51,240
from the above theorem and that is orderable mention equation to the

934
00:54:51,240 --> 00:54:54,420
orderable mention equation to the calling or tractable form so that the

935
00:54:54,420 --> 00:54:57,150
calling or tractable form so that the same expression we can write in this

936
00:54:57,150 --> 00:54:59,400
same expression we can write in this where F of 2 is expressed in the

937
00:54:59,400 --> 00:55:03,690
where F of 2 is expressed in the equation here later we are going to plug

938
00:55:03,690 --> 00:55:05,550
equation here later we are going to plug in this a of theta into the main

939
00:55:05,550 --> 00:55:08,880
in this a of theta into the main equation here

940
00:55:08,880 --> 00:55:08,890


941
00:55:08,890 --> 00:55:14,350
into the main equation here and then later simplifying it we obtain this

942
00:55:14,350 --> 00:55:16,210
later simplifying it we obtain this optimization problem available to

943
00:55:16,210 --> 00:55:17,980
optimization problem available to minimize this and where this is my

944
00:55:17,980 --> 00:55:20,200
minimize this and where this is my different matrix we have a theta belongs

945
00:55:20,200 --> 00:55:23,440
different matrix we have a theta belongs to and this this expression is nothing

946
00:55:23,440 --> 00:55:25,390
to and this this expression is nothing but the expression that we had observed

947
00:55:25,390 --> 00:55:30,190
but the expression that we had observed when Cody Baker explained that yesterday

948
00:55:30,190 --> 00:55:32,340
when Cody Baker explained that yesterday yesterday afternoon so when he explained

949
00:55:32,340 --> 00:55:36,790
yesterday afternoon so when he explained this group graphical so yesterday so

950
00:55:36,790 --> 00:55:39,610
this group graphical so yesterday so this is something similar to that so to

951
00:55:39,610 --> 00:55:43,480
this is something similar to that so to make it group so what we do is we for a

952
00:55:43,480 --> 00:55:45,550
make it group so what we do is we for a zero-mean graph a Gaussian amara with

953
00:55:45,550 --> 00:55:50,400
zero-mean graph a Gaussian amara with additional constraint that is

954
00:55:50,400 --> 00:55:50,410


955
00:55:50,410 --> 00:55:57,880
conditionally independent then what we do is we study to 0 and in that case

956
00:55:57,880 --> 00:55:59,500
do is we study to 0 and in that case profit is equal to the graphical Rezo

957
00:55:59,500 --> 00:56:01,360
profit is equal to the graphical Rezo but there's one difference in the

958
00:56:01,360 --> 00:56:03,520
but there's one difference in the optimization procedure what he or what

959
00:56:03,520 --> 00:56:04,990
optimization procedure what he or what he has followed and what is followed

960
00:56:04,990 --> 00:56:08,470
he has followed and what is followed here in that the graphical ticket

961
00:56:08,470 --> 00:56:10,630
here in that the graphical ticket optimizes you with respect to the

962
00:56:10,630 --> 00:56:12,940
optimizes you with respect to the empirical covariance matrix but in our

963
00:56:12,940 --> 00:56:15,010
empirical covariance matrix but in our case or what we do is we optimize with

964
00:56:15,010 --> 00:56:17,260
case or what we do is we optimize with respect to using the sample average of

965
00:56:17,260 --> 00:56:19,600
respect to using the sample average of the sufficient statistics of the EMRs

966
00:56:19,600 --> 00:56:21,720
the sufficient statistics of the EMRs those things we are talking about the

967
00:56:21,720 --> 00:56:26,040
those things we are talking about the optimization procedure we are going to

968
00:56:26,040 --> 00:56:29,290
optimization procedure we are going to this alternating direction or method of

969
00:56:29,290 --> 00:56:32,970
this alternating direction or method of multipliers and I just give you a brief

970
00:56:32,970 --> 00:56:37,150
multipliers and I just give you a brief explanation of what DMMs and then and

971
00:56:37,150 --> 00:56:41,530
explanation of what DMMs and then and then explain again like how the being

972
00:56:41,530 --> 00:56:44,440
then explain again like how the being implemented so this is just a small

973
00:56:44,440 --> 00:56:48,220
implemented so this is just a small explanation and algorithm which also

974
00:56:48,220 --> 00:56:50,020
explanation and algorithm which also complex optimization problem by breaking

975
00:56:50,020 --> 00:56:52,990
complex optimization problem by breaking into smaller pieces each other which can

976
00:56:52,990 --> 00:56:55,870
into smaller pieces each other which can easily then be handled example it is

977
00:56:55,870 --> 00:56:57,520
easily then be handled example it is considered this would be objective

978
00:56:57,520 --> 00:56:59,350
considered this would be objective function we have to minimize this

979
00:56:59,350 --> 00:57:01,030
function we have to minimize this objective function f of X plus G of Z

980
00:57:01,030 --> 00:57:02,890
objective function f of X plus G of Z and we have to design variables

981
00:57:02,890 --> 00:57:05,170
and we have to design variables X and Z which is subjected to a

982
00:57:05,170 --> 00:57:07,960
X and Z which is subjected to a constraint that is our 8 plus B Z equals

983
00:57:07,960 --> 00:57:10,750
constraint that is our 8 plus B Z equals T and now we define an argument a

984
00:57:10,750 --> 00:57:12,730
T and now we define an argument a Lagrangian with the three parameter Rho

985
00:57:12,730 --> 00:57:15,430
Lagrangian with the three parameter Rho or just greater than zero and we say

986
00:57:15,430 --> 00:57:19,240
or just greater than zero and we say that is nothing but the objective

987
00:57:19,240 --> 00:57:22,440
that is nothing but the objective function plus u u transpose of this

988
00:57:22,440 --> 00:57:25,540
function plus u u transpose of this constraint here plus my LP parameter to

989
00:57:25,540 --> 00:57:32,890
constraint here plus my LP parameter to the norm and what we do we update X for

990
00:57:32,890 --> 00:57:35,350
the norm and what we do we update X for updating X we take the value of Z of the

991
00:57:35,350 --> 00:57:36,880
updating X we take the value of Z of the previous clip and you of the previous

992
00:57:36,880 --> 00:57:40,870
previous clip and you of the previous clip and then later or we update Z when

993
00:57:40,870 --> 00:57:43,990
clip and then later or we update Z when we updating Z we take the updated value

994
00:57:43,990 --> 00:57:48,460
we updating Z we take the updated value of x and then we update we update the D

995
00:57:48,460 --> 00:57:50,260
of x and then we update we update the D but we we do not have the update develop

996
00:57:50,260 --> 00:57:51,850
but we we do not have the update develop view so we are using the previous

997
00:57:51,850 --> 00:57:53,860
view so we are using the previous updated value here and later we get the

998
00:57:53,860 --> 00:57:57,130
updated value here and later we get the closed form or solution with with with

999
00:57:57,130 --> 00:57:59,650
closed form or solution with with with this expression here and that is reason

1000
00:57:59,650 --> 00:58:01,660
this expression here and that is reason why it is known as alternating direction

1001
00:58:01,660 --> 00:58:04,660
why it is known as alternating direction because we do it for X then we do it for

1002
00:58:04,660 --> 00:58:06,640
because we do it for X then we do it for Z and then we do it again at last for

1003
00:58:06,640 --> 00:58:10,180
Z and then we do it again at last for closed form solution for UK I mean so we

1004
00:58:10,180 --> 00:58:12,790
closed form solution for UK I mean so we do it until the stop criterion is

1005
00:58:12,790 --> 00:58:15,370
do it until the stop criterion is mentioned so this is in general so

1006
00:58:15,370 --> 00:58:17,200
mentioned so this is in general so basically we have to update these three

1007
00:58:17,200 --> 00:58:19,900
basically we have to update these three parameters coming back to this problem

1008
00:58:19,900 --> 00:58:23,380
parameters coming back to this problem here we dated again saying that we have

1009
00:58:23,380 --> 00:58:25,780
here we dated again saying that we have to minimize this and again you say that

1010
00:58:25,780 --> 00:58:28,320
to minimize this and again you say that we have to update three things theta big

1011
00:58:28,320 --> 00:58:31,300
we have to update three things theta big and you it is the same thing but for

1012
00:58:31,300 --> 00:58:34,540
and you it is the same thing but for this problem we prom late top-40 to

1013
00:58:34,540 --> 00:58:37,030
this problem we prom late top-40 to update this is the formulation where eta

1014
00:58:37,030 --> 00:58:39,340
update this is the formulation where eta is nothing but Rho by n and this is

1015
00:58:39,340 --> 00:58:43,000
is nothing but Rho by n and this is nothing but Q this my decomposition and

1016
00:58:43,000 --> 00:58:45,670
nothing but Q this my decomposition and later for Z update again this is the

1017
00:58:45,670 --> 00:58:47,470
later for Z update again this is the expression which we have already seen

1018
00:58:47,470 --> 00:58:49,960
expression which we have already seen and also this is from a you updated cell

1019
00:58:49,960 --> 00:58:52,660
and also this is from a you updated cell phone solution so I'll be able to get an

1020
00:58:52,660 --> 00:58:59,370
phone solution so I'll be able to get an optimum value okay I mean it is used for

1021
00:58:59,370 --> 00:59:02,770
optimum value okay I mean it is used for to visualize the wafer to inspect the

1022
00:59:02,770 --> 00:59:05,410
to visualize the wafer to inspect the performance of classification and in

1023
00:59:05,410 --> 00:59:07,570
performance of classification and in this we have two x axis and y axis which

1024
00:59:07,570 --> 00:59:10,609
this we have two x axis and y axis which is to positive and false positive and

1025
00:59:10,609 --> 00:59:14,599
is to positive and false positive and my book club is part of my corner then

1026
00:59:14,599 --> 00:59:17,150
my book club is part of my corner then the system is accurate its operating the

1027
00:59:17,150 --> 00:59:19,969
the system is accurate its operating the venue and since the area under this

1028
00:59:19,969 --> 00:59:25,309
venue and since the area under this curve gives me the appreciation okay I

1029
00:59:25,309 --> 00:59:28,039
curve gives me the appreciation okay I took a small toy problem because I did

1030
00:59:28,039 --> 00:59:29,660
took a small toy problem because I did not get the data set what the paper has

1031
00:59:29,660 --> 00:59:32,359
not get the data set what the paper has implemented so in this in this forum

1032
00:59:32,359 --> 00:59:34,729
implemented so in this in this forum I've taken eight Bernoulli eight gamma

1033
00:59:34,729 --> 00:59:36,739
I've taken eight Bernoulli eight gamma eight Gaussian and made a distillate

1034
00:59:36,739 --> 00:59:38,870
eight Gaussian and made a distillate with K is equal to 3 and there are two

1035
00:59:38,870 --> 00:59:41,509
with K is equal to 3 and there are two cases one has power he other is the

1036
00:59:41,509 --> 00:59:44,359
cases one has power he other is the dense case was fast case taken or the

1037
00:59:44,359 --> 00:59:46,940
dense case was fast case taken or the 10% of the potential that exists and the

1038
00:59:46,940 --> 00:59:49,069
10% of the potential that exists and the for dense case have taken the 50% of the

1039
00:59:49,069 --> 00:59:52,160
for dense case have taken the 50% of the potential edge exists and then these are

1040
00:59:52,160 --> 00:59:53,989
potential edge exists and then these are the results that have obtained this is

1041
00:59:53,989 --> 00:59:55,819
the results that have obtained this is for highly structure and this world

1042
00:59:55,819 --> 00:59:58,130
for highly structure and this world knows fast structure and we can observe

1043
00:59:58,130 --> 01:00:01,430
knows fast structure and we can observe that the the that they illustrate the de

1044
01:00:01,430 --> 01:00:03,670
that the the that they illustrate the de Graaff with the high degree of sparsity

1045
01:00:03,670 --> 01:00:06,349
Graaff with the high degree of sparsity to recover few samples it is much

1046
01:00:06,349 --> 01:00:08,410
to recover few samples it is much possible so that is this is much better

1047
01:00:08,410 --> 01:00:16,969
possible so that is this is much better that is the illustrate just and ok again

1048
01:00:16,969 --> 01:00:20,829
that is the illustrate just and ok again this is the same I'm going to conclude

1049
01:00:20,829 --> 01:00:23,420
this is the same I'm going to conclude to conclude this so in this work we have

1050
01:00:23,420 --> 01:00:25,459
to conclude this so in this work we have discussed the method for learning the

1051
01:00:25,459 --> 01:00:27,009
discussed the method for learning the Markov networks that is from

1052
01:00:27,009 --> 01:00:31,430
Markov networks that is from observational data and then this PMRF is

1053
01:00:31,430 --> 01:00:36,140
observational data and then this PMRF is a multivariate genius data which is much

1054
01:00:36,140 --> 01:00:37,729
a multivariate genius data which is much required for example you take our

1055
01:00:37,729 --> 01:00:39,650
required for example you take our medical database or something like that

1056
01:00:39,650 --> 01:00:44,420
medical database or something like that and then we have not used the exact use

1057
01:00:44,420 --> 01:00:46,519
and then we have not used the exact use approximate maximum likelihood for this

1058
01:00:46,519 --> 01:00:48,680
approximate maximum likelihood for this problem and you also developed our a DMM

1059
01:00:48,680 --> 01:00:50,539
problem and you also developed our a DMM algorithm which can reduce me a

1060
01:00:50,539 --> 01:00:54,499
algorithm which can reduce me a closed-form updates and these are the

1061
01:00:54,499 --> 01:00:56,870
closed-form updates and these are the references and from this paper or taken

1062
01:00:56,870 --> 01:01:00,589
references and from this paper or taken the like the main people that have

1063
01:01:00,589 --> 01:01:03,829
the like the main people that have effort and further to obtain a of theta

1064
01:01:03,829 --> 01:01:07,069
effort and further to obtain a of theta I hope that theta the upper bound refers

1065
01:01:07,069 --> 01:01:09,859
I hope that theta the upper bound refers for this paper and this paper gives a

1066
01:01:09,859 --> 01:01:12,920
for this paper and this paper gives a brief explanation of early how do we go

1067
01:01:12,920 --> 01:02:12,880
brief explanation of early how do we go about of using the approximately

1068
01:02:12,880 --> 01:02:12,890


1069
01:02:12,890 --> 01:02:16,950
Oh

1070
01:02:16,950 --> 01:02:16,960


1071
01:02:16,960 --> 01:02:24,480
so hi I am a wishing that from the physics department the generation of the

1072
01:02:24,480 --> 01:02:28,470
physics department the generation of the handwritten like image using variational

1073
01:02:28,470 --> 01:02:31,470
handwritten like image using variational auto encoder it has been trained on the

1074
01:02:31,470 --> 01:02:37,430
auto encoder it has been trained on the eminence to dataset the outline is that

1075
01:02:37,430 --> 01:02:39,660
eminence to dataset the outline is that introduction a little bit about

1076
01:02:39,660 --> 01:02:43,109
introduction a little bit about missionaries which are like standard we

1077
01:02:43,109 --> 01:02:44,940
missionaries which are like standard we start with scanner auto encoder we

1078
01:02:44,940 --> 01:02:47,450
start with scanner auto encoder we introduced variation on experience

1079
01:02:47,450 --> 01:02:49,380
introduced variation on experience variational outside color -

1080
01:02:49,380 --> 01:02:52,890
variational outside color - normalization and the resultant clock so

1081
01:02:52,890 --> 01:02:55,109
normalization and the resultant clock so I'll be talking about a little bit in

1082
01:02:55,109 --> 01:02:57,930
I'll be talking about a little bit in little about this machineries because I

1083
01:02:57,930 --> 01:03:01,140
little about this machineries because I learned about them while starting after

1084
01:03:01,140 --> 01:03:06,349
learned about them while starting after starting working on this project itself

1085
01:03:06,349 --> 01:03:06,359


1086
01:03:06,359 --> 01:03:15,140
so an autoencoder network is a pair of true connected networks

1087
01:03:15,140 --> 01:03:15,150


1088
01:03:15,150 --> 01:03:25,020
the encoder component makes and gains the representation of the original data

1089
01:03:25,020 --> 01:03:30,030
the representation of the original data Angele la or grid of pixels and then we

1090
01:03:30,030 --> 01:03:32,039
Angele la or grid of pixels and then we call that n color information and the

1091
01:03:32,039 --> 01:03:35,160
call that n color information and the decoder part decodes it back to the

1092
01:03:35,160 --> 01:03:37,950
decoder part decodes it back to the original dimension usually higher than

1093
01:03:37,950 --> 01:03:42,799
original dimension usually higher than the dimensional representation like this

1094
01:03:42,799 --> 01:03:48,330
the dimensional representation like this this is the description

1095
01:03:48,330 --> 01:03:48,340


1096
01:03:48,340 --> 01:03:55,100
so in general the output of the decoder network would not match with the input

1097
01:03:55,100 --> 01:04:42,150
network would not match with the input using the input data point like new data

1098
01:04:42,150 --> 01:04:44,130
using the input data point like new data points then it tells because it won't

1099
01:04:44,130 --> 01:04:47,880
points then it tells because it won't it can only give must output replicating

1100
01:04:47,880 --> 01:04:52,140
it can only give must output replicating the denser representation of our

1101
01:04:52,140 --> 01:04:54,840
the denser representation of our original input so it cannot create new

1102
01:04:54,840 --> 01:04:58,350
original input so it cannot create new things so it's that's that's where

1103
01:04:58,350 --> 01:05:04,500
things so it's that's that's where standard auto encoder slack so these are

1104
01:05:04,500 --> 01:05:11,010
standard auto encoder slack so these are kind of an example of the representation

1105
01:05:11,010 --> 01:05:14,370
kind of an example of the representation of the MST data state where we have like

1106
01:05:14,370 --> 01:05:15,720
of the MST data state where we have like twenty eighteen to twenty eighth

1107
01:05:15,720 --> 01:05:17,490
twenty eighteen to twenty eighth dimension we have converted it to the

1108
01:05:17,490 --> 01:05:20,690
dimension we have converted it to the two dimension so their distribution and

1109
01:05:20,690 --> 01:05:23,160
two dimension so their distribution and it's kind of get rich there is no

1110
01:05:23,160 --> 01:05:26,100
it's kind of get rich there is no continuity from one the representation

1111
01:05:26,100 --> 01:05:29,370
continuity from one the representation data point of one visit to the other so

1112
01:05:29,370 --> 01:05:33,840
data point of one visit to the other so we won't want that because where it's

1113
01:05:33,840 --> 01:05:35,150
we won't want that because where it's difficult to general

1114
01:05:35,150 --> 01:05:37,730
difficult to general models while the distribution is like

1115
01:05:37,730 --> 01:05:41,990
models while the distribution is like that in generative models we would not

1116
01:05:41,990 --> 01:05:44,900
that in generative models we would not want to replicate data that we used it

1117
01:05:44,900 --> 01:05:47,270
want to replicate data that we used it to train in the first place but also we

1118
01:05:47,270 --> 01:05:49,130
to train in the first place but also we want to sample data points from the

1119
01:05:49,130 --> 01:05:51,650
want to sample data points from the distribution the latent space and decode

1120
01:05:51,650 --> 01:05:55,180
distribution the latent space and decode them to form Chancellor's and the higher

1121
01:05:55,180 --> 01:06:09,050
them to form Chancellor's and the higher data points so suppose we we have a lot

1122
01:06:09,050 --> 01:06:12,230
data points so suppose we we have a lot of data points of the handwritten digits

1123
01:06:12,230 --> 01:06:16,130
of data points of the handwritten digits and we want to create a model to get the

1124
01:06:16,130 --> 01:06:17,900
and we want to create a model to get the probability distribution of life of

1125
01:06:17,900 --> 01:06:20,720
probability distribution of life of those data points now these images are

1126
01:06:20,720 --> 01:06:23,990
those data points now these images are 28 by trend age so to get a maximum

1127
01:06:23,990 --> 01:06:30,440
28 by trend age so to get a maximum likelihood would have to get a min of 28

1128
01:06:30,440 --> 01:06:34,340
likelihood would have to get a min of 28 into 28 dimension and and the covariance

1129
01:06:34,340 --> 01:06:36,200
into 28 dimension and and the covariance matrix of the similar dimensions those

1130
01:06:36,200 --> 01:06:37,640
matrix of the similar dimensions those are very computationally intractable

1131
01:06:37,640 --> 01:06:41,090
are very computationally intractable that's why the idea of auto-encoders

1132
01:06:41,090 --> 01:06:44,540
that's why the idea of auto-encoders comes into play in the first place to

1133
01:06:44,540 --> 01:06:50,240
comes into play in the first place to get a more compressed representation so

1134
01:06:50,240 --> 01:06:52,010
get a more compressed representation so from the probabilistic graphical models

1135
01:06:52,010 --> 01:07:19,880
from the probabilistic graphical models from what we can say

1136
01:07:19,880 --> 01:07:19,890


1137
01:07:19,890 --> 01:07:33,140
[Music]

1138
01:07:33,140 --> 01:07:33,150


1139
01:07:33,150 --> 01:07:41,810
in the denominator itself is very difficult to compute very expensive

1140
01:07:41,810 --> 01:07:48,359
difficult to compute very expensive that's why we use variation insurance to

1141
01:07:48,359 --> 01:07:50,580
that's why we use variation insurance to approximate the posterior with a family

1142
01:07:50,580 --> 01:07:59,670
approximate the posterior with a family of distribution which represents the

1143
01:07:59,670 --> 01:08:01,920
of distribution which represents the parameters of the distributions we are

1144
01:08:01,920 --> 01:08:04,890
parameters of the distributions we are trying to fit for example a cube or a

1145
01:08:04,890 --> 01:08:07,620
trying to fit for example a cube or a normal distribution it will mean the new

1146
01:08:07,620 --> 01:08:13,440
normal distribution it will mean the new X I and Sigma X we will use KL

1147
01:08:13,440 --> 01:08:15,960
X I and Sigma X we will use KL divergence which should quantify the

1148
01:08:15,960 --> 01:08:19,640
divergence which should quantify the information lost we try to replace the

1149
01:08:19,640 --> 01:08:31,740
information lost we try to replace the primary distribution P with lambda

1150
01:08:31,740 --> 01:08:31,750


1151
01:08:31,750 --> 01:08:41,910
for that yeah kill divergences define as the expected expectation value of the

1152
01:08:41,910 --> 01:08:49,220
the expected expectation value of the log of Q lambda over P and we can

1153
01:08:49,220 --> 01:08:52,460
log of Q lambda over P and we can express in there the subtractive swarm

1154
01:08:52,460 --> 01:08:57,660
express in there the subtractive swarm and using Bayes theorem we can express

1155
01:08:57,660 --> 01:09:05,779
and using Bayes theorem we can express it in the like a form of equation two

1156
01:09:05,779 --> 01:09:05,789


1157
01:09:05,789 --> 01:09:12,879
like here the approximate for optimal posture is clear I swore to get

1158
01:09:12,879 --> 01:09:16,879
posture is clear I swore to get approximate optimal posture we minimize

1159
01:09:16,879 --> 01:09:19,640
approximate optimal posture we minimize KL divergence with respect to lambda to

1160
01:09:19,640 --> 01:09:25,640
KL divergence with respect to lambda to get the optimized Q lambda but the

1161
01:09:25,640 --> 01:09:29,979
get the optimized Q lambda but the intractable term log of px till lost

1162
01:09:29,979 --> 01:09:32,479
intractable term log of px till lost within the expression of the KL

1163
01:09:32,479 --> 01:09:35,390
within the expression of the KL divergence like this is equal to that

1164
01:09:35,390 --> 01:09:38,200
divergence like this is equal to that and that term is very computationally

1165
01:09:38,200 --> 01:09:42,519
and that term is very computationally expensive so we want to remove that

1166
01:09:42,519 --> 01:09:47,499
expensive so we want to remove that that's why we just flip side and Express

1167
01:09:47,499 --> 01:09:51,890
that's why we just flip side and Express as this subtractive form of to

1168
01:09:51,890 --> 01:09:55,810
as this subtractive form of to expectation values plus KL divergence

1169
01:09:55,810 --> 01:09:59,450
expectation values plus KL divergence now we know that KL divergence is always

1170
01:09:59,450 --> 01:10:05,709
now we know that KL divergence is always non-negative so it's I'm sorry so its

1171
01:10:05,709 --> 01:10:10,790
non-negative so it's I'm sorry so its minimum value is 0 so to maximize this

1172
01:10:10,790 --> 01:10:14,060
minimum value is 0 so to maximize this we want to minimize this and maximum

1173
01:10:14,060 --> 01:10:20,439
we want to minimize this and maximum discharge so now this becomes a bit

1174
01:10:20,439 --> 01:10:25,260
discharge so now this becomes a bit tractable to compute

1175
01:10:25,260 --> 01:10:25,270


1176
01:10:25,270 --> 01:10:37,890
and informational insurance we use union vo checking Forrester into the Thomas

1177
01:10:37,890 --> 01:10:40,140
vo checking Forrester into the Thomas Trump's dependent only upon the single

1178
01:10:40,140 --> 01:10:42,690
Trump's dependent only upon the single data points this allows for the use of

1179
01:10:42,690 --> 01:10:44,940
data points this allows for the use of stochastic gradient descent with respect

1180
01:10:44,940 --> 01:10:46,440
stochastic gradient descent with respect to the variation of parameters lambda

1181
01:10:46,440 --> 01:10:50,130
to the variation of parameters lambda for a given data point X right the LPO

1182
01:10:50,130 --> 01:11:00,950
for a given data point X right the LPO are defined on this so long T exceeds

1183
01:11:00,950 --> 01:11:00,960


1184
01:11:00,960 --> 01:11:10,490
expectation of log of P of a given Z minus Q lambda Disney different extend

1185
01:11:10,490 --> 01:11:14,460
minus Q lambda Disney different extend teasing minus KL divergence of Q lambda

1186
01:11:14,460 --> 01:11:20,340
teasing minus KL divergence of Q lambda Z given X and P Z given X so we want to

1187
01:11:20,340 --> 01:11:23,190
Z given X and P Z given X so we want to optimize log tapes by trying to find a

1188
01:11:23,190 --> 01:11:24,990
optimize log tapes by trying to find a lower bound of the quantity inside the

1189
01:11:24,990 --> 01:11:29,550
lower bound of the quantity inside the curly bracket computationally

1190
01:11:29,550 --> 01:11:44,430
curly bracket computationally intractable maximizing this is a sample

1191
01:11:44,430 --> 01:11:48,060
intractable maximizing this is a sample PC is really in the latent space also we

1192
01:11:48,060 --> 01:11:50,220
PC is really in the latent space also we can add an advantage of obtaining closed

1193
01:11:50,220 --> 01:11:52,890
can add an advantage of obtaining closed formulation of the KL divergence which

1194
01:11:52,890 --> 01:11:56,070
formulation of the KL divergence which will see Z and Q lambda Z given X we

1195
01:11:56,070 --> 01:11:59,550
will see Z and Q lambda Z given X we check easy to the standard normal

1196
01:11:59,550 --> 01:12:01,380
check easy to the standard normal distribution with mean 0 and standard

1197
01:12:01,380 --> 01:12:07,080
distribution with mean 0 and standard deviation is 1 and 2 lambda Z given X to

1198
01:12:07,080 --> 01:12:08,189
deviation is 1 and 2 lambda Z given X to be

1199
01:12:08,189 --> 01:12:10,439
be like this also another normal

1200
01:12:10,439 --> 01:12:14,100
like this also another normal distribution here this is the covariance

1201
01:12:14,100 --> 01:12:16,229
distribution here this is the covariance matrices constraint to the diagonal

1202
01:12:16,229 --> 01:12:25,500
matrices constraint to the diagonal matrix to keep computation easy but

1203
01:12:25,500 --> 01:12:29,580
matrix to keep computation easy but final the final expression is easy if

1204
01:12:29,580 --> 01:12:31,379
final the final expression is easy if the closed form expression that we get

1205
01:12:31,379 --> 01:12:40,830
the closed form expression that we get by some algebraic manipulation is the

1206
01:12:40,830 --> 01:12:44,069
by some algebraic manipulation is the normalization which is a very useful

1207
01:12:44,069 --> 01:12:46,080
normalization which is a very useful technique introduced to improve the

1208
01:12:46,080 --> 01:12:55,339
technique introduced to improve the performance of GAE and in general vision

1209
01:12:55,339 --> 01:12:55,349


1210
01:12:55,349 --> 01:13:01,169
normalization output of f3 hidden layer is standardized over every single mini

1211
01:13:01,169 --> 01:13:03,989
is standardized over every single mini batch path in the training session it

1212
01:13:03,989 --> 01:13:09,810
batch path in the training session it will be clear suppose we have a

1213
01:13:09,810 --> 01:13:12,089
will be clear suppose we have a classifier and we want the classifier to

1214
01:13:12,089 --> 01:13:14,790
classifier and we want the classifier to classify between some images which

1215
01:13:14,790 --> 01:13:18,239
classify between some images which contents cat and which doesn't have a

1216
01:13:18,239 --> 01:13:22,620
contents cat and which doesn't have a cat in the image so our training data

1217
01:13:22,620 --> 01:13:25,830
cat in the image so our training data contents on the black cat and all other

1218
01:13:25,830 --> 01:13:29,699
contents on the black cat and all other non cat images and then will be after

1219
01:13:29,699 --> 01:13:34,259
non cat images and then will be after training we introduced cat images which

1220
01:13:34,259 --> 01:13:36,989
training we introduced cat images which are colorful cat in general and non cat

1221
01:13:36,989 --> 01:13:41,219
are colorful cat in general and non cat or transfer won't be trouble too far

1222
01:13:41,219 --> 01:13:44,089
or transfer won't be trouble too far from that good as it did for the

1223
01:13:44,089 --> 01:13:47,790
from that good as it did for the training dataset this is called this is

1224
01:13:47,790 --> 01:13:50,339
training dataset this is called this is a very fancy term called covariate shift

1225
01:13:50,339 --> 01:13:54,359
a very fancy term called covariate shift which means the a shift in the data

1226
01:13:54,359 --> 01:13:58,220
which means the a shift in the data point in its distribution

1227
01:13:58,220 --> 01:13:58,230


1228
01:13:58,230 --> 01:14:04,880
situation model

1229
01:14:04,880 --> 01:14:04,890


1230
01:14:04,890 --> 01:14:11,420
so suppose the the data parts of black cats - like this very thing the data

1231
01:14:11,420 --> 01:14:14,230
cats - like this very thing the data points and the non cats are like this

1232
01:14:14,230 --> 01:14:20,980
points and the non cats are like this black crosses so usually this poor each

1233
01:14:20,980 --> 01:14:24,560
black crosses so usually this poor each we don't use vaginal normalization our

1234
01:14:24,560 --> 01:14:29,390
we don't use vaginal normalization our model would likely learn straight line

1235
01:14:29,390 --> 01:14:31,790
model would likely learn straight line difference we should separate these data

1236
01:14:31,790 --> 01:14:35,180
difference we should separate these data points like this straight line when we

1237
01:14:35,180 --> 01:14:39,820
points like this straight line when we introduced these colorful cats these

1238
01:14:39,820 --> 01:14:42,650
introduced these colorful cats these data points of this colorful cat lines

1239
01:14:42,650 --> 01:14:46,790
data points of this colorful cat lines here which our model won't be able to

1240
01:14:46,790 --> 01:14:50,630
here which our model won't be able to classify it correctly because now they

1241
01:14:50,630 --> 01:14:53,360
classify it correctly because now they have only joined a straight line which

1242
01:14:53,360 --> 01:14:56,710
have only joined a straight line which clearly don't classify them accurately

1243
01:14:56,710 --> 01:15:02,120
clearly don't classify them accurately that so that's why we want to we want to

1244
01:15:02,120 --> 01:15:05,360
that so that's why we want to we want to use batch normalization so that even if

1245
01:15:05,360 --> 01:15:08,350
use batch normalization so that even if our data points to vary by some amount

1246
01:15:08,350 --> 01:15:14,360
our data points to vary by some amount if normalizes it back and our our model

1247
01:15:14,360 --> 01:15:18,640
if normalizes it back and our our model learns faster learns more reliable

1248
01:15:18,640 --> 01:15:23,910
learns faster learns more reliable denser representation

1249
01:15:23,910 --> 01:15:23,920


1250
01:15:23,920 --> 01:15:30,650
these are some advantages of using that's normalization which makes our

1251
01:15:30,650 --> 01:15:34,530
that's normalization which makes our model to launch faster makes it converge

1252
01:15:34,530 --> 01:15:38,490
model to launch faster makes it converge better in a better value and in a faster

1253
01:15:38,490 --> 01:15:41,220
better in a better value and in a faster manner while each training iteration

1254
01:15:41,220 --> 01:15:42,959
manner while each training iteration will become somewhat skewer because the

1255
01:15:42,959 --> 01:15:45,720
will become somewhat skewer because the externalization calculations in the

1256
01:15:45,720 --> 01:15:47,370
externalization calculations in the forward path but eventually it would

1257
01:15:47,370 --> 01:15:49,800
forward path but eventually it would converge quickly to overall combination

1258
01:15:49,800 --> 01:15:55,169
converge quickly to overall combination because pockets these are the I used for

1259
01:15:55,169 --> 01:15:57,030
because pockets these are the I used for layer deep fully connected Network as

1260
01:15:57,030 --> 01:15:59,490
layer deep fully connected Network as both encoder and decoder salsa eminence

1261
01:15:59,490 --> 01:16:04,260
both encoder and decoder salsa eminence together set I take one more minute this

1262
01:16:04,260 --> 01:16:10,100
together set I take one more minute this is the

1263
01:16:10,100 --> 01:16:10,110


1264
01:16:10,110 --> 01:16:17,320
yeah so this is the convergence of lost function without using rationalization

1265
01:16:17,320 --> 01:16:20,870
function without using rationalization and this is the generated data set

1266
01:16:20,870 --> 01:16:26,300
and this is the generated data set without rationalization next I am used

1267
01:16:26,300 --> 01:16:29,390
without rationalization next I am used batch normalization and the it converges

1268
01:16:29,390 --> 01:16:36,040
batch normalization and the it converges to a better value which is a bit more

1269
01:16:36,040 --> 01:16:39,130
to a better value which is a bit more cleaner

1270
01:16:39,130 --> 01:16:39,140


1271
01:16:39,140 --> 01:16:44,980
these are using the four four dimensions for the Latin twist instead of two with

1272
01:16:44,980 --> 01:16:50,300
for the Latin twist instead of two with without BN this is again it using BM so

1273
01:16:50,300 --> 01:17:07,630
without BN this is again it using BM so you can see that the loss function

1274
01:17:07,630 --> 01:17:07,640


1275
01:17:07,640 --> 01:17:13,040
visitor references

1276
01:17:13,040 --> 01:17:13,050


1277
01:17:13,050 --> 01:17:16,409
  [Applause] 

