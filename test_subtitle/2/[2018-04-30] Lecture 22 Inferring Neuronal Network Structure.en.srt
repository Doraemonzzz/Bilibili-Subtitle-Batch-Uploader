1
00:02:01,530 --> 00:02:06,260
okay hello everyone my name is Cody Baker and a third year student in the

2
00:02:06,260 --> 00:02:07,850
Baker and a third year student in the Department of a fighting computational

3
00:02:07,850 --> 00:02:10,460
Department of a fighting computational mathematics and statistics and I study

4
00:02:10,460 --> 00:02:12,470
mathematics and statistics and I study in computational neuroscience and

5
00:02:12,470 --> 00:02:14,180
in computational neuroscience and particular the problem of inferring

6
00:02:14,180 --> 00:02:17,180
particular the problem of inferring neuronal network structure specifically

7
00:02:17,180 --> 00:02:19,130
neuronal network structure specifically with data coming from extracellular

8
00:02:19,130 --> 00:02:21,770
with data coming from extracellular recording techniques so before we start

9
00:02:21,770 --> 00:02:23,060
recording techniques so before we start to talk about results we also do a

10
00:02:23,060 --> 00:02:25,310
to talk about results we also do a little bit of background in neuroscience

11
00:02:25,310 --> 00:02:28,400
little bit of background in neuroscience and the models that we use now you don't

12
00:02:28,400 --> 00:02:29,990
and the models that we use now you don't have to know too much but you do have to

13
00:02:29,990 --> 00:02:31,070
have to know too much but you do have to know that the brain is an organ

14
00:02:31,070 --> 00:02:34,520
know that the brain is an organ comprised of very large numbers of

15
00:02:34,520 --> 00:02:36,890
comprised of very large numbers of individual computing unit cells known as

16
00:02:36,890 --> 00:02:41,420
individual computing unit cells known as neurons and neurons are able to connect

17
00:02:41,420 --> 00:02:44,900
neurons and neurons are able to connect to one another by way of a synapse which

18
00:02:44,900 --> 00:02:47,680
to one another by way of a synapse which is when the axon extension of one neuron

19
00:02:47,680 --> 00:02:51,190
is when the axon extension of one neuron attaches to the dendrites of another and

20
00:02:51,190 --> 00:02:54,949
attaches to the dendrites of another and they share information across the

21
00:02:54,949 --> 00:02:56,780
they share information across the synapse by way of electrochemical

22
00:02:56,780 --> 00:02:59,570
synapse by way of electrochemical impulses known as spikes these are brief

23
00:02:59,570 --> 00:03:02,600
impulses known as spikes these are brief transients observable via voltage traces

24
00:03:02,600 --> 00:03:05,750
transients observable via voltage traces from the inside of a neuron you see the

25
00:03:05,750 --> 00:03:09,440
from the inside of a neuron you see the spikes are the two big events there and

26
00:03:09,440 --> 00:03:11,150
spikes are the two big events there and they decay very quickly Dunn's back to

27
00:03:11,150 --> 00:03:13,330
they decay very quickly Dunn's back to the resting potential

28
00:03:13,330 --> 00:03:16,280
the resting potential now neurons come in two types excitatory

29
00:03:16,280 --> 00:03:20,060
now neurons come in two types excitatory and inhibitory the important difference

30
00:03:20,060 --> 00:03:22,789
and inhibitory the important difference is that the effect of spiking and a

31
00:03:22,789 --> 00:03:25,910
is that the effect of spiking and a presynaptic cell can either cause more

32
00:03:25,910 --> 00:03:27,890
presynaptic cell can either cause more or less light in the postsynaptic cell

33
00:03:27,890 --> 00:03:31,970
or less light in the postsynaptic cell and intelligent behavior people like you

34
00:03:31,970 --> 00:03:35,780
and intelligent behavior people like you and me are thought to arise by complex

35
00:03:35,780 --> 00:03:37,970
and me are thought to arise by complex very complex interactions of spiking

36
00:03:37,970 --> 00:03:40,640
very complex interactions of spiking events in very large networks now there

37
00:03:40,640 --> 00:03:42,410
events in very large networks now there are a lot of different computational

38
00:03:42,410 --> 00:03:45,860
are a lot of different computational theories that can explain very specific

39
00:03:45,860 --> 00:03:47,810
theories that can explain very specific tasks such as memory visual

40
00:03:47,810 --> 00:03:50,690
tasks such as memory visual classification District in making go on

41
00:03:50,690 --> 00:03:54,800
classification District in making go on but we don't have enough real data to

42
00:03:54,800 --> 00:03:57,650
but we don't have enough real data to confirm that of actual brains follow any

43
00:03:57,650 --> 00:04:01,310
confirm that of actual brains follow any specific theory and a big part of this

44
00:04:01,310 --> 00:04:03,050
specific theory and a big part of this is because in the past people had been

45
00:04:03,050 --> 00:04:04,740
is because in the past people had been able to report from my hands

46
00:04:04,740 --> 00:04:08,340
able to report from my hands neurons simultaneously at a time there

47
00:04:08,340 --> 00:04:10,590
neurons simultaneously at a time there recently and this has been developing

48
00:04:10,590 --> 00:04:12,480
recently and this has been developing almost analogous to Moore's law every

49
00:04:12,480 --> 00:04:15,000
almost analogous to Moore's law every few years we find a way to record from

50
00:04:15,000 --> 00:04:18,569
few years we find a way to record from twice as many neurons as before in

51
00:04:18,569 --> 00:04:20,009
twice as many neurons as before in recent years we have developed

52
00:04:20,009 --> 00:04:22,500
recent years we have developed techniques that can record from hundreds

53
00:04:22,500 --> 00:04:24,240
techniques that can record from hundreds thousands I think now they're starting

54
00:04:24,240 --> 00:04:26,790
thousands I think now they're starting to push tens of thousands and we're

55
00:04:26,790 --> 00:04:28,740
to push tens of thousands and we're going to focus on one in particular on

56
00:04:28,740 --> 00:04:31,170
going to focus on one in particular on that that called calcium imaging now

57
00:04:31,170 --> 00:04:33,180
that that called calcium imaging now active imaging you have a central laser

58
00:04:33,180 --> 00:04:36,030
active imaging you have a central laser hub is gaining gaining some cubic volume

59
00:04:36,030 --> 00:04:40,250
hub is gaining gaining some cubic volume of neural space and a whole bunch of

60
00:04:40,250 --> 00:04:43,860
of neural space and a whole bunch of post processing machine learning based

61
00:04:43,860 --> 00:04:45,300
post processing machine learning based techniques you can identify the location

62
00:04:45,300 --> 00:04:48,800
techniques you can identify the location in space of every individual neuron and

63
00:04:48,800 --> 00:04:53,040
in space of every individual neuron and then you can extract a measure of

64
00:04:53,040 --> 00:04:55,740
then you can extract a measure of fluorescence that indicates how active

65
00:04:55,740 --> 00:04:58,050
fluorescence that indicates how active and individual neuron is and this is

66
00:04:58,050 --> 00:05:00,060
and individual neuron is and this is actual real data you're looking at here

67
00:05:00,060 --> 00:05:03,120
actual real data you're looking at here and you can see some neurons are spiking

68
00:05:03,120 --> 00:05:05,190
and you can see some neurons are spiking very frequently that's like that one

69
00:05:05,190 --> 00:05:08,550
very frequently that's like that one that's always colored Thurmont spiking

70
00:05:08,550 --> 00:05:10,260
that's always colored Thurmont spiking at all and others are going very

71
00:05:10,260 --> 00:05:13,020
at all and others are going very sporadically according to what's on top

72
00:05:13,020 --> 00:05:20,100
sporadically according to what's on top of these and so in the context of the

73
00:05:20,100 --> 00:05:21,719
of these and so in the context of the data what does it look like it looks

74
00:05:21,719 --> 00:05:24,840
data what does it look like it looks like a bunch of calcium sorry it looks

75
00:05:24,840 --> 00:05:27,750
like a bunch of calcium sorry it looks like a bunch of Gaussian Prophecy you

76
00:05:27,750 --> 00:05:29,700
like a bunch of Gaussian Prophecy you get traces of relative fluorescence for

77
00:05:29,700 --> 00:05:34,310
get traces of relative fluorescence for every pixel in the face these are the

78
00:05:34,310 --> 00:05:36,370
every pixel in the face these are the cameras

79
00:05:36,370 --> 00:05:45,690
cameras constant optical beam splitter you

80
00:05:45,690 --> 00:05:45,700


81
00:05:45,700 --> 00:05:52,330
should automatically learn to the next thing to follow that video so I my goal

82
00:05:52,330 --> 00:05:56,200
thing to follow that video so I my goal here is to look at these traces as

83
00:05:56,200 --> 00:05:58,660
here is to look at these traces as Gaussian processes and to try to guess

84
00:05:58,660 --> 00:06:00,790
Gaussian processes and to try to guess which neurons are connected to one

85
00:06:00,790 --> 00:06:04,060
which neurons are connected to one another and we can statistically maulvis

86
00:06:04,060 --> 00:06:05,710
another and we can statistically maulvis data in a number of different ways we

87
00:06:05,710 --> 00:06:07,510
data in a number of different ways we can use stochastic differential equation

88
00:06:07,510 --> 00:06:09,430
can use stochastic differential equation for linear dynamical systems we talked

89
00:06:09,430 --> 00:06:12,370
for linear dynamical systems we talked about in class general auto regressive

90
00:06:12,370 --> 00:06:14,560
about in class general auto regressive processes or any sort of time series

91
00:06:14,560 --> 00:06:18,550
processes or any sort of time series model and since we're trying to go a

92
00:06:18,550 --> 00:06:19,780
model and since we're trying to go a little faster today I probably won't

93
00:06:19,780 --> 00:06:22,230
little faster today I probably won't talk too much about the gel on models

94
00:06:22,230 --> 00:06:25,000
talk too much about the gel on models okay so in the context of everything

95
00:06:25,000 --> 00:06:26,320
okay so in the context of everything we're going to talk about today we're

96
00:06:26,320 --> 00:06:28,030
we're going to talk about today we're always going to be referencing an

97
00:06:28,030 --> 00:06:31,000
always going to be referencing an adjacency matrix or our probabilistic

98
00:06:31,000 --> 00:06:33,840
adjacency matrix or our probabilistic graphical models shown by this network

99
00:06:33,840 --> 00:06:36,850
graphical models shown by this network now it is decomposable into two distinct

100
00:06:36,850 --> 00:06:40,540
now it is decomposable into two distinct parts it has a weighted part that is

101
00:06:40,540 --> 00:06:43,540
parts it has a weighted part that is random and normally distributed and it

102
00:06:43,540 --> 00:06:45,910
random and normally distributed and it has a structure driven by what's called

103
00:06:45,910 --> 00:06:47,680
has a structure driven by what's called further journey which has means it's

104
00:06:47,680 --> 00:06:50,410
further journey which has means it's random binary at some level it's party

105
00:06:50,410 --> 00:06:53,950
random binary at some level it's party so this is an example adjacency matrix

106
00:06:53,950 --> 00:06:56,680
so this is an example adjacency matrix remember it's back wise because we have

107
00:06:56,680 --> 00:07:00,550
remember it's back wise because we have excitatory and inhibitory neurons in our

108
00:07:00,550 --> 00:07:02,320
excitatory and inhibitory neurons in our model and here you continue inhibitory

109
00:07:02,320 --> 00:07:06,220
model and here you continue inhibitory ones always have negative weights so you

110
00:07:06,220 --> 00:07:08,410
ones always have negative weights so you have random winding and spar structure

111
00:07:08,410 --> 00:07:10,840
have random winding and spar structure on top of that and explicitly stating

112
00:07:10,840 --> 00:07:12,550
on top of that and explicitly stating the problem we're interested in

113
00:07:12,550 --> 00:07:15,610
the problem we're interested in differing the sparse structure Omega or

114
00:07:15,610 --> 00:07:18,070
differing the sparse structure Omega or the undirected case of Omega plus Omega

115
00:07:18,070 --> 00:07:19,950
the undirected case of Omega plus Omega transpose

116
00:07:19,950 --> 00:07:23,590
transpose and the quick note on this topic before

117
00:07:23,590 --> 00:07:26,830
and the quick note on this topic before we go on stability is a very important

118
00:07:26,830 --> 00:07:28,630
we go on stability is a very important issue with these networks and you need

119
00:07:28,630 --> 00:07:31,030
issue with these networks and you need very specific set of constraints to

120
00:07:31,030 --> 00:07:33,040
very specific set of constraints to guarantee that your system is always

121
00:07:33,040 --> 00:07:35,950
guarantee that your system is always stable and you do that by applying a 1

122
00:07:35,950 --> 00:07:39,030
stable and you do that by applying a 1 over square root and scaling on the

123
00:07:39,030 --> 00:07:42,160
over square root and scaling on the normally distributed snap express that

124
00:07:42,160 --> 00:07:44,110
normally distributed snap express that constrains a vast majority of the

125
00:07:44,110 --> 00:07:47,590
constrains a vast majority of the eigenvalues to the unit circle and but

126
00:07:47,590 --> 00:07:48,910
eigenvalues to the unit circle and but then you'll notice you have these two

127
00:07:48,910 --> 00:07:50,800
then you'll notice you have these two big eigen values that are conjugate

128
00:07:50,800 --> 00:07:53,260
big eigen values that are conjugate pairs those are related to the Kurdish

129
00:07:53,260 --> 00:07:55,750
pairs those are related to the Kurdish for any structure the sparsity and those

130
00:07:55,750 --> 00:07:59,740
for any structure the sparsity and those you can keep to the right simply by

131
00:07:59,740 --> 00:08:01,840
you can keep to the right simply by these two jury conditions the

132
00:08:01,840 --> 00:08:03,730
these two jury conditions the determinant of the trace on the block

133
00:08:03,730 --> 00:08:06,780
determinant of the trace on the block lies average for the network and

134
00:08:06,780 --> 00:08:08,800
lies average for the network and throughout this we're going to be

135
00:08:08,800 --> 00:08:10,570
throughout this we're going to be quantifying the performance of various

136
00:08:10,570 --> 00:08:13,120
quantifying the performance of various algorithms using policy analysis and

137
00:08:13,120 --> 00:08:14,650
algorithms using policy analysis and whether than first time seeing it or

138
00:08:14,650 --> 00:08:17,470
whether than first time seeing it or just need a little refresher the ROC

139
00:08:17,470 --> 00:08:20,320
just need a little refresher the ROC curve is a parametric function of false

140
00:08:20,320 --> 00:08:22,210
curve is a parametric function of false positive rate versus true positive rate

141
00:08:22,210 --> 00:08:24,660
positive rate versus true positive rate as the threshold of a linear classifier

142
00:08:24,660 --> 00:08:28,110
as the threshold of a linear classifier is buried and so this is an example

143
00:08:28,110 --> 00:08:30,190
is buried and so this is an example Piracy curve for some stuff we're going

144
00:08:30,190 --> 00:08:32,290
Piracy curve for some stuff we're going to talk about in just a bit

145
00:08:32,290 --> 00:08:34,060
to talk about in just a bit and you can see you get very high true

146
00:08:34,060 --> 00:08:36,630
and you can see you get very high true positives for very low false positives

147
00:08:36,630 --> 00:08:39,790
positives for very low false positives but overall we will often be interested

148
00:08:39,790 --> 00:08:41,950
but overall we will often be interested only in the area under this curve which

149
00:08:41,950 --> 00:08:44,890
only in the area under this curve which is a nice talented measure between 1/2

150
00:08:44,890 --> 00:08:47,470
is a nice talented measure between 1/2 and 1 corresponding to just random

151
00:08:47,470 --> 00:08:49,600
and 1 corresponding to just random guessing and perfect classification

152
00:08:49,600 --> 00:08:52,960
guessing and perfect classification respectively okay so the case model

153
00:08:52,960 --> 00:08:54,670
respectively okay so the case model we'll talk briefly about is the

154
00:08:54,670 --> 00:08:56,320
we'll talk briefly about is the stochastic differential equation which

155
00:08:56,320 --> 00:09:00,790
stochastic differential equation which is a Wilson cowan rate model which can

156
00:09:00,790 --> 00:09:04,690
is a Wilson cowan rate model which can be expressed as this equation in terms

157
00:09:04,690 --> 00:09:07,450
be expressed as this equation in terms of the stochastic differential of the

158
00:09:07,450 --> 00:09:09,520
of the stochastic differential of the white noise which is just the

159
00:09:09,520 --> 00:09:12,880
white noise which is just the differential of ear profit process but

160
00:09:12,880 --> 00:09:14,620
differential of ear profit process but if you just back up and look at it it's

161
00:09:14,620 --> 00:09:16,740
if you just back up and look at it it's really just an arrow one process in

162
00:09:16,740 --> 00:09:19,630
really just an arrow one process in continuous time so we know how to deal

163
00:09:19,630 --> 00:09:20,770
continuous time so we know how to deal with this

164
00:09:20,770 --> 00:09:23,410
with this and you know the most useful thing that

165
00:09:23,410 --> 00:09:25,690
and you know the most useful thing that we will use this equation for is the

166
00:09:25,690 --> 00:09:28,270
we will use this equation for is the fact that has analytical solutions to

167
00:09:28,270 --> 00:09:29,890
fact that has analytical solutions to stuff like the cross cross covariance

168
00:09:29,890 --> 00:09:33,700
stuff like the cross cross covariance and in particular the cross spectrum

169
00:09:33,700 --> 00:09:36,750
and in particular the cross spectrum which is the Fourier transform of that

170
00:09:36,750 --> 00:09:40,660
which is the Fourier transform of that the specific algebraic form of the zero

171
00:09:40,660 --> 00:09:42,970
the specific algebraic form of the zero frequency cross spectrum can be

172
00:09:42,970 --> 00:09:45,790
frequency cross spectrum can be explicitly expanded to give a one-to-one

173
00:09:45,790 --> 00:09:47,920
explicitly expanded to give a one-to-one relationship between bi-directional

174
00:09:47,920 --> 00:09:50,230
relationship between bi-directional connectivity and precision so this is

175
00:09:50,230 --> 00:09:53,800
connectivity and precision so this is kind of like an analogous to all the

176
00:09:53,800 --> 00:09:54,820
kind of like an analogous to all the stuff we talked about with Gaussian

177
00:09:54,820 --> 00:09:57,220
stuff we talked about with Gaussian graphical models except this is a

178
00:09:57,220 --> 00:10:01,930
graphical models except this is a process that's varying in time and so

179
00:10:01,930 --> 00:10:03,820
process that's varying in time and so you can equivalently express as a loan

180
00:10:03,820 --> 00:10:07,180
you can equivalently express as a loan process and disclose form as an LDS that

181
00:10:07,180 --> 00:10:09,430
process and disclose form as an LDS that we've seen a lot of throughout this

182
00:10:09,430 --> 00:10:13,450
we've seen a lot of throughout this class and there are a lot of different

183
00:10:13,450 --> 00:10:15,490
class and there are a lot of different sub problems related to sub ideal

184
00:10:15,490 --> 00:10:18,130
sub problems related to sub ideal observations you can have more

185
00:10:18,130 --> 00:10:19,810
observations you can have more observations than neurons in the model

186
00:10:19,810 --> 00:10:21,430
observations than neurons in the model that's a great problem to have because

187
00:10:21,430 --> 00:10:22,930
that's a great problem to have because then you have multiple samples from

188
00:10:22,930 --> 00:10:25,360
then you have multiple samples from every neuron but it's often not the case

189
00:10:25,360 --> 00:10:28,150
every neuron but it's often not the case often times you have fewer observations

190
00:10:28,150 --> 00:10:30,610
often times you have fewer observations than you do neurons in which case you

191
00:10:30,610 --> 00:10:31,690
than you do neurons in which case you pretty much have to shift the entire

192
00:10:31,690 --> 00:10:34,990
pretty much have to shift the entire argument around not talking about

193
00:10:34,990 --> 00:10:37,090
argument around not talking about inferring single neuron connectivity but

194
00:10:37,090 --> 00:10:39,550
inferring single neuron connectivity but other connectivity between multiple

195
00:10:39,550 --> 00:10:43,870
other connectivity between multiple neurons local and space I know the

196
00:10:43,870 --> 00:10:45,490
neurons local and space I know the really hard problem is you can have

197
00:10:45,490 --> 00:10:47,550
really hard problem is you can have different numbers of observations

198
00:10:47,550 --> 00:10:50,560
different numbers of observations throughout time whenever you apply

199
00:10:50,560 --> 00:10:52,690
throughout time whenever you apply calcium imaging you're applying it to a

200
00:10:52,690 --> 00:10:56,200
calcium imaging you're applying it to a live subject and so they are sometimes

201
00:10:56,200 --> 00:10:58,390
live subject and so they are sometimes anesthetized to try to minimize knowing

202
00:10:58,390 --> 00:10:59,860
anesthetized to try to minimize knowing but there's always a little bit of

203
00:10:59,860 --> 00:11:02,950
but there's always a little bit of jitter a little bit of movement and so

204
00:11:02,950 --> 00:11:05,890
jitter a little bit of movement and so you always have to be able to try to

205
00:11:05,890 --> 00:11:08,110
you always have to be able to try to identify the set of neurons that you are

206
00:11:08,110 --> 00:11:10,180
identify the set of neurons that you are somehow reporting from constantly

207
00:11:10,180 --> 00:11:11,800
somehow reporting from constantly throughout time and write together all

208
00:11:11,800 --> 00:11:15,310
throughout time and write together all the data samples from that but my focus

209
00:11:15,310 --> 00:11:16,770
the data samples from that but my focus today is on a more fundamental

210
00:11:16,770 --> 00:11:19,630
today is on a more fundamental difficulty so justr simplicity let's

211
00:11:19,630 --> 00:11:22,030
difficulty so justr simplicity let's assume we have near-perfect observations

212
00:11:22,030 --> 00:11:23,660
assume we have near-perfect observations for the LDL

213
00:11:23,660 --> 00:11:25,740
for the LDL so we're gonna do when we have a linear

214
00:11:25,740 --> 00:11:27,540
so we're gonna do when we have a linear dynamical system well the best way to

215
00:11:27,540 --> 00:11:29,630
dynamical system well the best way to get inference of parameters is

216
00:11:29,630 --> 00:11:31,920
get inference of parameters is expectation-maximization right you've

217
00:11:31,920 --> 00:11:33,600
expectation-maximization right you've run the Kalman filtering and smoothing

218
00:11:33,600 --> 00:11:35,910
run the Kalman filtering and smoothing by Muslims passing and then you get a

219
00:11:35,910 --> 00:11:38,580
by Muslims passing and then you get a rate DM algorithm on some initial

220
00:11:38,580 --> 00:11:40,470
rate DM algorithm on some initial guesses and after the few iterations

221
00:11:40,470 --> 00:11:43,170
guesses and after the few iterations it'll converge and this is an example

222
00:11:43,170 --> 00:11:46,250
it'll converge and this is an example application of it you see a true network

223
00:11:46,250 --> 00:11:48,870
application of it you see a true network there up on the left and then you have

224
00:11:48,870 --> 00:11:51,390
there up on the left and then you have an estimate given out a fixed false

225
00:11:51,390 --> 00:11:54,000
an estimate given out a fixed false positive ratio so this estimate of the

226
00:11:54,000 --> 00:11:56,490
positive ratio so this estimate of the network was chosen to be right about

227
00:11:56,490 --> 00:11:59,760
network was chosen to be right about there on the ROC curve to live there to

228
00:11:59,760 --> 00:12:03,330
there on the ROC curve to live there to have a similar level of sparsity and a

229
00:12:03,330 --> 00:12:04,890
have a similar level of sparsity and a fairly high degree of confidence that

230
00:12:04,890 --> 00:12:07,440
fairly high degree of confidence that the selected edges are in fact true

231
00:12:07,440 --> 00:12:11,010
the selected edges are in fact true edges but the most important thing here

232
00:12:11,010 --> 00:12:13,410
edges but the most important thing here is the fact that the area under the ROC

233
00:12:13,410 --> 00:12:16,260
is the fact that the area under the ROC curve monotonically increases that you

234
00:12:16,260 --> 00:12:18,510
curve monotonically increases that you feed it more and more data in near will

235
00:12:18,510 --> 00:12:21,000
feed it more and more data in near will on the scale of tens to hundreds of

236
00:12:21,000 --> 00:12:22,590
on the scale of tens to hundreds of thousands of data samples that will

237
00:12:22,590 --> 00:12:25,590
thousands of data samples that will become very important just a second and

238
00:12:25,590 --> 00:12:27,810
become very important just a second and you can also see that it converging to a

239
00:12:27,810 --> 00:12:31,920
you can also see that it converging to a perfect classifier for it these simple

240
00:12:31,920 --> 00:12:36,690
perfect classifier for it these simple networks now why is it we know that if

241
00:12:36,690 --> 00:12:39,330
networks now why is it we know that if you just have a ggm there's a standard

242
00:12:39,330 --> 00:12:42,990
you just have a ggm there's a standard time invariant graph we can explicitly

243
00:12:42,990 --> 00:12:46,320
time invariant graph we can explicitly get to a set of conditional

244
00:12:46,320 --> 00:12:47,910
get to a set of conditional independencies from the precision maker

245
00:12:47,910 --> 00:12:50,970
independencies from the precision maker we know that but the precision matrix is

246
00:12:50,970 --> 00:12:54,150
we know that but the precision matrix is just one type of a general idea that's

247
00:12:54,150 --> 00:12:56,730
just one type of a general idea that's called partial correlation and particle

248
00:12:56,730 --> 00:12:58,530
called partial correlation and particle relations are really just pairwise

249
00:12:58,530 --> 00:13:02,100
relations are really just pairwise correlations conditioned on from set of

250
00:13:02,100 --> 00:13:05,810
correlations conditioned on from set of the network and precision is in fact the

251
00:13:05,810 --> 00:13:08,400
the network and precision is in fact the largest type of partial correlation in

252
00:13:08,400 --> 00:13:10,860
largest type of partial correlation in the sense of you condition on everything

253
00:13:10,860 --> 00:13:13,080
the sense of you condition on everything else on the network besides the parity

254
00:13:13,080 --> 00:13:15,360
else on the network besides the parity that you're looking at but expectation

255
00:13:15,360 --> 00:13:18,060
that you're looking at but expectation maximization can also be viewed as a

256
00:13:18,060 --> 00:13:20,250
maximization can also be viewed as a type of partial correlation if you look

257
00:13:20,250 --> 00:13:22,080
type of partial correlation if you look at the values we derived in homework 5

258
00:13:22,080 --> 00:13:26,150
at the values we derived in homework 5 or in the equivalent common estimate

259
00:13:26,150 --> 00:13:29,110
or in the equivalent common estimate you can see you have quantities that are

260
00:13:29,110 --> 00:13:32,720
you can see you have quantities that are essentially proportional to the 0 large

261
00:13:32,720 --> 00:13:35,540
essentially proportional to the 0 large sample per student being projected in

262
00:13:35,540 --> 00:13:37,100
sample per student being projected in the direction of the live 1 cross

263
00:13:37,100 --> 00:13:39,949
the direction of the live 1 cross covariance because remember eeehm gives

264
00:13:39,949 --> 00:13:43,160
covariance because remember eeehm gives us a directed measure whereas precision

265
00:13:43,160 --> 00:13:45,889
us a directed measure whereas precision is a symmetric value you have to impose

266
00:13:45,889 --> 00:13:48,079
is a symmetric value you have to impose some sort of directionality in some way

267
00:13:48,079 --> 00:13:49,369
some sort of directionality in some way and there are a lot of other different

268
00:13:49,369 --> 00:13:51,619
and there are a lot of other different methods but I thought that was kind of

269
00:13:51,619 --> 00:13:53,869
methods but I thought that was kind of interesting that p.m. is really just a

270
00:13:53,869 --> 00:13:59,360
interesting that p.m. is really just a special case of correlation okay we

271
00:13:59,360 --> 00:14:01,819
special case of correlation okay we talked about directly estimating the

272
00:14:01,819 --> 00:14:03,619
talked about directly estimating the precision instead of doing the common

273
00:14:03,619 --> 00:14:07,579
precision instead of doing the common stuff a big issue with empirical data is

274
00:14:07,579 --> 00:14:10,480
stuff a big issue with empirical data is you have very short duration of trials

275
00:14:10,480 --> 00:14:12,889
you have very short duration of trials typically in the order of seconds to

276
00:14:12,889 --> 00:14:16,429
typically in the order of seconds to minutes if that aware is the plot that I

277
00:14:16,429 --> 00:14:17,679
minutes if that aware is the plot that I was just showing you where

278
00:14:17,679 --> 00:14:19,910
was just showing you where expectation-maximization converged to a

279
00:14:19,910 --> 00:14:22,550
expectation-maximization converged to a really good area under the ROC curve you

280
00:14:22,550 --> 00:14:24,319
really good area under the ROC curve you needed hundreds of thousands of data

281
00:14:24,319 --> 00:14:27,199
needed hundreds of thousands of data samples that would equate to having you

282
00:14:27,199 --> 00:14:28,910
samples that would equate to having you know trials that lasted for several

283
00:14:28,910 --> 00:14:32,509
know trials that lasted for several hours now not just impractical no living

284
00:14:32,509 --> 00:14:33,980
hours now not just impractical no living subject wants to sit there and watching

285
00:14:33,980 --> 00:14:35,870
subject wants to sit there and watching movies with three hours these typically

286
00:14:35,870 --> 00:14:40,550
movies with three hours these typically might have to deal with so we have very

287
00:14:40,550 --> 00:14:42,650
might have to deal with so we have very little data and so to make better use of

288
00:14:42,650 --> 00:14:44,900
little data and so to make better use of what little data we have we will use the

289
00:14:44,900 --> 00:14:49,699
what little data we have we will use the buzzword of regularization and so the

290
00:14:49,699 --> 00:14:51,740
buzzword of regularization and so the beijing of perspective regularization is

291
00:14:51,740 --> 00:14:53,960
beijing of perspective regularization is really just like applying a prior

292
00:14:53,960 --> 00:14:55,939
really just like applying a prior distribution to the coefficients for

293
00:14:55,939 --> 00:14:58,610
distribution to the coefficients for some regression model and concerning the

294
00:14:58,610 --> 00:15:01,160
some regression model and concerning the model in that way really makes better

295
00:15:01,160 --> 00:15:04,040
model in that way really makes better use of what data you have they it gives

296
00:15:04,040 --> 00:15:06,910
use of what data you have they it gives you a higher-quality estimate for

297
00:15:06,910 --> 00:15:10,429
you a higher-quality estimate for comfortable amounts of data and so the

298
00:15:10,429 --> 00:15:12,079
comfortable amounts of data and so the specific estimation approach for

299
00:15:12,079 --> 00:15:14,600
specific estimation approach for proceeding that I will use is graphical

300
00:15:14,600 --> 00:15:17,120
proceeding that I will use is graphical lasso a very popular approach very

301
00:15:17,120 --> 00:15:20,389
lasso a very popular approach very popular algorithm and if the solution of

302
00:15:20,389 --> 00:15:23,299
popular algorithm and if the solution of this optimization equation where you'll

303
00:15:23,299 --> 00:15:27,230
this optimization equation where you'll notice here we're using an l1 norm on

304
00:15:27,230 --> 00:15:29,500
notice here we're using an l1 norm on our regularization that is

305
00:15:29,500 --> 00:15:32,650
our regularization that is like imposing a sparse flyer on our

306
00:15:32,650 --> 00:15:36,880
like imposing a sparse flyer on our precision matrix and the regularization

307
00:15:36,880 --> 00:15:39,040
precision matrix and the regularization strength Ram that can be chosen a number

308
00:15:39,040 --> 00:15:42,670
strength Ram that can be chosen a number of different ways the simplest way is to

309
00:15:42,670 --> 00:15:44,410
of different ways the simplest way is to cross validated against target bono

310
00:15:44,410 --> 00:15:46,720
cross validated against target bono sparsity the far more interesting

311
00:15:46,720 --> 00:15:48,040
sparsity the far more interesting approach is to use something called the

312
00:15:48,040 --> 00:15:50,650
approach is to use something called the Stars algorithm which uses all sorts of

313
00:15:50,650 --> 00:15:54,220
Stars algorithm which uses all sorts of measures like kitv icy sports-based

314
00:15:54,220 --> 00:15:57,010
measures like kitv icy sports-based criterion and it runs all these to

315
00:15:57,010 --> 00:16:00,100
criterion and it runs all these to various iterations to kind of produce a

316
00:16:00,100 --> 00:16:05,620
various iterations to kind of produce a target level of expected sparsity and so

317
00:16:05,620 --> 00:16:08,200
target level of expected sparsity and so if you a pie graph Galasso and in

318
00:16:08,200 --> 00:16:09,670
if you a pie graph Galasso and in combination with a number of other

319
00:16:09,670 --> 00:16:12,430
combination with a number of other measures you'll notice that by far it

320
00:16:12,430 --> 00:16:15,340
measures you'll notice that by far it always surpasses every other measure for

321
00:16:15,340 --> 00:16:17,230
always surpasses every other measure for in terms of structure in these types of

322
00:16:17,230 --> 00:16:19,750
in terms of structure in these types of networks and here are notice the

323
00:16:19,750 --> 00:16:21,610
networks and here are notice the difference in the scale of T previously

324
00:16:21,610 --> 00:16:24,040
difference in the scale of T previously we were talking about tens hundreds of

325
00:16:24,040 --> 00:16:25,300
we were talking about tens hundreds of thousands of data points now we're

326
00:16:25,300 --> 00:16:27,550
thousands of data points now we're talking about hundreds which is why our

327
00:16:27,550 --> 00:16:30,190
talking about hundreds which is why our you know it recovery isn't that great

328
00:16:30,190 --> 00:16:32,560
you know it recovery isn't that great but the point is still that crossover

329
00:16:32,560 --> 00:16:35,200
but the point is still that crossover does a far better job than any other

330
00:16:35,200 --> 00:16:38,710
does a far better job than any other algorithm including the naive estimate

331
00:16:38,710 --> 00:16:40,150
algorithm including the naive estimate of precision which is just the inverse

332
00:16:40,150 --> 00:16:44,020
of precision which is just the inverse of sample covariance okay but there's a

333
00:16:44,020 --> 00:16:46,120
of sample covariance okay but there's a big unrealistic assumption that we've

334
00:16:46,120 --> 00:16:48,580
big unrealistic assumption that we've made throughout all the models so far

335
00:16:48,580 --> 00:16:52,000
made throughout all the models so far and that's a lack of external input now

336
00:16:52,000 --> 00:16:56,230
and that's a lack of external input now in in empirical studies you never have

337
00:16:56,230 --> 00:16:58,480
in in empirical studies you never have observations of these things and so they

338
00:16:58,480 --> 00:17:00,850
observations of these things and so they take on the role of latent variables

339
00:17:00,850 --> 00:17:03,160
take on the role of latent variables which makes them very difficult to deal

340
00:17:03,160 --> 00:17:05,710
which makes them very difficult to deal with and the way to interpret the

341
00:17:05,710 --> 00:17:08,050
with and the way to interpret the external inputs is typically in cortex

342
00:17:08,050 --> 00:17:11,829
external inputs is typically in cortex you have multiple recurrent layers where

343
00:17:11,829 --> 00:17:15,160
you have multiple recurrent layers where one subject's feed-forward offers

344
00:17:15,160 --> 00:17:19,370
one subject's feed-forward offers heapsort projections onto the other

345
00:17:19,370 --> 00:17:21,769
heapsort projections onto the other and typically it's only excitatory

346
00:17:21,769 --> 00:17:25,730
and typically it's only excitatory neurons that do that as well so they

347
00:17:25,730 --> 00:17:27,529
neurons that do that as well so they take on the role of latent variables and

348
00:17:27,529 --> 00:17:29,450
take on the role of latent variables and so you can update the FTEs and

349
00:17:29,450 --> 00:17:31,700
so you can update the FTEs and transition models as follows you add a

350
00:17:31,700 --> 00:17:34,970
transition models as follows you add a mean drive from some external adjacent

351
00:17:34,970 --> 00:17:36,860
mean drive from some external adjacent we made church so it's very similar in

352
00:17:36,860 --> 00:17:40,070
we made church so it's very similar in structure to the recurrent but typically

353
00:17:40,070 --> 00:17:41,840
structure to the recurrent but typically rectangular could you typically have

354
00:17:41,840 --> 00:17:45,590
rectangular could you typically have more or less number of neurons and then

355
00:17:45,590 --> 00:17:49,539
more or less number of neurons and then you also now have correlated noise terms

356
00:17:49,539 --> 00:17:52,700
you also now have correlated noise terms where B is the trusty decomposition of

357
00:17:52,700 --> 00:17:56,840
where B is the trusty decomposition of the external covariance matrix gamma and

358
00:17:56,840 --> 00:17:59,990
the external covariance matrix gamma and so there's been some proposals on the

359
00:17:59,990 --> 00:18:02,299
so there's been some proposals on the literature that you can at least in the

360
00:18:02,299 --> 00:18:04,850
literature that you can at least in the case of low rank external input where

361
00:18:04,850 --> 00:18:07,039
case of low rank external input where you just have a handful of neurons in

362
00:18:07,039 --> 00:18:09,350
you just have a handful of neurons in the external population there there's

363
00:18:09,350 --> 00:18:10,999
the external population there there's been proposed that you can use Gaussian

364
00:18:10,999 --> 00:18:14,419
been proposed that you can use Gaussian process factor analysis to fit those

365
00:18:14,419 --> 00:18:17,269
process factor analysis to fit those lighting dimensions and that should

366
00:18:17,269 --> 00:18:18,919
lighting dimensions and that should improve your estimate unfortunately I

367
00:18:18,919 --> 00:18:21,049
improve your estimate unfortunately I tried and tried and tried to get this to

368
00:18:21,049 --> 00:18:23,749
tried and tried and tried to get this to work unfortunately it appears is it

369
00:18:23,749 --> 00:18:27,289
work unfortunately it appears is it using GP FA over fits the process if

370
00:18:27,289 --> 00:18:29,690
using GP FA over fits the process if it's not only the latent variables but

371
00:18:29,690 --> 00:18:31,759
it's not only the latent variables but some of the valuable information from

372
00:18:31,759 --> 00:18:35,180
some of the valuable information from the stochastic process itself and so he

373
00:18:35,180 --> 00:18:38,860
the stochastic process itself and so he written C Grasso used on the residuals

374
00:18:38,860 --> 00:18:42,169
written C Grasso used on the residuals before and after an application of GP FA

375
00:18:42,169 --> 00:18:45,169
before and after an application of GP FA and the overfitting from GP FA causes it

376
00:18:45,169 --> 00:18:50,690
and the overfitting from GP FA causes it to decay a little lower it's also very

377
00:18:50,690 --> 00:18:53,379
to decay a little lower it's also very very slow algorithms for larger networks

378
00:18:53,379 --> 00:18:56,389
very slow algorithms for larger networks so it's not considered practical

379
00:18:56,389 --> 00:18:58,820
so it's not considered practical currently

380
00:18:58,820 --> 00:19:02,450
currently okay and a quick note the problem at

381
00:19:02,450 --> 00:19:05,060
okay and a quick note the problem at least of having a mean constant rise can

382
00:19:05,060 --> 00:19:06,560
least of having a mean constant rise can be solved in the context of a linear

383
00:19:06,560 --> 00:19:09,860
be solved in the context of a linear dynamical system this is a Homer problem

384
00:19:09,860 --> 00:19:12,010
dynamical system this is a Homer problem and vicious machine learning books

385
00:19:12,010 --> 00:19:15,590
and vicious machine learning books section 13 basically you just add some

386
00:19:15,590 --> 00:19:18,770
section 13 basically you just add some additional rows to your hidden States

387
00:19:18,770 --> 00:19:21,440
additional rows to your hidden States fix them at unity now you can absorb

388
00:19:21,440 --> 00:19:25,270
fix them at unity now you can absorb additional columns as the main input

389
00:19:25,270 --> 00:19:28,820
additional columns as the main input onto your new network W and that

390
00:19:28,820 --> 00:19:31,520
onto your new network W and that districts the whole problem but your ovm

391
00:19:31,520 --> 00:19:34,570
districts the whole problem but your ovm estimates would still be perfectly fine

392
00:19:34,570 --> 00:19:37,940
estimates would still be perfectly fine unfortunately it doesn't appear to work

393
00:19:37,940 --> 00:19:40,880
unfortunately it doesn't appear to work either and we'll explain why in just a

394
00:19:40,880 --> 00:19:42,650
either and we'll explain why in just a second okay we're back to the level

395
00:19:42,650 --> 00:19:44,930
second okay we're back to the level where we have tens of thousands of data

396
00:19:44,930 --> 00:19:46,520
where we have tens of thousands of data samples so they should be doing better

397
00:19:46,520 --> 00:19:49,040
samples so they should be doing better right but they're not this is in the

398
00:19:49,040 --> 00:19:51,350
right but they're not this is in the presence of external input Hackney has a

399
00:19:51,350 --> 00:19:55,250
presence of external input Hackney has a latent variables and so why is not a

400
00:19:55,250 --> 00:19:57,950
latent variables and so why is not a single measure giving us good influence

401
00:19:57,950 --> 00:20:02,560
single measure giving us good influence it's because we are now receiving biased

402
00:20:02,560 --> 00:20:05,450
it's because we are now receiving biased external input covariance that in fact

403
00:20:05,450 --> 00:20:08,210
external input covariance that in fact shares terms with the main input drive

404
00:20:08,210 --> 00:20:11,180
shares terms with the main input drive so at least expectation-maximization you

405
00:20:11,180 --> 00:20:14,030
so at least expectation-maximization you have to completely redo the end step now

406
00:20:14,030 --> 00:20:16,120
have to completely redo the end step now I've tried to do this analytically

407
00:20:16,120 --> 00:20:18,850
I've tried to do this analytically unfortunately there is one specific part

408
00:20:18,850 --> 00:20:22,430
unfortunately there is one specific part that is essentially intractable and so

409
00:20:22,430 --> 00:20:23,840
that is essentially intractable and so you would need to implement generalize

410
00:20:23,840 --> 00:20:29,720
you would need to implement generalize DM to a box made a local pollution but

411
00:20:29,720 --> 00:20:31,400
DM to a box made a local pollution but now generally none of the other measures

412
00:20:31,400 --> 00:20:33,880
now generally none of the other measures work well either sample precision

413
00:20:33,880 --> 00:20:37,160
work well either sample precision covariance even they are fit none of

414
00:20:37,160 --> 00:20:38,990
covariance even they are fit none of them did a good job because they're all

415
00:20:38,990 --> 00:20:42,140
them did a good job because they're all somehow functions of the precision for

416
00:20:42,140 --> 00:20:44,570
somehow functions of the precision for this process which in the presence of an

417
00:20:44,570 --> 00:20:47,630
this process which in the presence of an external input takes on the following

418
00:20:47,630 --> 00:20:49,520
external input takes on the following form where you have additive terms going

419
00:20:49,520 --> 00:20:51,500
form where you have additive terms going from the external input covariance and

420
00:20:51,500 --> 00:20:53,150
from the external input covariance and remember this is the part that gives us

421
00:20:53,150 --> 00:20:55,100
remember this is the part that gives us two one-to-one relationship with

422
00:20:55,100 --> 00:20:58,070
two one-to-one relationship with bi-directional network connectivity it's

423
00:20:58,070 --> 00:21:00,410
bi-directional network connectivity it's now being scrambled projected in all

424
00:21:00,410 --> 00:21:02,570
now being scrambled projected in all sorts of different directions by the XR

425
00:21:02,570 --> 00:21:04,220
sorts of different directions by the XR 1 for covariance and you're also getting

426
00:21:04,220 --> 00:21:05,030
1 for covariance and you're also getting more very

427
00:21:05,030 --> 00:21:07,700
more very from the triple product term so it's

428
00:21:07,700 --> 00:21:12,170
from the triple product term so it's just not a good problem and then yeah I

429
00:21:12,170 --> 00:21:14,360
just not a good problem and then yeah I just we just started to get systems

430
00:21:14,360 --> 00:21:17,840
just we just started to get systems we're going a little faster today quick

431
00:21:17,840 --> 00:21:19,880
we're going a little faster today quick plug my collaborators on a lot of this

432
00:21:19,880 --> 00:21:22,520
plug my collaborators on a lot of this research at Rice University in Houston

433
00:21:22,520 --> 00:21:25,640
research at Rice University in Houston is headed by the lab of Ginevra Allen

434
00:21:25,640 --> 00:21:27,920
is headed by the lab of Ginevra Allen they were exploring a couple other cases

435
00:21:27,920 --> 00:21:29,540
they were exploring a couple other cases of time dependence which are really

436
00:21:29,540 --> 00:21:32,270
of time dependence which are really interesting that I don't have to talk

437
00:21:32,270 --> 00:21:35,210
interesting that I don't have to talk about just as an upcoming talk next

438
00:21:35,210 --> 00:21:38,210
about just as an upcoming talk next Friday May 4th that's a part of a team

439
00:21:38,210 --> 00:21:39,590
Friday May 4th that's a part of a team that's colloquium but I think it's

440
00:21:39,590 --> 00:21:42,950
that's colloquium but I think it's sponsored by IBM he really goes there

441
00:21:42,950 --> 00:21:43,910
sponsored by IBM he really goes there interested in dealing with high

442
00:21:43,910 --> 00:21:45,410
interested in dealing with high dimensional data as she does a lot of

443
00:21:45,410 --> 00:21:48,950
dimensional data as she does a lot of other projects and biostatistics not

444
00:21:48,950 --> 00:21:52,610
other projects and biostatistics not just neuroscience and so here are many

445
00:21:52,610 --> 00:21:54,110
just neuroscience and so here are many other references that allow this is

446
00:21:54,110 --> 00:21:57,170
other references that allow this is based on the first two were kind of

447
00:21:57,170 --> 00:22:00,230
based on the first two were kind of joint publications by Allen and this is

448
00:22:00,230 --> 00:22:02,870
joint publications by Allen and this is just a general background on the problem

449
00:22:02,870 --> 00:22:04,880
just a general background on the problem in general and simply gonna talk about

450
00:22:04,880 --> 00:22:06,890
in general and simply gonna talk about GL and say if you want to learn more

451
00:22:06,890 --> 00:22:09,200
GL and say if you want to learn more about what genomes can offer any

452
00:22:09,200 --> 00:22:11,060
about what genomes can offer any publication hospital lab is the way to

453
00:22:11,060 --> 00:22:14,360
publication hospital lab is the way to go it's work is supported under multiple

454
00:22:14,360 --> 00:22:16,880
go it's work is supported under multiple NSF grants not like to thank everyone in

455
00:22:16,880 --> 00:22:18,530
NSF grants not like to thank everyone in the Rosenbaum lab for their feedback on

456
00:22:18,530 --> 00:22:20,470
the Rosenbaum lab for their feedback on the early versions of this presentation

457
00:22:20,470 --> 00:22:22,730
the early versions of this presentation so thank you for listening and do you

458
00:22:22,730 --> 00:22:37,480
so thank you for listening and do you have any questions

459
00:22:37,480 --> 00:22:37,490


460
00:22:37,490 --> 00:22:40,460
yes

461
00:22:40,460 --> 00:22:40,470


462
00:22:40,470 --> 00:22:47,090
okay so everything he asked how many neurons do I look at so I looked at beta

463
00:22:47,090 --> 00:22:51,140
neurons do I look at so I looked at beta test containing hundreds but everything

464
00:22:51,140 --> 00:22:54,680
test containing hundreds but everything you saw in this presentation had was 30

465
00:22:54,680 --> 00:22:58,160
you saw in this presentation had was 30 so very small Network now I have with

466
00:22:58,160 --> 00:23:01,640
so very small Network now I have with that hundreds and thousands plug neuron

467
00:23:01,640 --> 00:23:04,400
that hundreds and thousands plug neuron of neurons in the context of in silico

468
00:23:04,400 --> 00:23:09,050
of neurons in the context of in silico modeling and you the only thing is it's

469
00:23:09,050 --> 00:23:12,380
modeling and you the only thing is it's harder to get really clear results to

470
00:23:12,380 --> 00:23:15,110
harder to get really clear results to show you you can crank up the data that

471
00:23:15,110 --> 00:23:17,510
show you you can crank up the data that you feed the model do really like

472
00:23:17,510 --> 00:23:19,640
you feed the model do really like millions of data points and that's

473
00:23:19,640 --> 00:23:20,930
millions of data points and that's what's required to get really good

474
00:23:20,930 --> 00:23:23,420
what's required to get really good inference for life you know a thousand

475
00:23:23,420 --> 00:23:27,110
inference for life you know a thousand neurons or something like that so yeah I

476
00:23:27,110 --> 00:23:30,320
neurons or something like that so yeah I looked at fairly small networks in this

477
00:23:30,320 --> 00:23:35,060
looked at fairly small networks in this context but the goal is to develop

478
00:23:35,060 --> 00:23:37,040
context but the goal is to develop methods but there the reason there is

479
00:23:37,040 --> 00:23:41,300
methods but there the reason there is computational time glosso doesn't even

480
00:23:41,300 --> 00:23:44,300
computational time glosso doesn't even converge if you have more than a few

481
00:23:44,300 --> 00:23:46,400
converge if you have more than a few hundred neurons there's a certain step

482
00:23:46,400 --> 00:23:48,290
hundred neurons there's a certain step in the lasso algorithm where you have to

483
00:23:48,290 --> 00:23:52,280
in the lasso algorithm where you have to do a kind of shooting thing and it just

484
00:23:52,280 --> 00:23:55,400
do a kind of shooting thing and it just it takes forever to not converge if you

485
00:23:55,400 --> 00:23:57,500
it takes forever to not converge if you have a few hundred neurons and very

486
00:23:57,500 --> 00:24:00,549
have a few hundred neurons and very small amounts of data

487
00:24:00,549 --> 00:24:00,559


488
00:24:00,559 --> 00:24:05,769
but the hope is to one day spend all of our methods to the case where we can

489
00:24:05,769 --> 00:24:07,899
our methods to the case where we can look at tens of thousands simultaneously

490
00:24:07,899 --> 00:24:09,970
look at tens of thousands simultaneously and still perform this level of

491
00:24:09,970 --> 00:24:12,730
and still perform this level of inference we're just not there yet good

492
00:24:12,730 --> 00:25:14,380
inference we're just not there yet good question and the others right thank you

493
00:25:14,380 --> 00:25:14,390


494
00:25:14,390 --> 00:25:19,030
so good afternoon everybody my talk is about web-based tree-structured

495
00:25:19,030 --> 00:25:21,190
about web-based tree-structured compressive sensing with variation of

496
00:25:21,190 --> 00:25:25,000
compressive sensing with variation of Bayesian analysis so and wavelet based

497
00:25:25,000 --> 00:25:28,210
Bayesian analysis so and wavelet based transform can be used to decompose a

498
00:25:28,210 --> 00:25:30,700
transform can be used to decompose a given signal or function that we have in

499
00:25:30,700 --> 00:25:33,100
given signal or function that we have in terms of some basic functions called

500
00:25:33,100 --> 00:25:37,690
terms of some basic functions called wavelets and then we can study these the

501
00:25:37,690 --> 00:25:39,520
wavelets and then we can study these the structure of these wavelet coefficients

502
00:25:39,520 --> 00:25:42,160
structure of these wavelet coefficients proper to fill data that we have so

503
00:25:42,160 --> 00:25:44,530
proper to fill data that we have so imagine that we have a signal and we

504
00:25:44,530 --> 00:25:46,450
imagine that we have a signal and we apply whether the composition on this

505
00:25:46,450 --> 00:25:49,510
apply whether the composition on this the results are two types of data one is

506
00:25:49,510 --> 00:25:51,910
the results are two types of data one is the scaling coefficients the other ones

507
00:25:51,910 --> 00:25:55,390
the scaling coefficients the other ones are the favorite coefficient scaling

508
00:25:55,390 --> 00:25:58,360
are the favorite coefficient scaling coefficients give an approximation of

509
00:25:58,360 --> 00:26:00,720
coefficients give an approximation of the function that we have in some lower

510
00:26:00,720 --> 00:26:03,580
the function that we have in some lower resolution space and the other

511
00:26:03,580 --> 00:26:06,040
resolution space and the other coefficient provides us with the detail

512
00:26:06,040 --> 00:26:10,090
coefficient provides us with the detail that we need to reconstruct the original

513
00:26:10,090 --> 00:26:12,430
that we need to reconstruct the original data that we have from those that

514
00:26:12,430 --> 00:26:15,040
data that we have from those that approximation now we can apply the

515
00:26:15,040 --> 00:26:17,080
approximation now we can apply the wavelet decomposition as many at times

516
00:26:17,080 --> 00:26:20,410
wavelet decomposition as many at times as we want and each time we need to take

517
00:26:20,410 --> 00:26:22,360
as we want and each time we need to take the product mission that we have why the

518
00:26:22,360 --> 00:26:25,000
the product mission that we have why the composition on that and then we end up

519
00:26:25,000 --> 00:26:26,770
composition on that and then we end up with another set of reverberate

520
00:26:26,770 --> 00:26:30,040
with another set of reverberate coefficient and even lower resolution of

521
00:26:30,040 --> 00:26:33,490
coefficient and even lower resolution of the approximation so to reconstruct the

522
00:26:33,490 --> 00:26:38,470
the approximation so to reconstruct the data which and

523
00:26:38,470 --> 00:26:38,480


524
00:26:38,480 --> 00:26:45,430
we can take these Kelvin coefficients and sum them with the weather conditions

525
00:26:45,430 --> 00:26:48,150
and sum them with the weather conditions that we have for all the levels of

526
00:26:48,150 --> 00:26:51,480
that we have for all the levels of decomposition and we reconstruct our

527
00:26:51,480 --> 00:26:56,440
decomposition and we reconstruct our original data so for most ignores the

528
00:26:56,440 --> 00:26:58,480
original data so for most ignores the wavelet coefficients are part and

529
00:26:58,480 --> 00:27:01,300
wavelet coefficients are part and they're compressible and that means that

530
00:27:01,300 --> 00:27:03,250
they're compressible and that means that we can set to zero many of the waiver

531
00:27:03,250 --> 00:27:05,410
we can set to zero many of the waiver coefficients that we have many of them

532
00:27:05,410 --> 00:27:08,110
coefficients that we have many of them have negligible value and we can neglect

533
00:27:08,110 --> 00:27:12,760
have negligible value and we can neglect them and we can reconstruct the function

534
00:27:12,760 --> 00:27:14,890
them and we can reconstruct the function from the modified wavelet coefficients

535
00:27:14,890 --> 00:27:21,990
from the modified wavelet coefficients and end up with a good very accurately

536
00:27:21,990 --> 00:27:25,630
and end up with a good very accurately because the measurements that we have

537
00:27:25,630 --> 00:27:29,500
because the measurements that we have might not might not always be easy and

538
00:27:29,500 --> 00:27:32,470
might not might not always be easy and cheap to acquire for example in MRI

539
00:27:32,470 --> 00:27:35,980
cheap to acquire for example in MRI image imaging it's very tedious faster

540
00:27:35,980 --> 00:27:42,220
image imaging it's very tedious faster it's time-consuming imaging we need to

541
00:27:42,220 --> 00:27:44,830
it's time-consuming imaging we need to take image in many spiritual bands so

542
00:27:44,830 --> 00:27:48,100
take image in many spiritual bands so one might think that it's the useful way

543
00:27:48,100 --> 00:27:57,420
one might think that it's the useful way to only measure the informative data

544
00:27:57,420 --> 00:27:57,430


545
00:27:57,430 --> 00:28:07,450
measurements that we need to have so the incompressible testing we assume that

546
00:28:07,450 --> 00:28:09,490
incompressible testing we assume that the signal we have is compressible

547
00:28:09,490 --> 00:28:15,400
the signal we have is compressible instant basis and then we can exactly we

548
00:28:15,400 --> 00:28:18,040
instant basis and then we can exactly we construct the originals are underlying

549
00:28:18,040 --> 00:28:20,440
construct the originals are underlying original signal with a number of

550
00:28:20,440 --> 00:28:22,630
original signal with a number of appropriately designed projected

551
00:28:22,630 --> 00:28:33,880
appropriately designed projected measurement and we can be theta where

552
00:28:33,880 --> 00:28:37,450
measurement and we can be theta where theta is over bed with coefficient and T

553
00:28:37,450 --> 00:28:40,870
theta is over bed with coefficient and T is the matrix rows of which are the

554
00:28:40,870 --> 00:28:43,410
is the matrix rows of which are the random with the time projected

555
00:28:43,410 --> 00:28:46,430
random with the time projected projection vector

556
00:28:46,430 --> 00:28:50,580
projection vector so we would like if we assume that this

557
00:28:50,580 --> 00:28:53,700
so we would like if we assume that this V is n dimensional ended based with one

558
00:28:53,700 --> 00:28:56,909
V is n dimensional ended based with one x and and dimensional n elements and

559
00:28:56,909 --> 00:28:59,850
x and and dimensional n elements and theta is the has n elements we would

560
00:28:59,850 --> 00:29:01,590
theta is the has n elements we would like the number of measurements to be as

561
00:29:01,590 --> 00:29:13,370
like the number of measurements to be as low as possible conditions that we have

562
00:29:13,370 --> 00:29:13,380


563
00:29:13,380 --> 00:29:20,129
we have these theta coefficients we can reconstruct our image or signal that we

564
00:29:20,129 --> 00:29:24,480
reconstruct our image or signal that we have using an image transport but we

565
00:29:24,480 --> 00:29:30,629
have using an image transport but we measure it is a projection it is a

566
00:29:30,629 --> 00:29:52,110
measure it is a projection it is a projection this problem is undetermined

567
00:29:52,110 --> 00:29:54,289
projection this problem is undetermined and it still posed and a good way to

568
00:29:54,289 --> 00:29:57,119
and it still posed and a good way to approach this problem is using Bayesian

569
00:29:57,119 --> 00:30:05,070
approach this problem is using Bayesian Setia conversion so some structures and

570
00:30:05,070 --> 00:30:07,409
Setia conversion so some structures and we can incorporate this structure of

571
00:30:07,409 --> 00:30:10,350
we can incorporate this structure of this wave with to have our coefficients

572
00:30:10,350 --> 00:30:12,720
this wave with to have our coefficients to in order to reduce the number of

573
00:30:12,720 --> 00:30:15,690
to in order to reduce the number of required measurements that we have if we

574
00:30:15,690 --> 00:30:17,490
required measurements that we have if we want to apply the treatment web

575
00:30:17,490 --> 00:30:20,430
want to apply the treatment web transform on airsick given signal we can

576
00:30:20,430 --> 00:30:23,999
transform on airsick given signal we can define this as deploying the series the

577
00:30:23,999 --> 00:30:29,129
define this as deploying the series the high-pass and low-pass filters and that

578
00:30:29,129 --> 00:30:32,850
high-pass and low-pass filters and that would result in a quadtree here H stands

579
00:30:32,850 --> 00:30:35,549
would result in a quadtree here H stands for the high-pass and L stands for Lopes

580
00:30:35,549 --> 00:30:38,940
for the high-pass and L stands for Lopes we have a signal with labels will end up

581
00:30:38,940 --> 00:30:41,789
we have a signal with labels will end up with two types of results began on each

582
00:30:41,789 --> 00:30:43,980
with two types of results began on each of these we apply these two

583
00:30:43,980 --> 00:30:47,490
of these we apply these two levan filters at the end we end up with

584
00:30:47,490 --> 00:30:50,310
levan filters at the end we end up with three stop bands of death coefficients

585
00:30:50,310 --> 00:30:54,030
three stop bands of death coefficients if this one is the details in the

586
00:30:54,030 --> 00:30:56,700
if this one is the details in the horizontal Direction vertical and

587
00:30:56,700 --> 00:30:59,970
horizontal Direction vertical and diagonal region and this part is the

588
00:30:59,970 --> 00:31:02,490
diagonal region and this part is the scoring conditions that we have which

589
00:31:02,490 --> 00:31:04,830
scoring conditions that we have which gives us the formation in the universe

590
00:31:04,830 --> 00:31:06,750
gives us the formation in the universe space now

591
00:31:06,750 --> 00:31:09,419
space now this can again will be composed again

592
00:31:09,419 --> 00:31:12,600
this can again will be composed again from wavelet coefficients and from

593
00:31:12,600 --> 00:31:20,150
from wavelet coefficients and from killing coefficient and again and again

594
00:31:20,150 --> 00:31:20,160


595
00:31:20,160 --> 00:32:04,440
is called the last one is the weak note the parent-child relationship between

596
00:32:04,440 --> 00:32:27,060
the parent-child relationship between the coefficients across each each of

597
00:32:27,060 --> 00:32:36,680
the coefficients across each each of these have four other children until we

598
00:32:36,680 --> 00:32:36,690


599
00:32:36,690 --> 00:32:44,549
would like to use this structure that we have an incorporated in the prior in

600
00:32:44,549 --> 00:32:46,330
have an incorporated in the prior in order to reduce

601
00:32:46,330 --> 00:32:50,279
order to reduce of measurement that are required for a

602
00:32:50,279 --> 00:32:53,710
of measurement that are required for a basically a good accuracy reconstruction

603
00:32:53,710 --> 00:32:56,019
basically a good accuracy reconstruction of the signal this has been previously

604
00:32:56,019 --> 00:32:58,899
of the signal this has been previously used in other jobs like compressing

605
00:32:58,899 --> 00:33:02,950
used in other jobs like compressing email compressing data or texture

606
00:33:02,950 --> 00:33:07,779
email compressing data or texture synthesis or image denoising and we

607
00:33:07,779 --> 00:33:10,870
synthesis or image denoising and we would like to take this product exploit

608
00:33:10,870 --> 00:33:12,850
would like to take this product exploit the structure of these wavelet countries

609
00:33:12,850 --> 00:33:16,269
the structure of these wavelet countries and use them in our vision inversion one

610
00:33:16,269 --> 00:33:18,190
and use them in our vision inversion one thing about the weblog coefficients is

611
00:33:18,190 --> 00:33:20,110
thing about the weblog coefficients is that the nuke digital web web

612
00:33:20,110 --> 00:33:23,370
that the nuke digital web web coefficient tend to cluster together and

613
00:33:23,370 --> 00:33:29,019
coefficient tend to cluster together and they apparently a negligible coefficient

614
00:33:29,019 --> 00:33:31,990
they apparently a negligible coefficient it children with high probability would

615
00:33:31,990 --> 00:33:44,139
it children with high probability would be 0 trees it means that the parent is

616
00:33:44,139 --> 00:33:49,889
be 0 trees it means that the parent is doing children is do as 200 so forth and

617
00:33:49,889 --> 00:33:49,899


618
00:33:49,899 --> 00:34:00,789
we can structure using hidden markov trees in multiple trees we can say that

619
00:34:00,789 --> 00:34:01,480
trees in multiple trees we can say that all right

620
00:34:01,480 --> 00:34:04,090
all right so we have two types of coefficients one

621
00:34:04,090 --> 00:34:06,639
so we have two types of coefficients one unintelligible coefficient you ever try

622
00:34:06,639 --> 00:34:08,710
unintelligible coefficient you ever try for the significant coefficients we can

623
00:34:08,710 --> 00:34:11,800
for the significant coefficients we can draw the color negligible coefficient

624
00:34:11,800 --> 00:34:14,470
draw the color negligible coefficient from Adelphia with a low variance and we

625
00:34:14,470 --> 00:34:17,919
from Adelphia with a low variance and we can draw the coefficient significant

626
00:34:17,919 --> 00:34:19,960
can draw the coefficient significant coefficients we will from a Gaussian

627
00:34:19,960 --> 00:34:27,280
coefficients we will from a Gaussian with a larger variance and to define

628
00:34:27,280 --> 00:34:30,550
with a larger variance and to define this parent-child relationship we can

629
00:34:30,550 --> 00:34:34,659
this parent-child relationship we can use a markup representation across the

630
00:34:34,659 --> 00:34:37,899
use a markup representation across the skills so this mark appropriate

631
00:34:37,899 --> 00:34:41,050
skills so this mark appropriate transition can be defined using a two by

632
00:34:41,050 --> 00:34:44,530
transition can be defined using a two by two matrix P IJ where I stands for the

633
00:34:44,530 --> 00:34:46,150
two matrix P IJ where I stands for the state of the

634
00:34:46,150 --> 00:34:50,780
state of the pick up the child so tij for example kid

635
00:34:50,780 --> 00:34:54,230
pick up the child so tij for example kid Lolo is the probability of child and

636
00:34:54,230 --> 00:34:56,570
Lolo is the probability of child and parents both being in the new state

637
00:34:56,570 --> 00:34:58,910
parents both being in the new state which is a high probability so we show

638
00:34:58,910 --> 00:35:01,940
which is a high probability so we show it with one - extra everyone being a

639
00:35:01,940 --> 00:35:09,590
it with one - extra everyone being a small positive number if parent

640
00:35:09,590 --> 00:35:11,810
small positive number if parent coefficients is negligible its children

641
00:35:11,810 --> 00:35:19,370
coefficients is negligible its children would be negligible as well in the tree

642
00:35:19,370 --> 00:35:21,500
would be negligible as well in the tree we are assuming that our observations

643
00:35:21,500 --> 00:35:27,710
we are assuming that our observations are our coefficients but in compressive

644
00:35:27,710 --> 00:35:32,570
are our coefficients but in compressive sensing our measurements are the random

645
00:35:32,570 --> 00:35:34,190
sensing our measurements are the random projections of these weather

646
00:35:34,190 --> 00:35:37,610
projections of these weather coefficients and we don't observe these

647
00:35:37,610 --> 00:35:41,770
coefficients and we don't observe these coefficients directively directly

648
00:35:41,770 --> 00:35:45,800
coefficients directively directly alright assuming our signal is called X

649
00:35:45,800 --> 00:35:51,400
alright assuming our signal is called X we and we have a match basically bevit

650
00:35:51,400 --> 00:35:55,040
we and we have a match basically bevit the composition make sure it matrix Phi

651
00:35:55,040 --> 00:35:57,890
the composition make sure it matrix Phi we apply this and we get overridden with

652
00:35:57,890 --> 00:36:01,180
we apply this and we get overridden with coefficients and our measurements are

653
00:36:01,180 --> 00:36:12,980
coefficients and our measurements are random projections of theta M which

654
00:36:12,980 --> 00:36:16,300
random projections of theta M which includes the significant coefficients

655
00:36:16,300 --> 00:36:19,390
includes the significant coefficients the negligible ones are set to zero and

656
00:36:19,390 --> 00:36:23,240
the negligible ones are set to zero and the other one is theta e where the

657
00:36:23,240 --> 00:36:25,700
the other one is theta e where the natural significant ones are set to zero

658
00:36:25,700 --> 00:36:29,320
natural significant ones are set to zero and it only has the negligible one and

659
00:36:29,320 --> 00:36:32,150
and it only has the negligible one and we also assume that we have noise in our

660
00:36:32,150 --> 00:36:35,810
we also assume that we have noise in our measurement so we can start this to our

661
00:36:35,810 --> 00:36:40,610
measurement so we can start this to our and right there at n which is the noise

662
00:36:40,610 --> 00:36:42,500
and right there at n which is the noise it can be it is represented with a

663
00:36:42,500 --> 00:36:56,290
it can be it is represented with a Gaussian with some precision is the

664
00:36:56,290 --> 00:36:59,120
Gaussian with some precision is the significant wavelet coefficients first

665
00:36:59,120 --> 00:37:04,770
significant wavelet coefficients first their locations second their values and

666
00:37:04,770 --> 00:37:04,780


667
00:37:04,780 --> 00:37:09,790
[Applause] so the posterior distribution of this

668
00:37:09,790 --> 00:37:15,070
so the posterior distribution of this would be inferred using the updated

669
00:37:15,070 --> 00:37:18,590
would be inferred using the updated measurement we now this structure is a

670
00:37:18,590 --> 00:37:21,110
measurement we now this structure is a structure information that we would like

671
00:37:21,110 --> 00:37:23,960
structure information that we would like to incorporate in the prior our two

672
00:37:23,960 --> 00:37:24,410
to incorporate in the prior our two things

673
00:37:24,410 --> 00:37:27,410
things the parent-child relationship second one

674
00:37:27,410 --> 00:37:31,060
the parent-child relationship second one this person's of the wavelet coefficient

675
00:37:31,060 --> 00:37:34,220
this person's of the wavelet coefficient one can define it using these functions

676
00:37:34,220 --> 00:37:38,030
one can define it using these functions that prior which is a combined

677
00:37:38,030 --> 00:37:40,670
that prior which is a combined distribution and which combined

678
00:37:40,670 --> 00:37:45,170
distribution and which combined attribution which is a built in here is

679
00:37:45,170 --> 00:37:52,940
attribution which is a built in here is a match point and and and is a Gaussian

680
00:37:52,940 --> 00:37:54,950
a match point and and and is a Gaussian so we are saying that our rebel

681
00:37:54,950 --> 00:37:57,380
so we are saying that our rebel coefficients are drawn from there are

682
00:37:57,380 --> 00:37:59,510
coefficients are drawn from there are either zero with probability one minus

683
00:37:59,510 --> 00:38:03,410
either zero with probability one minus PI I or they are drawn from a Gaussian

684
00:38:03,410 --> 00:38:06,170
PI I or they are drawn from a Gaussian distribution with some variance with

685
00:38:06,170 --> 00:38:11,829
distribution with some variance with probability PI okay so

686
00:38:11,829 --> 00:38:15,949
probability PI okay so this first part represents the zero

687
00:38:15,949 --> 00:38:19,370
this first part represents the zero coefficient and this part represents the

688
00:38:19,370 --> 00:38:24,079
coefficient and this part represents the significant nonzero coefficient so each

689
00:38:24,079 --> 00:38:26,959
significant nonzero coefficient so each of these where are corresponds with one

690
00:38:26,959 --> 00:38:29,059
of these where are corresponds with one of the guardians that really defined we

691
00:38:29,059 --> 00:38:31,489
of the guardians that really defined we were talking about in the hidden markov

692
00:38:31,489 --> 00:38:36,559
were talking about in the hidden markov tree and here the coefficients in the

693
00:38:36,559 --> 00:38:40,519
tree and here the coefficients in the low state are set to zero as opposed to

694
00:38:40,519 --> 00:38:42,769
low state are set to zero as opposed to hidden markov tree model which is what

695
00:38:42,769 --> 00:38:45,670
hidden markov tree model which is what each was drawn from a Gaussian with low

696
00:38:45,670 --> 00:38:52,779
each was drawn from a Gaussian with low variance now the children will be zero

697
00:38:52,779 --> 00:38:59,509
variance now the children will be zero using some dependences across on the

698
00:38:59,509 --> 00:39:12,759
using some dependences across on the mixing way spike and prior in terms of

699
00:39:12,759 --> 00:39:17,949
mixing way spike and prior in terms of using hot water which is element product

700
00:39:17,949 --> 00:39:24,759
using hot water which is element product we have W equals W times W is the

701
00:39:24,759 --> 00:39:28,069
we have W equals W times W is the coefficient drawn from a Gaussian

702
00:39:28,069 --> 00:39:31,430
coefficient drawn from a Gaussian distribution as Z is a Bernoulli

703
00:39:31,430 --> 00:39:34,900
distribution as Z is a Bernoulli distribution which is the one or series

704
00:39:34,900 --> 00:39:37,549
distribution which is the one or series the one says that

705
00:39:37,549 --> 00:39:40,160
the one says that okay the rebel coalition is in a high

706
00:39:40,160 --> 00:39:42,469
okay the rebel coalition is in a high state and co-state status to low state

707
00:39:42,469 --> 00:39:48,499
state and co-state status to low state instead set to zero and the power in our

708
00:39:48,499 --> 00:39:50,509
instead set to zero and the power in our model we would like to define this

709
00:39:50,509 --> 00:39:53,599
model we would like to define this mixing waste their precision parameter

710
00:39:53,599 --> 00:39:57,160
mixing waste their precision parameter and precision for the noise we can

711
00:39:57,160 --> 00:40:00,289
and precision for the noise we can summarize the model as following we have

712
00:40:00,289 --> 00:40:04,789
summarize the model as following we have our theta it is the product of WS d / w

713
00:40:04,789 --> 00:40:07,219
our theta it is the product of WS d / w is valid position from from a Gaussian

714
00:40:07,219 --> 00:40:07,969
is valid position from from a Gaussian distribution

715
00:40:07,969 --> 00:40:09,000
distribution z

716
00:40:09,000 --> 00:40:13,380
z is defining whether they are in high or

717
00:40:13,380 --> 00:40:17,130
is defining whether they are in high or low and it's the Bernoulli distribution

718
00:40:17,130 --> 00:40:24,230
low and it's the Bernoulli distribution and pies are the over basically mixing

719
00:40:24,230 --> 00:40:28,320
and pies are the over basically mixing mixing waste now we can divide the types

720
00:40:28,320 --> 00:40:30,630
mixing waste now we can divide the types of mixing weights into four categories

721
00:40:30,630 --> 00:40:33,690
of mixing weights into four categories one is the mixing rate for the filling

722
00:40:33,690 --> 00:40:36,270
one is the mixing rate for the filling coefficient second one is the mixing

723
00:40:36,270 --> 00:40:39,690
coefficient second one is the mixing rate for their real note and third one

724
00:40:39,690 --> 00:40:43,170
rate for their real note and third one is the mixing weight toward the all

725
00:40:43,170 --> 00:40:46,760
is the mixing weight toward the all other river conditions that have nonzero

726
00:40:46,760 --> 00:40:50,250
other river conditions that have nonzero basically parents and lastly is the ones

727
00:40:50,250 --> 00:40:54,290
basically parents and lastly is the ones that have zero parent each of these are

728
00:40:54,290 --> 00:40:58,550
that have zero parent each of these are defined using from hyper trenchers which

729
00:40:58,550 --> 00:41:01,530
defined using from hyper trenchers which like to by choosing these hyper

730
00:41:01,530 --> 00:41:03,960
like to by choosing these hyper parameters they would like to say okay

731
00:41:03,960 --> 00:41:14,160
parameters they would like to say okay what idea we have about the scaling

732
00:41:14,160 --> 00:41:21,270
what idea we have about the scaling coefficient normally nonzero and we

733
00:41:21,270 --> 00:41:24,290
coefficient normally nonzero and we would like something clustered around

734
00:41:24,290 --> 00:41:30,990
would like something clustered around for the same type of distribution so the

735
00:41:30,990 --> 00:41:33,420
for the same type of distribution so the ones that have zero parent we would like

736
00:41:33,420 --> 00:41:36,900
ones that have zero parent we would like them to be clustered around 0 and for

737
00:41:36,900 --> 00:41:39,690
them to be clustered around 0 and for the other ones we that have nonzero

738
00:41:39,690 --> 00:41:42,240
the other ones we that have nonzero parents we don't impose any prior

739
00:41:42,240 --> 00:41:46,290
parents we don't impose any prior information they can be anything here is

740
00:41:46,290 --> 00:41:48,660
information they can be anything here is a graphical representation of the

741
00:41:48,660 --> 00:41:52,440
a graphical representation of the problem dr. model see these are the

742
00:41:52,440 --> 00:41:54,990
problem dr. model see these are the hyper temperatures that we have with

743
00:41:54,990 --> 00:41:57,240
hyper temperatures that we have with this hyper parameters are used basically

744
00:41:57,240 --> 00:42:00,960
this hyper parameters are used basically to define the mixing rate and these are

745
00:42:00,960 --> 00:42:06,090
to define the mixing rate and these are for defining the per station and V is

746
00:42:06,090 --> 00:42:10,230
for defining the per station and V is our observation here so I describe these

747
00:42:10,230 --> 00:42:11,980
our observation here so I describe these clips right on

748
00:42:11,980 --> 00:42:15,140
clips right on formulations like all right so the

749
00:42:15,140 --> 00:42:17,599
formulations like all right so the posterior distribution parameter here is

750
00:42:17,599 --> 00:42:20,930
posterior distribution parameter here is Theta and this is improv using the

751
00:42:20,930 --> 00:42:23,390
Theta and this is improv using the observed data from overseas measurements

752
00:42:23,390 --> 00:42:26,359
observed data from overseas measurements and we can find the posterior

753
00:42:26,359 --> 00:42:28,520
and we can find the posterior distribution for this using some method

754
00:42:28,520 --> 00:42:32,890
distribution for this using some method like a variational Bayesian method

755
00:42:32,890 --> 00:42:37,039
like a variational Bayesian method Markov chain Monte Carlo in a Markov

756
00:42:37,039 --> 00:42:39,079
Markov chain Monte Carlo in a Markov chain Monte Carlo gives a more accurate

757
00:42:39,079 --> 00:42:42,130
chain Monte Carlo gives a more accurate solution and it is basically

758
00:42:42,130 --> 00:42:46,940
solution and it is basically computationally very expensive on the

759
00:42:46,940 --> 00:42:49,339
computationally very expensive on the other hand variational based give us a

760
00:42:49,339 --> 00:42:52,099
other hand variational based give us a sum and good approximation to the

761
00:42:52,099 --> 00:42:53,900
sum and good approximation to the solution and it's fast and it has

762
00:42:53,900 --> 00:43:04,069
solution and it's fast and it has convergence criterion and defined which

763
00:43:04,069 --> 00:43:06,680
convergence criterion and defined which approximate the true posterior and we

764
00:43:06,680 --> 00:43:08,750
approximate the true posterior and we have some lower bound f which

765
00:43:08,750 --> 00:43:10,789
have some lower bound f which approximates the what the trail of

766
00:43:10,789 --> 00:43:13,190
approximates the what the trail of likelihood and we do iterations until

767
00:43:13,190 --> 00:43:32,900
likelihood and we do iterations until our little about approaches the DP one

768
00:43:32,900 --> 00:43:36,640
our little about approaches the DP one and the relative reconstruction error is

769
00:43:36,640 --> 00:43:41,799
and the relative reconstruction error is calculating is calculated using this

770
00:43:41,799 --> 00:43:41,809


771
00:43:41,809 --> 00:43:51,079
alright so here's the first image it's a hundred 28 128 image and thousand

772
00:43:51,079 --> 00:43:53,990
hundred 28 128 image and thousand measurements have been used we can't

773
00:43:53,990 --> 00:43:57,380
measurements have been used we can't basically it's decomposed and then the

774
00:43:57,380 --> 00:43:59,599
basically it's decomposed and then the first coefficient story we constructed

775
00:43:59,599 --> 00:44:01,880
first coefficient story we constructed and the image is recovered and this is

776
00:44:01,880 --> 00:44:05,059
and the image is recovered and this is the relative error second image is

777
00:44:05,059 --> 00:44:08,559
the relative error second image is tilted forward by support estimate and

778
00:44:08,559 --> 00:44:10,880
tilted forward by support estimate and 2500 measurements have been used for

779
00:44:10,880 --> 00:44:13,160
2500 measurements have been used for this test

780
00:44:13,160 --> 00:44:17,170
this test and the last one is a 32 by 32 image

781
00:44:17,170 --> 00:44:21,980
and the last one is a 32 by 32 image which has been decomposed using two

782
00:44:21,980 --> 00:44:26,960
which has been decomposed using two chocolate reconstructed and then this is

783
00:44:26,960 --> 00:44:29,690
chocolate reconstructed and then this is the error and we have used 600

784
00:44:29,690 --> 00:44:32,900
the error and we have used 600 measurements for this all right these

785
00:44:32,900 --> 00:44:35,539
measurements for this all right these are the references and thank you very

786
00:44:35,539 --> 00:45:51,300
are the references and thank you very much any question

787
00:45:51,300 --> 00:45:51,310


788
00:45:51,310 --> 00:45:54,760
hello all visit Tina and I am doing my present

789
00:45:54,760 --> 00:45:59,280
present on the pairwise chose Michael Chang

790
00:45:59,280 --> 00:46:03,810
on the pairwise chose Michael Chang actually I produced a paper by the

791
00:46:03,810 --> 00:46:06,720
actually I produced a paper by the grooving the Stanford University awarded

792
00:46:06,720 --> 00:46:09,220
grooving the Stanford University awarded more than they published it in to me

793
00:46:09,220 --> 00:46:14,530
more than they published it in to me next compliment 2016 which is a you know

794
00:46:14,530 --> 00:46:17,680
next compliment 2016 which is a you know the model the model discrete choice

795
00:46:17,680 --> 00:46:21,100
the model the model discrete choice models and there are developing gates

796
00:46:21,100 --> 00:46:25,080
models and there are developing gates now in California

797
00:46:25,080 --> 00:46:27,550
now in California Caltech used to Berkeley example

798
00:46:27,550 --> 00:46:35,109
Caltech used to Berkeley example University and you will see actually I

799
00:46:35,109 --> 00:46:37,540
University and you will see actually I want to first of all to I want to bring

800
00:46:37,540 --> 00:46:41,260
want to first of all to I want to bring the motivation about because models

801
00:46:41,260 --> 00:46:43,770
the motivation about because models actually the growing the dataset

802
00:46:43,770 --> 00:46:47,140
actually the growing the dataset captaining the choice capturing the

803
00:46:47,140 --> 00:46:51,570
captaining the choice capturing the human children in television especially

804
00:46:51,570 --> 00:46:55,770
human children in television especially some motivation and some cost for the

805
00:46:55,770 --> 00:46:59,660
some motivation and some cost for the increase meant of the

806
00:46:59,660 --> 00:47:03,859
increase meant of the choice model in order to escape from the

807
00:47:03,859 --> 00:47:07,039
choice model in order to escape from the traditional choice period accent for

808
00:47:07,039 --> 00:47:11,319
traditional choice period accent for example regularity or stochastic and

809
00:47:11,319 --> 00:47:16,249
example regularity or stochastic and Lucius church action so I want to

810
00:47:16,249 --> 00:47:18,769
Lucius church action so I want to discuss the motivations for introducing

811
00:47:18,769 --> 00:47:21,380
discuss the motivations for introducing this model prepare watch was Markov

812
00:47:21,380 --> 00:47:24,859
this model prepare watch was Markov chain is gently model it is inferential

813
00:47:24,859 --> 00:47:28,249
chain is gently model it is inferential interactive graphical model and it does

814
00:47:28,249 --> 00:47:32,299
interactive graphical model and it does not show me mention axiom producer

815
00:47:32,299 --> 00:47:35,239
not show me mention axiom producer active and insisted upon the foundation

816
00:47:35,239 --> 00:47:43,009
active and insisted upon the foundation of action extraction so this discrete

817
00:47:43,009 --> 00:47:45,950
of action extraction so this discrete choice models destroyed and actually

818
00:47:45,950 --> 00:47:51,670
choice models destroyed and actually predicted the decision between victim

819
00:47:51,670 --> 00:47:51,680


820
00:47:51,680 --> 00:47:57,099
alternatives and the applications or consumer purchasing decisions and

821
00:47:57,099 --> 00:47:59,650
consumer purchasing decisions and community changes in the mode for

822
00:47:59,650 --> 00:48:03,079
community changes in the mode for transportation to long available options

823
00:48:03,079 --> 00:48:05,900
transportation to long available options so there is a pressing need for

824
00:48:05,900 --> 00:48:09,319
so there is a pressing need for practical model capable of is describing

825
00:48:09,319 --> 00:48:15,809
practical model capable of is describing and predicting the choice behaviour

826
00:48:15,809 --> 00:48:15,819


827
00:48:15,819 --> 00:48:25,289
so one of the volunteer very actually arguably one of the most important of

828
00:48:25,289 --> 00:48:28,109
arguably one of the most important of these excellent to our solutions Church

829
00:48:28,109 --> 00:48:31,859
these excellent to our solutions Church axiom which is known as the independence

830
00:48:31,859 --> 00:48:35,539
axiom which is known as the independence of irrelevant alternatives

831
00:48:35,539 --> 00:48:35,549


832
00:48:35,549 --> 00:48:43,649
so it's example we applied this axiom on the substance alternatives and Smith

833
00:48:43,649 --> 00:48:46,919
the substance alternatives and Smith even broader universe you we want to

834
00:48:46,919 --> 00:48:50,399
even broader universe you we want to define some probabilities let pas PD

835
00:48:50,399 --> 00:48:55,819
define some probabilities let pas PD process a chosen from the best beginner

836
00:48:55,819 --> 00:48:58,439
process a chosen from the best beginner as the best with the substrate view and

837
00:48:58,439 --> 00:49:02,099
as the best with the substrate view and peds the problem chosen a fine chosen

838
00:49:02,099 --> 00:49:04,139
peds the problem chosen a fine chosen from these eight events for example we

839
00:49:04,139 --> 00:49:09,209
from these eight events for example we have two elements there are two

840
00:49:09,209 --> 00:49:13,379
have two elements there are two important statement English exam each P

841
00:49:13,379 --> 00:49:16,349
important statement English exam each P AV is equal to zero then pas will be 0

842
00:49:16,349 --> 00:49:20,639
AV is equal to zero then pas will be 0 for all contain and B the progress of

843
00:49:20,639 --> 00:49:22,889
for all contain and B the progress of choosing and from you condition on the

844
00:49:22,889 --> 00:49:28,319
choosing and from you condition on the choice line is equal to 3/8 so another

845
00:49:28,319 --> 00:49:30,799
choice line is equal to 3/8 so another another model easy

846
00:49:30,799 --> 00:49:34,289
another model easy Bradley turn loose model actually did

847
00:49:34,289 --> 00:49:38,219
Bradley turn loose model actually did this model I referred it it's got some

848
00:49:38,219 --> 00:49:42,329
this model I referred it it's got some idea about the rank blocking in detail

849
00:49:42,329 --> 00:49:46,219
idea about the rank blocking in detail comparison so it has been published in

850
00:49:46,219 --> 00:49:50,399
comparison so it has been published in 1956 it actually define the PAE

851
00:49:50,399 --> 00:49:53,969
1956 it actually define the PAE and gamma I over gamma a plus gamma

852
00:49:53,969 --> 00:49:56,849
and gamma I over gamma a plus gamma before latent quality parameters London

853
00:49:56,849 --> 00:50:02,240
before latent quality parameters London gamma I and he resulting

854
00:50:02,240 --> 00:50:05,600
gamma I and he resulting logit model employee

855
00:50:05,600 --> 00:50:08,810
logit model employee Visconti parameter gamma i for each i

856
00:50:08,810 --> 00:50:13,280
Visconti parameter gamma i for each i has the number of you so any model that

857
00:50:13,280 --> 00:50:16,640
has the number of you so any model that satisfied the dilution axiom is a cool

858
00:50:16,640 --> 00:50:17,720
satisfied the dilution axiom is a cool inequities

859
00:50:17,720 --> 00:50:23,980
inequities mmm model so one consequence

860
00:50:23,980 --> 00:50:26,980
mmm model so one consequence actually I am talking about the

861
00:50:26,980 --> 00:50:30,290
actually I am talking about the different models traditional models and

862
00:50:30,290 --> 00:50:32,480
different models traditional models and the consequences and then I want to

863
00:50:32,480 --> 00:50:35,330
the consequences and then I want to preach the efficiency and to say that

864
00:50:35,330 --> 00:50:37,970
preach the efficiency and to say that pigeons is doing a better job and a

865
00:50:37,970 --> 00:50:39,920
pigeons is doing a better job and a great job in comparison to the

866
00:50:39,920 --> 00:50:43,670
great job in comparison to the traditional choice model one consequence

867
00:50:43,670 --> 00:50:46,430
traditional choice model one consequence of these lucious action model is a

868
00:50:46,430 --> 00:50:48,620
of these lucious action model is a stochastic transitivity between

869
00:50:48,620 --> 00:50:51,200
stochastic transitivity between alternatives chance dignity means for

870
00:50:51,200 --> 00:50:54,200
alternatives chance dignity means for example we have two elements we have an

871
00:50:54,200 --> 00:50:58,340
example we have two elements we have an a has a connection with the BBC we can

872
00:50:58,340 --> 00:51:00,680
a has a connection with the BBC we can say that there is music relation between

873
00:51:00,680 --> 00:51:04,970
say that there is music relation between a and C alternative so it's the

874
00:51:04,970 --> 00:51:10,070
a and C alternative so it's the probability of PID greater than equals

875
00:51:10,070 --> 00:51:13,520
probability of PID greater than equals 1/2 and also the for P then th means

876
00:51:13,520 --> 00:51:16,670
1/2 and also the for P then th means equal to or greater than the marketing

877
00:51:16,670 --> 00:51:20,750
equal to or greater than the marketing world ta bian BC a broad important class

878
00:51:20,750 --> 00:51:22,910
world ta bian BC a broad important class of models in a study of the trade shows

879
00:51:22,910 --> 00:51:26,960
of models in a study of the trade shows in this ranch house or us or random

880
00:51:26,960 --> 00:51:31,430
in this ranch house or us or random utility models its affiliates each I at

881
00:51:31,430 --> 00:51:33,530
utility models its affiliates each I at the member of the broader universe you

882
00:51:33,530 --> 00:51:38,450
the member of the broader universe you and a random variable each I and it

883
00:51:38,450 --> 00:51:41,060
and a random variable each I and it defines the probability of our children

884
00:51:41,060 --> 00:51:44,120
defines the probability of our children from s as this expression probability of

885
00:51:44,120 --> 00:51:48,329
from s as this expression probability of X greater than or equal to XJ for any J

886
00:51:48,329 --> 00:51:51,569
X greater than or equal to XJ for any J at all um

887
00:51:51,569 --> 00:51:54,029
at all um don't neither choice as in north the

888
00:51:54,029 --> 00:52:03,479
don't neither choice as in north the Catholic transitivity so now the PNC

889
00:52:03,479 --> 00:52:08,269
Catholic transitivity so now the PNC model it is computationally imple

890
00:52:08,269 --> 00:52:10,529
model it is computationally imple conceptually and complicated simple and

891
00:52:10,529 --> 00:52:13,349
conceptually and complicated simple and in fresh a tractable model there are

892
00:52:13,349 --> 00:52:14,999
in fresh a tractable model there are some parameters we have some parameters

893
00:52:14,999 --> 00:52:19,319
some parameters we have some parameters in this model the afterburner induced Qi

894
00:52:19,319 --> 00:52:24,479
in this model the afterburner induced Qi generates matrix Q index value and also

895
00:52:24,479 --> 00:52:27,359
generates matrix Q index value and also if this defines the P is the collection

896
00:52:27,359 --> 00:52:30,719
if this defines the P is the collection probability of alternatives as the

897
00:52:30,719 --> 00:52:33,870
probability of alternatives as the probability mass of the genetic eye of

898
00:52:33,870 --> 00:52:36,630
probability mass of the genetic eye of take station and distribution of the

899
00:52:36,630 --> 00:52:41,549
take station and distribution of the continuous time Markov chain the

900
00:52:41,549 --> 00:52:43,950
continuous time Markov chain the stationary distribution actually the

901
00:52:43,950 --> 00:52:45,809
stationary distribution actually the stationary distribution in the Mark of

902
00:52:45,809 --> 00:52:48,920
stationary distribution in the Mark of change the probability which can be

903
00:52:48,920 --> 00:52:53,130
change the probability which can be shown between Mexico and the in this

904
00:52:53,130 --> 00:52:55,229
shown between Mexico and the in this virtual we have some problems with

905
00:52:55,229 --> 00:52:58,519
virtual we have some problems with distribution coming to one and if if you

906
00:52:58,519 --> 00:53:04,079
distribution coming to one and if if you multiply the Phi transpose to DQ between

907
00:53:04,079 --> 00:53:06,569
multiply the Phi transpose to DQ between the rate matrix you get the you get

908
00:53:06,569 --> 00:53:10,040
the rate matrix you get the you get depart so

909
00:53:10,040 --> 00:53:12,650
depart so the station is the reason change to

910
00:53:12,650 --> 00:53:15,950
the station is the reason change to found to satisfy the stationary

911
00:53:15,950 --> 00:53:17,990
found to satisfy the stationary condition of the eminent likelihood

912
00:53:17,990 --> 00:53:20,360
condition of the eminent likelihood function and establishing a strong

913
00:53:20,360 --> 00:53:23,350
function and establishing a strong connection between another model and

914
00:53:23,350 --> 00:53:27,230
connection between another model and though the pcrc model generalizes

915
00:53:27,230 --> 00:53:29,030
though the pcrc model generalizes disconnection between terminal

916
00:53:29,030 --> 00:53:35,060
disconnection between terminal remarketing this is the Shema did the

917
00:53:35,060 --> 00:53:36,980
remarketing this is the Shema did the left side is the schematic view of the

918
00:53:36,980 --> 00:53:40,580
left side is the schematic view of the markov chains four choices a and b BN c

919
00:53:40,580 --> 00:53:44,000
markov chains four choices a and b BN c EST those arrows the thickness of those

920
00:53:44,000 --> 00:53:48,200
EST those arrows the thickness of those arrows are the rate the rate of

921
00:53:48,200 --> 00:53:52,010
arrows are the rate the rate of transition the right side is the

922
00:53:52,010 --> 00:53:55,370
transition the right side is the assembly of these exactly these choices

923
00:53:55,370 --> 00:54:02,350
assembly of these exactly these choices in the descrition triangle shape Vince

924
00:54:02,350 --> 00:54:06,350
in the descrition triangle shape Vince transition rates so we define some

925
00:54:06,350 --> 00:54:09,050
transition rates so we define some actually imposed some constraint here

926
00:54:09,050 --> 00:54:11,900
actually imposed some constraint here which is a Qi they've lived through plus

927
00:54:11,900 --> 00:54:15,050
which is a Qi they've lived through plus QJ r is equal to greater than 1 for all

928
00:54:15,050 --> 00:54:20,510
QJ r is equal to greater than 1 for all fills IJ which is the reducibility in

929
00:54:20,510 --> 00:54:23,300
fills IJ which is the reducibility in short the irreducibility in our model so

930
00:54:23,300 --> 00:54:25,730
short the irreducibility in our model so given a set s and remember as a

931
00:54:25,730 --> 00:54:28,550
given a set s and remember as a substitute view we want to know we can

932
00:54:28,550 --> 00:54:31,160
substitute view we want to know we can control the Cure's constructors by

933
00:54:31,160 --> 00:54:33,620
control the Cure's constructors by restricting you can do that but

934
00:54:33,620 --> 00:54:38,600
restricting you can do that but retraining the rows and columns of q 2

935
00:54:38,600 --> 00:54:41,630
retraining the rows and columns of q 2 elements instead and set the Q III as

936
00:54:41,630 --> 00:54:45,970
elements instead and set the Q III as this expression and peopIe of us is

937
00:54:45,970 --> 00:54:47,360
this expression and peopIe of us is these

938
00:54:47,360 --> 00:54:49,880
these reaching the stationary distribution of

939
00:54:49,880 --> 00:54:51,069
reaching the stationary distribution of people

940
00:54:51,069 --> 00:54:55,029
people to change so the running the choice

941
00:54:55,029 --> 00:54:58,209
to change so the running the choice probability P is which is equal to PI of

942
00:54:58,209 --> 00:55:01,209
probability P is which is equal to PI of I we now want to show that the piece

943
00:55:01,209 --> 00:55:05,829
I we now want to show that the piece density model is the del define so

944
00:55:05,829 --> 00:55:08,349
density model is the del define so proposition one we want to show that

945
00:55:08,349 --> 00:55:09,999
proposition one we want to show that these nice choice probabilities are

946
00:55:09,999 --> 00:55:13,959
these nice choice probabilities are well-defined for the I as a member of as

947
00:55:13,959 --> 00:55:17,859
well-defined for the I as a member of as an all-access as some of you this is

948
00:55:17,859 --> 00:55:19,870
an all-access as some of you this is some sort of problem that in order to

949
00:55:19,870 --> 00:55:24,549
some sort of problem that in order to prove it B we need to assume something

950
00:55:24,549 --> 00:55:26,920
prove it B we need to assume something against the target that we want to prove

951
00:55:26,920 --> 00:55:33,069
against the target that we want to prove and continually some contradiction so in

952
00:55:33,069 --> 00:55:35,499
and continually some contradiction so in order to prove that this is well-defined

953
00:55:35,499 --> 00:55:38,859
order to prove that this is well-defined I need to show that it there is just

954
00:55:38,859 --> 00:55:41,920
I need to show that it there is just only one communication class we know

955
00:55:41,920 --> 00:55:46,479
only one communication class we know that s finite so there must be on just

956
00:55:46,479 --> 00:55:50,049
that s finite so there must be on just one communication class here so I know

957
00:55:50,049 --> 00:55:52,930
one communication class here so I know that to me more than one through

958
00:55:52,930 --> 00:55:55,630
that to me more than one through experimentation class but I know that

959
00:55:55,630 --> 00:55:58,749
experimentation class but I know that face under a disability link constraint

960
00:55:58,749 --> 00:56:02,650
face under a disability link constraint imposing a disability I know that Q I J

961
00:56:02,650 --> 00:56:05,979
imposing a disability I know that Q I J plus 2 J I is greater than or equal to 1

962
00:56:05,979 --> 00:56:09,579
plus 2 J I is greater than or equal to 1 and so at least one of these q IJ or two

963
00:56:09,579 --> 00:56:12,099
and so at least one of these q IJ or two jurors should be strictly positive and

964
00:56:12,099 --> 00:56:16,089
jurors should be strictly positive and this implies that there is there is some

965
00:56:16,089 --> 00:56:17,650
this implies that there is there is some screeching distinct communication

966
00:56:17,650 --> 00:56:20,840
screeching distinct communication classes and so

967
00:56:20,840 --> 00:56:23,720
classes and so this is undetermined which would be done

968
00:56:23,720 --> 00:56:25,340
this is undetermined which would be done through the transmission

969
00:56:25,340 --> 00:56:26,960
through the transmission it's tricky positively which is the

970
00:56:26,960 --> 00:56:31,690
it's tricky positively which is the contradictory and some note here is that

971
00:56:31,690 --> 00:56:35,840
contradictory and some note here is that for the irreducibility argument this

972
00:56:35,840 --> 00:56:38,750
for the irreducibility argument this expression should be positive not

973
00:56:38,750 --> 00:56:42,350
expression should be positive not necessarily one as I as we brought in

974
00:56:42,350 --> 00:56:45,650
necessarily one as I as we brought in the proof of our proposition so it could

975
00:56:45,650 --> 00:56:50,660
the proof of our proposition so it could be some constrain like this and for some

976
00:56:50,660 --> 00:56:53,750
be some constrain like this and for some positive it's belong and we can multiply

977
00:56:53,750 --> 00:56:57,290
positive it's belong and we can multiply by the one origin as a constant and it

978
00:56:57,290 --> 00:57:02,140
by the one origin as a constant and it does not affect our patient distribution

979
00:57:02,140 --> 00:57:02,150


980
00:57:02,150 --> 00:57:09,110
proposition to let gamma be the parameter of the detail mothering you to

981
00:57:09,110 --> 00:57:12,020
parameter of the detail mothering you to chew IJ equal to gamma over gamma plus

982
00:57:12,020 --> 00:57:14,150
chew IJ equal to gamma over gamma plus gamma J please can see progress

983
00:57:14,150 --> 00:57:16,460
gamma J please can see progress experience or consistent because even

984
00:57:16,460 --> 00:57:21,830
experience or consistent because even our model so we want to show that Pius s

985
00:57:21,830 --> 00:57:25,550
our model so we want to show that Pius s is equal to gamma over the Manhattan

986
00:57:25,550 --> 00:57:27,980
is equal to gamma over the Manhattan norm of the gamma which is the summation

987
00:57:27,980 --> 00:57:31,580
norm of the gamma which is the summation of the gamma a gamma R for I equal to 1

988
00:57:31,580 --> 00:57:37,820
of the gamma a gamma R for I equal to 1 to N so we want to show that sorry we

989
00:57:37,820 --> 00:57:39,590
to N so we want to show that sorry we want to show that that pi is

990
00:57:39,590 --> 00:57:41,019
want to show that that pi is interpretation distribution of

991
00:57:41,019 --> 00:57:44,289
interpretation distribution of the chance to change but I will I want

992
00:57:44,289 --> 00:57:48,249
the chance to change but I will I want to reach that PS transmission is equal

993
00:57:48,249 --> 00:57:53,669
to reach that PS transmission is equal to zero so for this expression I have

994
00:57:53,669 --> 00:57:58,029
to zero so for this expression I have this one so we can pull out this lambda

995
00:57:58,029 --> 00:58:02,380
this one so we can pull out this lambda I and so we know the values of q ji q ji

996
00:58:02,380 --> 00:58:07,949
I and so we know the values of q ji q ji and here so this is equal to zero that

997
00:58:07,949 --> 00:58:11,349
and here so this is equal to zero that PI RS is always the additional

998
00:58:11,349 --> 00:58:17,319
PI RS is always the additional distribution of the chain now I want to

999
00:58:17,319 --> 00:58:19,719
distribution of the chain now I want to talk about some properties of this piece

1000
00:58:19,719 --> 00:58:23,979
talk about some properties of this piece unity we want to demonstrate that

1001
00:58:23,979 --> 00:58:26,919
unity we want to demonstrate that pigeons he exists this structure of the

1002
00:58:26,919 --> 00:58:30,219
pigeons he exists this structure of the fourth contract ability which implies

1003
00:58:30,219 --> 00:58:34,479
fourth contract ability which implies uniform expansion so for the universe

1004
00:58:34,479 --> 00:58:36,669
uniform expansion so for the universe slash and we should talk about two

1005
00:58:36,669 --> 00:58:40,630
slash and we should talk about two definitions first of all first of all it

1006
00:58:40,630 --> 00:58:43,870
definitions first of all first of all it is the definition of the copies for I

1007
00:58:43,870 --> 00:58:48,929
is the definition of the copies for I and J in a structure view we take that

1008
00:58:48,929 --> 00:58:52,239
and J in a structure view we take that iron general cookies with each other if

1009
00:58:52,239 --> 00:58:55,870
iron general cookies with each other if for all K as the member of s minus I

1010
00:58:55,870 --> 00:59:02,380
for all K as the member of s minus I minus J we have these two expressions

1011
00:59:02,380 --> 00:59:05,260
minus J we have these two expressions so because there are upper of example

1012
00:59:05,260 --> 00:59:07,960
so because there are upper of example choice of memories for K I didn't have

1013
00:59:07,960 --> 00:59:10,090
choice of memories for K I didn't have stuff perfectly identified tough stuff

1014
00:59:10,090 --> 00:59:13,300
stuff perfectly identified tough stuff he and K identical glands of milk so

1015
00:59:13,300 --> 00:59:17,590
he and K identical glands of milk so they say it's her niche content that the

1016
00:59:17,590 --> 00:59:20,080
they say it's her niche content that the probability that the reader uses a type

1017
00:59:20,080 --> 00:59:24,190
probability that the reader uses a type of beverage in this scenario it is the

1018
00:59:24,190 --> 00:59:26,890
of beverage in this scenario it is the same as if they was shown just only one

1019
00:59:26,890 --> 00:59:32,940
same as if they was shown just only one cup of some sort of the beverage and

1020
00:59:32,940 --> 00:59:36,130
cup of some sort of the beverage and regardless of the the values of k equal

1021
00:59:36,130 --> 00:59:39,640
regardless of the the values of k equal to or greater than one definition to is

1022
00:59:39,640 --> 00:59:43,000
to or greater than one definition to is the uniform expansion because there a

1023
00:59:43,000 --> 00:59:46,720
the uniform expansion because there a choice between an element in a set as

1024
00:59:46,720 --> 00:59:49,390
choice between an element in a set as one from Iowa one to iron one and

1025
00:59:49,390 --> 00:59:52,480
one from Iowa one to iron one and another choice promise that okay

1026
00:59:52,480 --> 00:59:54,940
another choice promise that okay containing K cookie love each of those

1027
00:59:54,940 --> 00:59:58,930
containing K cookie love each of those and n elements the axiom of uniform

1028
00:59:58,930 --> 01:00:02,230
and n elements the axiom of uniform expansion States as put each M and all K

1029
01:00:02,230 --> 01:00:04,540
expansion States as put each M and all K greater than or equal to 1 we have this

1030
01:00:04,540 --> 01:00:09,390
greater than or equal to 1 we have this expression

1031
01:00:09,390 --> 01:00:09,400


1032
01:00:09,400 --> 01:00:15,510
so we conclude that PTSD model only exhibiting more general property of

1033
01:00:15,510 --> 01:00:20,550
exhibiting more general property of contract ability which it's always

1034
01:00:20,550 --> 01:00:27,060
contract ability which it's always exhibit uniform expansion so me I want

1035
01:00:27,060 --> 01:00:31,800
exhibit uniform expansion so me I want to hear bring the preposition tree for

1036
01:00:31,800 --> 01:00:36,510
to hear bring the preposition tree for given gamma which is the lambda IJ let

1037
01:00:36,510 --> 01:00:39,870
given gamma which is the lambda IJ let a1 to a2 be a contractible partition for

1038
01:00:39,870 --> 01:00:43,290
a1 to a2 be a contractible partition for to PT and similar than you represented

1039
01:00:43,290 --> 01:00:45,600
to PT and similar than you represented by Q and Q Prime with the station

1040
01:00:45,600 --> 01:00:48,840
by Q and Q Prime with the station distribution Python then for any AI we

1041
01:00:48,840 --> 01:00:51,000
distribution Python then for any AI we have this expression or equivalently we

1042
01:00:51,000 --> 01:00:58,470
have this expression or equivalently we say that PI AI is equal to - hey Ari so

1043
01:00:58,470 --> 01:01:02,610
say that PI AI is equal to - hey Ari so for the proof these patrols that Q has

1044
01:01:02,610 --> 01:01:06,620
for the proof these patrols that Q has partition a 1 to a K with respect to

1045
01:01:06,620 --> 01:01:10,290
partition a 1 to a K with respect to gamma if it be composed the balanced

1046
01:01:10,290 --> 01:01:12,990
gamma if it be composed the balanced equation each law of the future virtue

1047
01:01:12,990 --> 01:01:16,170
equation each law of the future virtue which is equal to 0 for each member of a

1048
01:01:16,170 --> 01:01:18,450
which is equal to 0 for each member of a 1 without loss of generality we can

1049
01:01:18,450 --> 01:01:25,180
1 without loss of generality we can obtain these expression so

1050
01:01:25,180 --> 01:01:25,190


1051
01:01:25,190 --> 01:01:31,329
here noting that for the a small area in India the member of Capital Area and gay

1052
01:01:31,329 --> 01:01:38,010
India the member of Capital Area and gay and AJ is equal to lambda IJ here we can

1053
01:01:38,010 --> 01:01:41,579
and AJ is equal to lambda IJ here we can we can actually decompose it

1054
01:01:41,579 --> 01:01:45,700
we can actually decompose it multivariate each coil 60 each of these

1055
01:01:45,700 --> 01:01:48,910
multivariate each coil 60 each of these terms in this expression and then coming

1056
01:01:48,910 --> 01:01:52,660
terms in this expression and then coming over act as a member of a one we we will

1057
01:01:52,660 --> 01:01:57,160
over act as a member of a one we we will reach this this long expression the

1058
01:01:57,160 --> 01:02:00,640
reach this this long expression the leftmost Titanic II studies equal so

1059
01:02:00,640 --> 01:02:03,760
leftmost Titanic II studies equal so this one and this one so we have we can

1060
01:02:03,760 --> 01:02:11,200
this one and this one so we have we can get the PA one which which makes the pay

1061
01:02:11,200 --> 01:02:17,220
get the PA one which which makes the pay feed upon a one as the solution to

1062
01:02:17,220 --> 01:02:17,230


1063
01:02:17,230 --> 01:02:22,990
balanced equation the above proposition and the concert the contract the ability

1064
01:02:22,990 --> 01:02:26,260
and the concert the contract the ability of efficiency model so it implies that

1065
01:02:26,260 --> 01:02:30,069
of efficiency model so it implies that the PDMP model exhibits uniform

1066
01:02:30,069 --> 01:02:34,359
the PDMP model exhibits uniform expansion

1067
01:02:34,359 --> 01:02:34,369


1068
01:02:34,369 --> 01:02:40,250
Indians inference and prediction part our ultimate goal is through berating it

1069
01:02:40,250 --> 01:02:42,950
our ultimate goal is through berating it model to make predictions using past

1070
01:02:42,950 --> 01:02:45,800
model to make predictions using past choices from different subsets to

1071
01:02:45,800 --> 01:02:49,340
choices from different subsets to predict future choices there are three

1072
01:02:49,340 --> 01:02:51,680
predict future choices there are three part three accept India in the

1073
01:02:51,680 --> 01:02:55,910
part three accept India in the prediction part leaving the no blank

1074
01:02:55,910 --> 01:03:00,859
prediction part leaving the no blank function which is the rate matrix Q

1075
01:03:00,859 --> 01:03:03,710
function which is the rate matrix Q given a choice data collection of the

1076
01:03:03,710 --> 01:03:09,950
given a choice data collection of the form T where the I K is the member of SK

1077
01:03:09,950 --> 01:03:12,040
form T where the I K is the member of SK was the eyes and chosen from escaped

1078
01:03:12,040 --> 01:03:14,330
was the eyes and chosen from escaped investigating the ability of learnt each

1079
01:03:14,330 --> 01:03:15,650
investigating the ability of learnt each day and tomorrow to make choice

1080
01:03:15,650 --> 01:03:18,440
day and tomorrow to make choice prediction on empirical data benchmark

1081
01:03:18,440 --> 01:03:24,260
prediction on empirical data benchmark against learn ml and then interpreting

1082
01:03:24,260 --> 01:03:30,530
against learn ml and then interpreting the informative promises to have for the

1083
01:03:30,530 --> 01:03:31,670
the informative promises to have for the muscular fluids

1084
01:03:31,670 --> 01:03:38,470
muscular fluids let's define T is as the number of items

1085
01:03:38,470 --> 01:03:42,109
let's define T is as the number of items number of times in the data that I was

1086
01:03:42,109 --> 01:03:45,920
number of times in the data that I was chosen from the success I also see it as

1087
01:03:45,920 --> 01:03:49,280
chosen from the success I also see it as the number of times that X was chosen

1088
01:03:49,280 --> 01:03:56,240
the number of times that X was chosen for each as discussed above you so there

1089
01:03:56,240 --> 01:04:00,800
for each as discussed above you so there are two recordings here for each as if P

1090
01:04:00,800 --> 01:04:04,609
are two recordings here for each as if P is a Q is the probability for the

1091
01:04:04,609 --> 01:04:07,400
is a Q is the probability for the selection of s as a function of the rate

1092
01:04:07,400 --> 01:04:11,120
selection of s as a function of the rate market q and after dropping all the

1093
01:04:11,120 --> 01:04:13,310
market q and after dropping all the constants the loaf likely would have Q

1094
01:04:13,310 --> 01:04:16,040
constants the loaf likely would have Q given the data T which have been here

1095
01:04:16,040 --> 01:04:19,630
given the data T which have been here derived from the probability

1096
01:04:19,630 --> 01:04:21,999
derived from the probability function of the multinomial distribution

1097
01:04:21,999 --> 01:04:27,729
function of the multinomial distribution which is and we get this expression and

1098
01:04:27,729 --> 01:04:30,739
which is and we get this expression and for the recording to the 45th entry

1099
01:04:30,739 --> 01:04:35,180
for the recording to the 45th entry model we have we want to be we have D

1100
01:04:35,180 --> 01:04:38,509
model we have we want to be we have D the purifier is the attention

1101
01:04:38,509 --> 01:04:39,949
the purifier is the attention distribution for the continuous time

1102
01:04:39,949 --> 01:04:45,769
distribution for the continuous time Markov chain and actually they have they

1103
01:04:45,769 --> 01:04:48,620
Markov chain and actually they have they have used the non taped problem

1104
01:04:48,620 --> 01:04:51,680
have used the non taped problem optimization problem and if anyone is

1105
01:04:51,680 --> 01:04:53,690
optimization problem and if anyone is interested just for the informational

1106
01:04:53,690 --> 01:04:57,789
interested just for the informational thing that they have used the SLS Q

1107
01:04:57,789 --> 01:05:00,259
thing that they have used the SLS Q programming the credential it is for

1108
01:05:00,259 --> 01:05:02,539
programming the credential it is for programming in order to optimize them

1109
01:05:02,539 --> 01:05:06,019
programming in order to optimize them they use the type file that minimize a

1110
01:05:06,019 --> 01:05:10,249
they use the type file that minimize a function in the Python or you can use

1111
01:05:10,249 --> 01:05:15,579
function in the Python or you can use the minimize the method the security

1112
01:05:15,579 --> 01:05:15,589


1113
01:05:15,589 --> 01:05:21,199
there the empirical data results they evaluate the procedure and three

1114
01:05:21,199 --> 01:05:25,630
evaluate the procedure and three empirical data sets from surveillance

1115
01:05:25,630 --> 01:05:28,729
empirical data sets from surveillance survey of Transportation around the San

1116
01:05:28,729 --> 01:05:31,269
survey of Transportation around the San Francisco Bay Area

1117
01:05:31,269 --> 01:05:35,589
Francisco Bay Area the San Francisco war is contained

1118
01:05:35,589 --> 01:05:38,299
the San Francisco war is contained twelve thousand twenty nine of the

1119
01:05:38,299 --> 01:05:40,219
twelve thousand twenty nine of the relation consists of community options

1120
01:05:40,219 --> 01:05:42,380
relation consists of community options and the choice mode on a given community

1121
01:05:42,380 --> 01:05:48,499
and the choice mode on a given community and as we shall contain 3157

1122
01:05:48,499 --> 01:05:51,079
and as we shall contain 3157 observations each consistent choice set

1123
01:05:51,079 --> 01:05:52,849
observations each consistent choice set of transportation alternatives available

1124
01:05:52,849 --> 01:05:55,969
of transportation alternatives available to individuals traveling and returning

1125
01:05:55,969 --> 01:05:59,979
to individuals traveling and returning from a shopping center

1126
01:05:59,979 --> 01:05:59,989


1127
01:05:59,989 --> 01:06:06,309
for the training for turning our modern observation she trained and evaluate

1128
01:06:06,309 --> 01:06:07,670
observation she trained and evaluate unattested

1129
01:06:07,670 --> 01:06:10,729
unattested teachers this is be the bearer of

1130
01:06:10,729 --> 01:06:12,769
teachers this is be the bearer of teacher and teachers that we want to

1131
01:06:12,769 --> 01:06:14,930
teacher and teachers that we want to evaluate actually comparative kitchens

1132
01:06:14,930 --> 01:06:18,459
evaluate actually comparative kitchens amount of a conditional be others model

1133
01:06:18,459 --> 01:06:21,859
amount of a conditional be others model beach in in this expression the Q

1134
01:06:21,859 --> 01:06:23,509
beach in in this expression the Q hottest teacher an estimate for view

1135
01:06:23,509 --> 01:06:25,660
hottest teacher an estimate for view obtained from observation of kitchen and

1136
01:06:25,660 --> 01:06:29,209
obtained from observation of kitchen and P tilde is theta is equal to cos

1137
01:06:29,209 --> 01:06:31,670
P tilde is theta is equal to cos teachers who were skilled as of today

1138
01:06:31,670 --> 01:06:34,279
teachers who were skilled as of today which is the empirical probability of I

1139
01:06:34,279 --> 01:06:37,130
which is the empirical probability of I was selected from as among of the ratio

1140
01:06:37,130 --> 01:06:47,329
was selected from as among of the ratio cheetahs this is actually I have done it

1141
01:06:47,329 --> 01:06:49,939
cheetahs this is actually I have done it for first of all for some Frances to

1142
01:06:49,939 --> 01:06:53,420
for first of all for some Frances to work data and first the number of

1143
01:06:53,420 --> 01:06:56,120
work data and first the number of simulations wrong it's ten this is the

1144
01:06:56,120 --> 01:06:58,430
simulations wrong it's ten this is the comparison between the error versus the

1145
01:06:58,430 --> 01:07:00,199
comparison between the error versus the percentage of data used for training

1146
01:07:00,199 --> 01:07:04,009
percentage of data used for training from five to seventy five percent

1147
01:07:04,009 --> 01:07:05,689
from five to seventy five percent nineteen seventy five percent of the

1148
01:07:05,689 --> 01:07:08,920
nineteen seventy five percent of the data for training and 25 for the test

1149
01:07:08,920 --> 01:07:11,989
data for training and 25 for the test and fifteen see model is doing great job

1150
01:07:11,989 --> 01:07:14,749
and fifteen see model is doing great job hearing in comparison to mmm a nano

1151
01:07:14,749 --> 01:07:17,959
hearing in comparison to mmm a nano mixer manner and his are they the heat

1152
01:07:17,959 --> 01:07:22,069
mixer manner and his are they the heat match for the forty probability for each

1153
01:07:22,069 --> 01:07:25,789
match for the forty probability for each side actually horizontal and vertical we

1154
01:07:25,789 --> 01:07:27,519
side actually horizontal and vertical we have dick

1155
01:07:27,519 --> 01:07:31,759
have dick we have picked pale of the options which

1156
01:07:31,759 --> 01:07:35,390
we have picked pale of the options which are driving alone and sharing around

1157
01:07:35,390 --> 01:07:37,160
are driving alone and sharing around with one another first to work in public

1158
01:07:37,160 --> 01:07:39,380
with one another first to work in public transit biking and carpooling with at

1159
01:07:39,380 --> 01:07:42,979
transit biking and carpooling with at least two others and we are showing the

1160
01:07:42,979 --> 01:07:46,760
least two others and we are showing the probability of need relative risk rate

1161
01:07:46,760 --> 01:07:48,980
probability of need relative risk rate between pairs of items as well as how

1162
01:07:48,980 --> 01:07:50,900
between pairs of items as well as how the total rates between first and

1163
01:07:50,900 --> 01:07:56,990
the total rates between first and personal tutor editing other page here I

1164
01:07:56,990 --> 01:08:01,700
personal tutor editing other page here I did the simulation for number of equal

1165
01:08:01,700 --> 01:08:07,420
did the simulation for number of equal to 50 so when we activate when we

1166
01:08:07,420 --> 01:08:10,130
to 50 so when we activate when we increase the number of stimulation it

1167
01:08:10,130 --> 01:08:18,440
increase the number of stimulation it will be so in the previous one with the

1168
01:08:18,440 --> 01:08:21,830
will be so in the previous one with the tenth simulation we see here that first

1169
01:08:21,830 --> 01:08:25,490
tenth simulation we see here that first of our TNT model starts with little more

1170
01:08:25,490 --> 01:08:29,900
of our TNT model starts with little more but then we increase the increase the

1171
01:08:29,900 --> 01:08:33,860
but then we increase the increase the number of simulations they start on

1172
01:08:33,860 --> 01:08:39,200
number of simulations they start on something the same but after that

1173
01:08:39,200 --> 01:08:41,510
something the same but after that efficiency model in comparison to other

1174
01:08:41,510 --> 01:08:46,280
efficiency model in comparison to other mothers during the great job so even in

1175
01:08:46,280 --> 01:08:50,720
mothers during the great job so even in data with violations of iia that was the

1176
01:08:50,720 --> 01:08:54,140
data with violations of iia that was the that was known for the lucia first axiom

1177
01:08:54,140 --> 01:08:56,120
that was known for the lucia first axiom that i talked about lately which is the

1178
01:08:56,120 --> 01:08:58,210
that i talked about lately which is the independence relevant of alternatives

1179
01:08:58,210 --> 01:09:02,030
independence relevant of alternatives PMT does 20 to 30 percent better at

1180
01:09:02,030 --> 01:09:04,430
PMT does 20 to 30 percent better at prediction out-of-sample without

1181
01:09:04,430 --> 01:09:07,490
prediction out-of-sample without violations PT and is followed back to a

1182
01:09:07,490 --> 01:09:13,790
violations PT and is followed back to a manner and is it for san francisco shock

1183
01:09:13,790 --> 01:09:19,090
manner and is it for san francisco shock data and again visit for eight of eight

1184
01:09:19,090 --> 01:09:22,910
data and again visit for eight of eight actually ordered options and previous

1185
01:09:22,910 --> 01:09:27,290
actually ordered options and previous almost 60 so here again the 15 T model

1186
01:09:27,290 --> 01:09:34,269
almost 60 so here again the 15 T model is doing better than two other models

1187
01:09:34,269 --> 01:09:34,279


1188
01:09:34,279 --> 01:09:40,700
so which we can say that error on the data as the learning procedures applied

1189
01:09:40,700 --> 01:09:43,579
data as the learning procedures applied to increase the amount of data the

1190
01:09:43,579 --> 01:09:45,590
to increase the amount of data the results are averaged over 1,000

1191
01:09:45,590 --> 01:09:47,870
results are averaged over 1,000 different permutations of the data to be

1192
01:09:47,870 --> 01:09:51,189
different permutations of the data to be in the 7520 upon strain and text data

1193
01:09:51,189 --> 01:09:54,860
in the 7520 upon strain and text data prediction error on a 25 percent hold of

1194
01:09:54,860 --> 01:09:58,490
prediction error on a 25 percent hold of the data for the PNC M&amp;L and mmm mix

1195
01:09:58,490 --> 01:10:01,880
the data for the PNC M&amp;L and mmm mix terminal models have been shown and 50

1196
01:10:01,880 --> 01:10:05,209
terminal models have been shown and 50 mg mother's these improvement of thirty

1197
01:10:05,209 --> 01:10:07,729
mg mother's these improvement of thirty five point nine percent and twenty four

1198
01:10:07,729 --> 01:10:09,709
five point nine percent and twenty four point five percent in prediction error

1199
01:10:09,709 --> 01:10:12,830
point five percent in prediction error over M&amp;L and mix terminal respectively

1200
01:10:12,830 --> 01:10:15,890
over M&amp;L and mix terminal respectively when training on seven five percent as a

1201
01:10:15,890 --> 01:10:19,729
when training on seven five percent as a danger for this summary we introduced a

1202
01:10:19,729 --> 01:10:22,130
danger for this summary we introduced a very wise choice Markov chain models and

1203
01:10:22,130 --> 01:10:24,860
very wise choice Markov chain models and the speed choice which define selection

1204
01:10:24,860 --> 01:10:26,780
the speed choice which define selection properties according to the assertion

1205
01:10:26,780 --> 01:10:30,020
properties according to the assertion distribution of a continuous time Markov

1206
01:10:30,020 --> 01:10:34,340
distribution of a continuous time Markov chains the flexibility for which the

1207
01:10:34,340 --> 01:10:37,760
chains the flexibility for which the flexibility we demonstrate that tthe

1208
01:10:37,760 --> 01:10:42,950
flexibility we demonstrate that tthe model exhibits desirable structure was

1209
01:10:42,950 --> 01:10:45,740
model exhibits desirable structure was preceding the uniform expansion property

1210
01:10:45,740 --> 01:10:48,740
preceding the uniform expansion property previously found only in the M&amp;L model

1211
01:10:48,740 --> 01:10:52,729
previously found only in the M&amp;L model and elimination by aspects model the

1212
01:10:52,729 --> 01:10:54,200
and elimination by aspects model the working on states that the kitchen

1213
01:10:54,200 --> 01:10:55,959
working on states that the kitchen remodel exhibits concern

1214
01:10:55,959 --> 01:10:58,280
remodel exhibits concern constructability which implies uniform

1215
01:10:58,280 --> 01:11:00,950
constructability which implies uniform expansion we have to show that the

1216
01:11:00,950 --> 01:11:02,600
expansion we have to show that the pigeons amount of offers

1217
01:11:02,600 --> 01:11:06,200
pigeons amount of offers thenceforward inference through Emily

1218
01:11:06,200 --> 01:11:08,540
thenceforward inference through Emily and that will learn the agency model

1219
01:11:08,540 --> 01:11:09,290
and that will learn the agency model predicts

1220
01:11:09,290 --> 01:11:11,180
predicts improve her choice it will be

1221
01:11:11,180 --> 01:11:14,149
improve her choice it will be significantly higher utility the respect

1222
01:11:14,149 --> 01:11:16,830
significantly higher utility the respect to two other models

1223
01:11:16,830 --> 01:11:18,959
to two other models for the future because this is a very

1224
01:11:18,959 --> 01:11:23,970
for the future because this is a very novel idea by the sample group the

1225
01:11:23,970 --> 01:11:27,479
novel idea by the sample group the possibility and tractability of the PCs

1226
01:11:27,479 --> 01:11:30,330
possibility and tractability of the PCs the model opens up many computing

1227
01:11:30,330 --> 01:11:33,000
the model opens up many computing research directions the efficiency of

1228
01:11:33,000 --> 01:11:35,069
research directions the efficiency of the TMC model suggests exploring other

1229
01:11:35,069 --> 01:11:37,470
the TMC model suggests exploring other effective parameterization for q the

1230
01:11:37,470 --> 01:11:40,080
effective parameterization for q the rate matrix including developing L

1231
01:11:40,080 --> 01:11:42,649
rate matrix including developing L special methods which excludability

1232
01:11:42,649 --> 01:11:45,149
special methods which excludability there are two open computational

1233
01:11:45,149 --> 01:11:47,879
there are two open computational questions such as extremely large lining

1234
01:11:47,879 --> 01:11:50,189
questions such as extremely large lining the large Luud maximization using

1235
01:11:50,189 --> 01:11:53,640
the large Luud maximization using gradients of the implicit function

1236
01:11:53,640 --> 01:11:57,510
gradients of the implicit function definitions and these are the references

1237
01:11:57,510 --> 01:12:02,760
definitions and these are the references for for this room thank you

1238
01:12:02,760 --> 01:12:02,770


1239
01:12:02,770 --> 01:12:06,020
  [Applause] 

