1
00:03:29,150 --> 00:03:34,830
right

2
00:03:34,830 --> 00:03:34,840


3
00:03:34,840 --> 00:03:42,570
so we're supposed to start a new topic that is not only applicable to

4
00:03:42,570 --> 00:03:45,809
that is not only applicable to probabilistic graphical models but to

5
00:03:45,809 --> 00:03:49,290
probabilistic graphical models but to any sort of Bayesian inference and that

6
00:03:49,290 --> 00:03:53,430
any sort of Bayesian inference and that is with school variational method to

7
00:03:53,430 --> 00:03:55,860
is with school variational method to approximate posterior distributions okay

8
00:03:55,860 --> 00:04:00,750
approximate posterior distributions okay and we will do some elementary examples

9
00:04:00,750 --> 00:04:03,509
and we will do some elementary examples today maybe some applied to

10
00:04:03,509 --> 00:04:07,770
today maybe some applied to probabilistic graphical models and there

11
00:04:07,770 --> 00:04:09,600
probabilistic graphical models and there is a lots of this topic store maybe we

12
00:04:09,600 --> 00:04:12,750
is a lots of this topic store maybe we will spend another two lectures or three

13
00:04:12,750 --> 00:04:14,970
will spend another two lectures or three lectures depending how exhausted you

14
00:04:14,970 --> 00:04:17,819
lectures depending how exhausted you seem to be you know in the days ahead

15
00:04:17,819 --> 00:04:24,659
seem to be you know in the days ahead so try not to sleep okay all right I

16
00:04:24,659 --> 00:04:27,150
so try not to sleep okay all right I believe Subic sent you the notes but I

17
00:04:27,150 --> 00:04:28,860
believe Subic sent you the notes but I will post them if you find any mistakes

18
00:04:28,860 --> 00:04:35,790
will post them if you find any mistakes on this nas please email me so we're

19
00:04:35,790 --> 00:04:38,909
on this nas please email me so we're back to the standard problem where we

20
00:04:38,909 --> 00:04:43,020
back to the standard problem where we have some observed data that I denote

21
00:04:43,020 --> 00:04:45,840
have some observed data that I denote here with text Z are the latent

22
00:04:45,840 --> 00:04:48,540
here with text Z are the latent variables and we want to compute the

23
00:04:48,540 --> 00:04:50,100
variables and we want to compute the posterior of the latent variables given

24
00:04:50,100 --> 00:04:54,659
posterior of the latent variables given the data the light came on and it seems

25
00:04:54,659 --> 00:04:56,850
the data the light came on and it seems the camera I am assuming should be

26
00:04:56,850 --> 00:05:02,580
the camera I am assuming should be working okay it was not exactly when

27
00:05:02,580 --> 00:05:07,469
working okay it was not exactly when it's supposed to be it was dead natural

28
00:05:07,469 --> 00:05:14,990
it's supposed to be it was dead natural not with a miracle yes thank you guys

29
00:05:14,990 --> 00:05:15,000


30
00:05:15,000 --> 00:05:23,540
so so the probability of G of X Bayes rule basically is given like that so the

31
00:05:23,540 --> 00:05:27,060
rule basically is given like that so the most of the machine learning subject

32
00:05:27,060 --> 00:05:30,570
most of the machine learning subject area is to compute this posterior okay

33
00:05:30,570 --> 00:05:33,000
area is to compute this posterior okay now in addition to G you may have all

34
00:05:33,000 --> 00:05:34,590
now in addition to G you may have all the parameters the way we have seen up

35
00:05:34,590 --> 00:05:36,450
the parameters the way we have seen up to now but and we will do some examples

36
00:05:36,450 --> 00:05:45,180
to now but and we will do some examples but let's cross the credit MC so we are

37
00:05:45,180 --> 00:05:46,830
but let's cross the credit MC so we are assuming that this post is not very easy

38
00:05:46,830 --> 00:05:50,190
assuming that this post is not very easy to compute or you know it's not maybe

39
00:05:50,190 --> 00:05:52,110
to compute or you know it's not maybe even given in any other litical form

40
00:05:52,110 --> 00:05:55,560
even given in any other litical form maybe it's a computer code and somehow

41
00:05:55,560 --> 00:05:57,450
maybe it's a computer code and somehow we're gonna try today to discuss

42
00:05:57,450 --> 00:06:00,560
we're gonna try today to discuss methodology square we can have some

43
00:06:00,560 --> 00:06:04,320
methodology square we can have some approximation of the posterior that they

44
00:06:04,320 --> 00:06:06,600
approximation of the posterior that they have explicitly not sir so you can think

45
00:06:06,600 --> 00:06:09,300
have explicitly not sir so you can think you know taking a general of course

46
00:06:09,300 --> 00:06:11,490
you know taking a general of course tyranny approximating it as a Gaussian I

47
00:06:11,490 --> 00:06:14,010
tyranny approximating it as a Gaussian I mean that can be in a simple example

48
00:06:14,010 --> 00:06:16,550
mean that can be in a simple example maybe a Gaussian won't be enough

49
00:06:16,550 --> 00:06:18,990
maybe a Gaussian won't be enough we don't be enough because maybe these

50
00:06:18,990 --> 00:06:21,480
we don't be enough because maybe these very high dimensional so that you can

51
00:06:21,480 --> 00:06:23,580
very high dimensional so that you can think this in the examples we did with

52
00:06:23,580 --> 00:06:26,070
think this in the examples we did with graphs Z can involve many variables

53
00:06:26,070 --> 00:06:28,650
graphs Z can involve many variables approximating the Joint Distribution of

54
00:06:28,650 --> 00:06:33,320
approximating the Joint Distribution of C as a Gaussian wave is not a good idea

55
00:06:33,320 --> 00:06:37,200
C as a Gaussian wave is not a good idea all right

56
00:06:37,200 --> 00:06:37,210


57
00:06:37,210 --> 00:06:43,700
so there are methods of doing this and

58
00:06:43,700 --> 00:06:43,710


59
00:06:43,710 --> 00:06:51,120
so one is basically to approximate the posterior with samples and in some sense

60
00:06:51,120 --> 00:06:54,840
posterior with samples and in some sense we discuss this a little bit in the

61
00:06:54,840 --> 00:06:57,659
we discuss this a little bit in the context of the previous lecture when I

62
00:06:57,659 --> 00:07:00,180
context of the previous lecture when I refer to particle methods basically to

63
00:07:00,180 --> 00:07:03,180
refer to particle methods basically to compute the predictive and the smoking

64
00:07:03,180 --> 00:07:08,189
compute the predictive and the smoking distribution in in state-space models so

65
00:07:08,189 --> 00:07:09,600
distribution in in state-space models so this is using on the counter

66
00:07:09,600 --> 00:07:12,930
this is using on the counter attractively right and the method of

67
00:07:12,930 --> 00:07:16,980
attractively right and the method of well are discuss today is basically a

68
00:07:16,980 --> 00:07:20,900
well are discuss today is basically a deterministic method in many ways and

69
00:07:20,900 --> 00:07:23,339
deterministic method in many ways and small service category of what we call

70
00:07:23,339 --> 00:07:25,379
small service category of what we call various turn on the inference so we're

71
00:07:25,379 --> 00:07:27,870
various turn on the inference so we're gonna try to approximate the posterior

72
00:07:27,870 --> 00:07:32,820
gonna try to approximate the posterior with some form that is sort of

73
00:07:32,820 --> 00:07:35,490
with some form that is sort of computational easy to work with and that

74
00:07:35,490 --> 00:07:37,080
computational easy to work with and that prong is going to involve the

75
00:07:37,080 --> 00:07:40,170
prong is going to involve the factorization so an approximation that

76
00:07:40,170 --> 00:07:45,659
factorization so an approximation that factorizes this in distribution software

77
00:07:45,659 --> 00:07:47,700
factorizes this in distribution software each of the variables to disease so

78
00:07:47,700 --> 00:07:50,730
each of the variables to disease so let's say if Z is three variables we're

79
00:07:50,730 --> 00:07:53,820
let's say if Z is three variables we're going to proximate this as q1 of Z 1 and

80
00:07:53,820 --> 00:07:57,120
going to proximate this as q1 of Z 1 and Q 2 of Z 2 times 2 3 of Z 3 and then try

81
00:07:57,120 --> 00:08:00,140
Q 2 of Z 2 times 2 3 of Z 3 and then try to persuade each of this Q I

82
00:08:00,140 --> 00:08:03,180
to persuade each of this Q I distribution we will say that this

83
00:08:03,180 --> 00:08:05,879
distribution we will say that this actually leads to methodologies already

84
00:08:05,879 --> 00:08:09,629
actually leads to methodologies already known in statistical mechanics for you

85
00:08:09,629 --> 00:08:12,450
known in statistical mechanics for you know centuries basically so in many ways

86
00:08:12,450 --> 00:08:15,060
know centuries basically so in many ways these methods are coming directly from

87
00:08:15,060 --> 00:08:19,620
these methods are coming directly from physics ok and and sort of the

88
00:08:19,620 --> 00:08:22,050
physics ok and and sort of the techniques that they applied for simple

89
00:08:22,050 --> 00:08:24,659
techniques that they applied for simple inference problems to probabilistic

90
00:08:24,659 --> 00:08:26,430
inference problems to probabilistic graphical models to deep learning to

91
00:08:26,430 --> 00:08:28,310
graphical models to deep learning to everything to they're very powerful

92
00:08:28,310 --> 00:08:31,159
everything to they're very powerful methods what their approach metrics

93
00:08:31,159 --> 00:08:34,290
methods what their approach metrics approximate method is you get what you

94
00:08:34,290 --> 00:08:36,480
approximate method is you get what you approximated with right so you're not

95
00:08:36,480 --> 00:08:37,880
approximated with right so you're not happy with it too much

96
00:08:37,880 --> 00:08:41,310
happy with it too much okay so in some sense this is a good

97
00:08:41,310 --> 00:08:43,660
okay so in some sense this is a good active area preserved if you want to

98
00:08:43,660 --> 00:08:46,600
active area preserved if you want to come up with something better following

99
00:08:46,600 --> 00:08:47,890
come up with something better following basically the ideas of a rational

100
00:08:47,890 --> 00:08:51,430
basically the ideas of a rational inference all right so let's introduce

101
00:08:51,430 --> 00:08:54,730
inference all right so let's introduce the some of the basic of the problems so

102
00:08:54,730 --> 00:08:57,760
the some of the basic of the problems so we assume we have collected some data x1

103
00:08:57,760 --> 00:09:02,880
we assume we have collected some data x1 to xn we are assuming that we have

104
00:09:02,880 --> 00:09:07,120
to xn we are assuming that we have latent variables Z 1 to Z n so you know

105
00:09:07,120 --> 00:09:10,000
latent variables Z 1 to Z n so you know this year the number of latent variables

106
00:09:10,000 --> 00:09:12,190
this year the number of latent variables is the same as the number of data so you

107
00:09:12,190 --> 00:09:15,750
is the same as the number of data so you can think of a problem where you know

108
00:09:15,750 --> 00:09:18,520
can think of a problem where you know this is you know if this you are talking

109
00:09:18,520 --> 00:09:19,930
this is you know if this you are talking let's say birth of a Gaussian mixture

110
00:09:19,930 --> 00:09:22,930
let's say birth of a Gaussian mixture for its extra ZZ that tells you what

111
00:09:22,930 --> 00:09:25,600
for its extra ZZ that tells you what from what component of the mixture this

112
00:09:25,600 --> 00:09:28,870
from what component of the mixture this is coming so we have n data points and

113
00:09:28,870 --> 00:09:32,350
is coming so we have n data points and and latent parameters but later on we

114
00:09:32,350 --> 00:09:35,410
and latent parameters but later on we can enrich this with static parameters

115
00:09:35,410 --> 00:09:37,800
can enrich this with static parameters beta but right now this is what we have

116
00:09:37,800 --> 00:09:42,340
beta but right now this is what we have this is how Joint Distribution who have

117
00:09:42,340 --> 00:09:44,650
this is how Joint Distribution who have played with it when we work with the e/m

118
00:09:44,650 --> 00:09:47,200
played with it when we work with the e/m algorithm and we want to approximate the

119
00:09:47,200 --> 00:09:52,080
algorithm and we want to approximate the posterior and also the model evidence ok

120
00:09:52,080 --> 00:09:56,080
posterior and also the model evidence ok so we're going to denote the

121
00:09:56,080 --> 00:09:59,350
so we're going to denote the approximation as qz so this is the exact

122
00:09:59,350 --> 00:10:02,590
approximation as qz so this is the exact posterior and this is the approximation

123
00:10:02,590 --> 00:10:05,920
posterior and this is the approximation of that posterior we're going to to try

124
00:10:05,920 --> 00:10:08,530
of that posterior we're going to to try to compute we don't used usually in the

125
00:10:08,530 --> 00:10:11,230
to compute we don't used usually in the notation Q of Z given X right so this is

126
00:10:11,230 --> 00:10:14,320
notation Q of Z given X right so this is the distribution in Z but remember it's

127
00:10:14,320 --> 00:10:15,550
the distribution in Z but remember it's an approximation of a posterior

128
00:10:15,550 --> 00:10:20,500
an approximation of a posterior distribution ok the very fundamental a

129
00:10:20,500 --> 00:10:22,480
distribution ok the very fundamental a question that I have to use today and

130
00:10:22,480 --> 00:10:24,040
question that I have to use today and you need to remember it because it's

131
00:10:24,040 --> 00:10:27,790
you need to remember it because it's essential every quality machine learning

132
00:10:27,790 --> 00:10:29,740
essential every quality machine learning is the question that we see there that

133
00:10:29,740 --> 00:10:35,320
is the question that we see there that it raised the long of the of the

134
00:10:35,320 --> 00:10:37,660
it raised the long of the of the evidence so this is the likelihood of

135
00:10:37,660 --> 00:10:39,490
evidence so this is the likelihood of the data so the log of the length of the

136
00:10:39,490 --> 00:10:43,990
the data so the log of the length of the data is this lower bound which I will

137
00:10:43,990 --> 00:10:46,600
data is this lower bound which I will define shortly the trolley function of Q

138
00:10:46,600 --> 00:10:49,210
define shortly the trolley function of Q plus the cool but liberal distance

139
00:10:49,210 --> 00:10:50,900
plus the cool but liberal distance between Q and P

140
00:10:50,900 --> 00:10:53,690
between Q and P so P is the exact posterior cure is the

141
00:10:53,690 --> 00:10:57,950
so P is the exact posterior cure is the approximation we try to model you may

142
00:10:57,950 --> 00:10:59,750
approximation we try to model you may say if you know why is this useful well

143
00:10:59,750 --> 00:11:01,480
say if you know why is this useful well you will see shortly why is this useful

144
00:11:01,480 --> 00:11:05,390
you will see shortly why is this useful okay so let me define things the lower

145
00:11:05,390 --> 00:11:08,260
okay so let me define things the lower bound is defined basically as the

146
00:11:08,260 --> 00:11:10,070
bound is defined basically as the expectation with respect to the

147
00:11:10,070 --> 00:11:13,360
expectation with respect to the distribution Q of the log of the joint

148
00:11:13,360 --> 00:11:17,020
distribution Q of the log of the joint likelihood divided by Q of Z so this is

149
00:11:17,020 --> 00:11:19,610
likelihood divided by Q of Z so this is the lower bound you may say why do I

150
00:11:19,610 --> 00:11:21,710
the lower bound you may say why do I call it a lower bound we will see that

151
00:11:21,710 --> 00:11:25,130
call it a lower bound we will see that shortly and the KL distance between Q

152
00:11:25,130 --> 00:11:29,150
shortly and the KL distance between Q and P is defined this way so you may

153
00:11:29,150 --> 00:11:32,360
and P is defined this way so you may want to memorize one of the two

154
00:11:32,360 --> 00:11:34,400
want to memorize one of the two definitions with the sign so that you do

155
00:11:34,400 --> 00:11:36,350
definitions with the sign so that you do this properly right so if you put a

156
00:11:36,350 --> 00:11:38,780
this properly right so if you put a minus sign and this is the distance

157
00:11:38,780 --> 00:11:40,940
minus sign and this is the distance between Q and P Q will come there and

158
00:11:40,940 --> 00:11:43,340
between Q and P Q will come there and there and this would be the the exact

159
00:11:43,340 --> 00:11:46,580
there and this would be the the exact distribution it rise approximate if you

160
00:11:46,580 --> 00:11:48,830
distribution it rise approximate if you want to put the sign in tight and get

161
00:11:48,830 --> 00:11:50,600
want to put the sign in tight and get rid of it basically you're going to have

162
00:11:50,600 --> 00:11:53,480
rid of it basically you're going to have to reverse the order and have QC devices

163
00:11:53,480 --> 00:11:54,680
to reverse the order and have QC devices by PLC effects

164
00:11:54,680 --> 00:12:01,160
by PLC effects okay everybody familiar with the KL

165
00:12:01,160 --> 00:12:03,680
okay everybody familiar with the KL distance of distributions okay it's not

166
00:12:03,680 --> 00:12:06,440
distance of distributions okay it's not a real distance okay

167
00:12:06,440 --> 00:12:10,670
a real distance okay it is a non-symmetric quantity we will

168
00:12:10,670 --> 00:12:12,860
it is a non-symmetric quantity we will see actually today but if you have the

169
00:12:12,860 --> 00:12:15,260
see actually today but if you have the distance between P and Q is not equal to

170
00:12:15,260 --> 00:12:17,600
distance between P and Q is not equal to this so this is very important for this

171
00:12:17,600 --> 00:12:20,930
this so this is very important for this the distance of Q from P not P from Q

172
00:12:20,930 --> 00:12:23,110
the distance of Q from P not P from Q okay

173
00:12:23,110 --> 00:12:26,390
okay this distance is zero when Q and P are

174
00:12:26,390 --> 00:12:28,850
this distance is zero when Q and P are equal it is greater or equal to zero and

175
00:12:28,850 --> 00:12:30,950
equal it is greater or equal to zero and because it's greater or equal to zero

176
00:12:30,950 --> 00:12:34,090
because it's greater or equal to zero what you see here is greater or equal to

177
00:12:34,090 --> 00:12:35,540
what you see here is greater or equal to 2l q

178
00:12:35,540 --> 00:12:38,120
2l q and that's why we call L Q the lower

179
00:12:38,120 --> 00:12:41,920
and that's why we call L Q the lower bound to the log-likelihood you select

180
00:12:41,920 --> 00:12:44,450
bound to the log-likelihood you select because the KO distance is positive

181
00:12:44,450 --> 00:12:47,750
because the KO distance is positive right so the M of Q is a lower bound

182
00:12:47,750 --> 00:12:51,590
right so the M of Q is a lower bound basically to the log likelihood so if

183
00:12:51,590 --> 00:12:54,440
basically to the log likelihood so if you want really an approximation of the

184
00:12:54,440 --> 00:12:56,000
you want really an approximation of the log likelihood because this is

185
00:12:56,000 --> 00:12:57,430
log likelihood because this is computationally the

186
00:12:57,430 --> 00:13:00,400
computationally the to come thing to do what is very easy to

187
00:13:00,400 --> 00:13:02,410
to come thing to do what is very easy to do is computer love you because it

188
00:13:02,410 --> 00:13:04,810
do is computer love you because it involves the joining of log magnitude

189
00:13:04,810 --> 00:13:08,590
involves the joining of log magnitude here okay and we only have seen in the

190
00:13:08,590 --> 00:13:11,470
here okay and we only have seen in the contest of expectation maximization in

191
00:13:11,470 --> 00:13:13,240
contest of expectation maximization in fact there is a reason you would like to

192
00:13:13,240 --> 00:13:16,360
fact there is a reason you would like to work with the affection Z because for

193
00:13:16,360 --> 00:13:18,190
work with the affection Z because for example if this joint distribution is

194
00:13:18,190 --> 00:13:20,800
example if this joint distribution is Gaussian everything falls nicely and the

195
00:13:20,800 --> 00:13:22,840
Gaussian everything falls nicely and the calculations is easy but working

196
00:13:22,840 --> 00:13:26,230
calculations is easy but working directly with this it requires

197
00:13:26,230 --> 00:13:28,840
directly with this it requires integration in Z and that's not

198
00:13:28,840 --> 00:13:33,570
integration in Z and that's not convenient to do let's prove this

199
00:13:33,570 --> 00:13:39,340
convenient to do let's prove this equation the proof is trivial

200
00:13:39,340 --> 00:13:42,100
equation the proof is trivial so the we use the definition of L of Q

201
00:13:42,100 --> 00:13:44,020
so the we use the definition of L of Q so a lot Q we define it to be like that

202
00:13:44,020 --> 00:13:46,780
so a lot Q we define it to be like that so this is what it is so I'm expanding

203
00:13:46,780 --> 00:13:52,510
so this is what it is so I'm expanding our Q Z log of T of Z of X plus Q of Z

204
00:13:52,510 --> 00:13:57,730
our Q Z log of T of Z of X plus Q of Z log of T of X minus Q of Z log of Q of Z

205
00:13:57,730 --> 00:14:05,590
log of T of X minus Q of Z log of Q of Z all right nice okay now longer effects

206
00:14:05,590 --> 00:14:07,750
all right nice okay now longer effects does not depend on Z so I can put

207
00:14:07,750 --> 00:14:10,600
does not depend on Z so I can put outside the integral and I am assuming Q

208
00:14:10,600 --> 00:14:13,780
outside the integral and I am assuming Q the distribution huge normalize so the

209
00:14:13,780 --> 00:14:16,540
the distribution huge normalize so the integral of Q DZ will give me 1 so what

210
00:14:16,540 --> 00:14:18,640
integral of Q DZ will give me 1 so what I'm gonna get is from this term I'm

211
00:14:18,640 --> 00:14:21,490
I'm gonna get is from this term I'm gonna get the log likelihood and then I

212
00:14:21,490 --> 00:14:22,870
gonna get the log likelihood and then I have basically the rest of the terms

213
00:14:22,870 --> 00:14:26,740
have basically the rest of the terms which is Q log of P of Z X okay this

214
00:14:26,740 --> 00:14:29,890
which is Q log of P of Z X okay this term comes from here and then minus Q

215
00:14:29,890 --> 00:14:34,810
term comes from here and then minus Q log of Q is there so this is minus the

216
00:14:34,810 --> 00:14:39,040
log of Q is there so this is minus the cubot liberal distance this is the log

217
00:14:39,040 --> 00:14:41,170
cubot liberal distance this is the log of the evidence so you can see that the

218
00:14:41,170 --> 00:14:43,000
of the evidence so you can see that the love of the evidence is the lower bound

219
00:14:43,000 --> 00:14:47,290
love of the evidence is the lower bound plus the KL distance of QP okay very

220
00:14:47,290 --> 00:14:52,030
plus the KL distance of QP okay very simple derivation is extremely from the

221
00:14:52,030 --> 00:14:54,340
simple derivation is extremely from the mental side prior to you know your head

222
00:14:54,340 --> 00:14:56,040
mental side prior to you know your head spinning and you say to many equations

223
00:14:56,040 --> 00:14:58,560
spinning and you say to many equations you know

224
00:14:58,560 --> 00:15:04,680
well you know this is you will see it actually in publications this things

225
00:15:04,680 --> 00:15:07,380
actually in publications this things used with zero explanation because you

226
00:15:07,380 --> 00:15:09,810
used with zero explanation because you supposed to know it okay so you don't

227
00:15:09,810 --> 00:15:11,600
supposed to know it okay so you don't explain things that you supposed to know

228
00:15:11,600 --> 00:15:13,590
explain things that you supposed to know all right

229
00:15:13,590 --> 00:15:17,160
all right so we explain why this is a lower bound

230
00:15:17,160 --> 00:15:19,920
so we explain why this is a lower bound okay and this is the picture sort of you

231
00:15:19,920 --> 00:15:24,090
okay and this is the picture sort of you want to remember like the only thing we

232
00:15:24,090 --> 00:15:26,910
want to remember like the only thing we have when we have data is this evidence

233
00:15:26,910 --> 00:15:29,280
have when we have data is this evidence so we're always trying to maximize the

234
00:15:29,280 --> 00:15:30,870
so we're always trying to maximize the evidence of the data so whatever unknown

235
00:15:30,870 --> 00:15:34,350
evidence of the data so whatever unknown parameters we care starts as Z or static

236
00:15:34,350 --> 00:15:36,540
parameters we care starts as Z or static parameters theta the only really really

237
00:15:36,540 --> 00:15:38,760
parameters theta the only really really we can in a Bayesian setting we can

238
00:15:38,760 --> 00:15:41,370
we can in a Bayesian setting we can compute the unknowns is by maximizing

239
00:15:41,370 --> 00:15:45,360
compute the unknowns is by maximizing the evidence okay and we just have seen

240
00:15:45,360 --> 00:15:47,280
the evidence okay and we just have seen that the evidence yes two terms the

241
00:15:47,280 --> 00:15:49,080
that the evidence yes two terms the killing of distance between Q and P and

242
00:15:49,080 --> 00:15:51,690
killing of distance between Q and P and the lower bound so you can think the

243
00:15:51,690 --> 00:15:54,720
the lower bound so you can think the problem in two ways one is you say I'm

244
00:15:54,720 --> 00:15:57,350
problem in two ways one is you say I'm going to maximize the lower bound right

245
00:15:57,350 --> 00:16:00,600
going to maximize the lower bound right if you maximize the lower about and to

246
00:16:00,600 --> 00:16:03,000
if you maximize the lower about and to make it equal to that basically you

247
00:16:03,000 --> 00:16:05,640
make it equal to that basically you minimize the KO distance you bring it to

248
00:16:05,640 --> 00:16:06,750
minimize the KO distance you bring it to zero

249
00:16:06,750 --> 00:16:08,850
zero or you can directly work with the K of

250
00:16:08,850 --> 00:16:12,210
or you can directly work with the K of distance so whatever is computationally

251
00:16:12,210 --> 00:16:15,360
distance so whatever is computationally profitable would be the way to go but as

252
00:16:15,360 --> 00:16:18,450
profitable would be the way to go but as we will see only for trivial toy

253
00:16:18,450 --> 00:16:20,690
we will see only for trivial toy problems we work with the KL distance

254
00:16:20,690 --> 00:16:24,480
problems we work with the KL distance the lower bound is what we have to play

255
00:16:24,480 --> 00:16:31,759
the lower bound is what we have to play with

256
00:16:31,759 --> 00:16:31,769


257
00:16:31,769 --> 00:16:37,009
in the literature depending what book you read you know - what community to

258
00:16:37,009 --> 00:16:40,069
you read you know - what community to discuss things with the terminology for

259
00:16:40,069 --> 00:16:41,420
discuss things with the terminology for the lower bound has different fiscal

260
00:16:41,420 --> 00:16:43,009
the lower bound has different fiscal makes I'm going to give you one of them

261
00:16:43,009 --> 00:16:47,569
makes I'm going to give you one of them in the context of 36 so we said the

262
00:16:47,569 --> 00:16:49,430
in the context of 36 so we said the objective is to maximize the lower bound

263
00:16:49,430 --> 00:16:52,430
objective is to maximize the lower bound you can think of this as minimizing -

264
00:16:52,430 --> 00:16:56,210
you can think of this as minimizing - the lower bound so if you calculate -

265
00:16:56,210 --> 00:16:58,699
the lower bound so if you calculate - the lower bound by the initiative plug

266
00:16:58,699 --> 00:17:02,240
the lower bound by the initiative plug in a - there then you get minus Q over Q

267
00:17:02,240 --> 00:17:05,500
in a - there then you get minus Q over Q that's the entropy of the distribution Q

268
00:17:05,500 --> 00:17:08,780
that's the entropy of the distribution Q and then you have minus Q log of P of X

269
00:17:08,780 --> 00:17:13,510
and then you have minus Q log of P of X and Z and the second term here alright

270
00:17:13,510 --> 00:17:19,909
and Z and the second term here alright you can write it as the expectation with

271
00:17:19,909 --> 00:17:23,659
you can write it as the expectation with respect to Q an energy function that I

272
00:17:23,659 --> 00:17:26,270
respect to Q an energy function that I defined here to be minus the log of the

273
00:17:26,270 --> 00:17:28,909
defined here to be minus the log of the drawing love like the joint likelihood

274
00:17:28,909 --> 00:17:34,760
drawing love like the joint likelihood of X and C so this T here is minus Ln of

275
00:17:34,760 --> 00:17:38,510
of X and C so this T here is minus Ln of P of X MC okay so basically what you

276
00:17:38,510 --> 00:17:42,620
P of X MC okay so basically what you have here is you have this average you

277
00:17:42,620 --> 00:17:46,730
have here is you have this average you know energy minus this entropy and this

278
00:17:46,730 --> 00:17:50,210
know energy minus this entropy and this is basically what people in in physics

279
00:17:50,210 --> 00:17:53,240
is basically what people in in physics called the Helmholtz free energy all

280
00:17:53,240 --> 00:17:55,520
called the Helmholtz free energy all right so the problem really of computing

281
00:17:55,520 --> 00:17:58,250
right so the problem really of computing Q is the same as minimizing the kelvins

282
00:17:58,250 --> 00:18:01,850
Q is the same as minimizing the kelvins free energy okay and actually you know

283
00:18:01,850 --> 00:18:04,400
free energy okay and actually you know you can play out with the definition of

284
00:18:04,400 --> 00:18:07,820
you can play out with the definition of the energy and you can build the problem

285
00:18:07,820 --> 00:18:10,130
the energy and you can build the problem to a form of imaging other sort of

286
00:18:10,130 --> 00:18:12,919
to a form of imaging other sort of Pharisees like the Gibbs energy and the

287
00:18:12,919 --> 00:18:16,190
Pharisees like the Gibbs energy and the like okay so you can immediately see the

288
00:18:16,190 --> 00:18:19,610
like okay so you can immediately see the similarities with with the distal

289
00:18:19,610 --> 00:18:21,600
similarities with with the distal mechanics

290
00:18:21,600 --> 00:18:23,280
mechanics all right I have another interpretation

291
00:18:23,280 --> 00:18:26,100
all right I have another interpretation we don't really need it so rather

292
00:18:26,100 --> 00:18:39,930
we don't really need it so rather elementary a distribution a posterior of

293
00:18:39,930 --> 00:18:42,570
elementary a distribution a posterior of Z given X and we want approximate it

294
00:18:42,570 --> 00:18:46,710
Z given X and we want approximate it with Q of G so this you know high

295
00:18:46,710 --> 00:18:52,370
with Q of G so this you know high dimension also have lots of this and

296
00:18:52,370 --> 00:18:52,380


297
00:18:52,380 --> 00:18:57,720
let's not worry about that parameter you know well actually these are not static

298
00:18:57,720 --> 00:18:59,490
know well actually these are not static parameters so the way it is written here

299
00:18:59,490 --> 00:19:02,580
parameters so the way it is written here it says if you approach babies let's say

300
00:19:02,580 --> 00:19:04,440
it says if you approach babies let's say to be a Gaussian and Omega is the mean

301
00:19:04,440 --> 00:19:05,970
to be a Gaussian and Omega is the mean and the variance of a Gaussian then

302
00:19:05,970 --> 00:19:08,940
and the variance of a Gaussian then effectively you can think of the problem

303
00:19:08,940 --> 00:19:11,370
effectively you can think of the problem of computing this parameter Omega the

304
00:19:11,370 --> 00:19:14,700
of computing this parameter Omega the defining this distribution okay that's

305
00:19:14,700 --> 00:19:17,040
defining this distribution okay that's all what I wanted to do but there is an

306
00:19:17,040 --> 00:19:19,830
all what I wanted to do but there is an example basically that demonstrates this

307
00:19:19,830 --> 00:19:21,420
example basically that demonstrates this before we go to the general of the food

308
00:19:21,420 --> 00:19:24,060
before we go to the general of the food ology so let me show it to you

309
00:19:24,060 --> 00:19:27,480
ology so let me show it to you this simple problem it's really not

310
00:19:27,480 --> 00:19:31,220
this simple problem it's really not variational metal but it demonstrates

311
00:19:31,220 --> 00:19:33,330
variational metal but it demonstrates what exactly we're going to be doing

312
00:19:33,330 --> 00:19:35,940
what exactly we're going to be doing there in a general context so pimples we

313
00:19:35,940 --> 00:19:40,860
there in a general context so pimples we have a distribution that is you know not

314
00:19:40,860 --> 00:19:42,510
have a distribution that is you know not very easy to work with so there is an

315
00:19:42,510 --> 00:19:44,760
very easy to work with so there is an exponential there like a Gaussian but

316
00:19:44,760 --> 00:19:46,950
exponential there like a Gaussian but there is a sigmoid function and the

317
00:19:46,950 --> 00:19:49,860
there is a sigmoid function and the distribution of suppose is this one okay

318
00:19:49,860 --> 00:19:52,560
distribution of suppose is this one okay it looks like that and you say I want to

319
00:19:52,560 --> 00:19:56,010
it looks like that and you say I want to fluctuate this with a nauseam okay now

320
00:19:56,010 --> 00:19:58,590
fluctuate this with a nauseam okay now an obvious way to portray this with the

321
00:19:58,590 --> 00:20:01,130
an obvious way to portray this with the Gaussian is using the Laplace expression

322
00:20:01,130 --> 00:20:03,660
Gaussian is using the Laplace expression can use somebody the Miami with the

323
00:20:03,660 --> 00:20:05,160
can use somebody the Miami with the Laplace approximation or maybe we'll

324
00:20:05,160 --> 00:20:07,290
Laplace approximation or maybe we'll have a gives it two months in this class

325
00:20:07,290 --> 00:20:09,870
have a gives it two months in this class but we use it last fall and it is found

326
00:20:09,870 --> 00:20:12,240
but we use it last fall and it is found that the machine learning so and maybe

327
00:20:12,240 --> 00:20:14,910
that the machine learning so and maybe you can guess from the picture what the

328
00:20:14,910 --> 00:20:18,420
you can guess from the picture what the Laplace approximation is so what's the

329
00:20:18,420 --> 00:20:28,990
Laplace approximation is so what's the Laplace approximation

330
00:20:28,990 --> 00:20:29,000


331
00:20:29,000 --> 00:20:44,190
okay but eventually it's a Gaussian with what mean where is the Gaussian Center

332
00:20:44,190 --> 00:20:44,200


333
00:20:44,200 --> 00:20:50,880
it's centered on the zero under the mean of the of the actual distribution on the

334
00:20:50,880 --> 00:20:53,160
of the of the actual distribution on the main okay I mean it looks to be zero

335
00:20:53,160 --> 00:20:57,750
main okay I mean it looks to be zero here and the covariance is defined using

336
00:20:57,750 --> 00:21:02,940
here and the covariance is defined using what the curvature of the actual

337
00:21:02,940 --> 00:21:06,570
what the curvature of the actual distribution that we should here on the

338
00:21:06,570 --> 00:21:09,930
distribution that we should here on the main okay so basically this yes did you

339
00:21:09,930 --> 00:21:11,669
main okay so basically this yes did you do a Taylor series expansion for

340
00:21:11,669 --> 00:21:13,710
do a Taylor series expansion for expected mean and the second order term

341
00:21:13,710 --> 00:21:16,230
expected mean and the second order term and and effectively you get a Gaussian

342
00:21:16,230 --> 00:21:18,659
and and effectively you get a Gaussian center are out the mean with the correct

343
00:21:18,659 --> 00:21:21,600
center are out the mean with the correct temperature that much is you know the

344
00:21:21,600 --> 00:21:23,880
temperature that much is you know the cabbage is defined by the covariance I'm

345
00:21:23,880 --> 00:21:26,640
cabbage is defined by the covariance I'm sorry the temperature of the the koala

346
00:21:26,640 --> 00:21:29,240
sorry the temperature of the the koala is defined by the curvature of the

347
00:21:29,240 --> 00:21:32,120
is defined by the curvature of the distribution you want to approximate now

348
00:21:32,120 --> 00:21:35,340
distribution you want to approximate now if you see is and we will be discussing

349
00:21:35,340 --> 00:21:38,610
if you see is and we will be discussing these ideas today is actually a

350
00:21:38,610 --> 00:21:41,730
these ideas today is actually a variation approximation where in an

351
00:21:41,730 --> 00:21:43,799
variation approximation where in an elementary way because this is a

352
00:21:43,799 --> 00:21:47,400
elementary way because this is a distribution in a single variable Z you

353
00:21:47,400 --> 00:21:49,289
distribution in a single variable Z you can tell the variation or explanation as

354
00:21:49,289 --> 00:21:52,799
can tell the variation or explanation as one that minimizes the distance between

355
00:21:52,799 --> 00:21:54,720
one that minimizes the distance between Q and P for if we take you to be

356
00:21:54,720 --> 00:21:57,270
Q and P for if we take you to be Gaussian so you want the approachable

357
00:21:57,270 --> 00:21:59,039
Gaussian so you want the approachable distribution to be Gaussian you say what

358
00:21:59,039 --> 00:22:00,780
distribution to be Gaussian you say what is the Gaussian e mais de que el

359
00:22:00,780 --> 00:22:02,970
is the Gaussian e mais de que el districts and noticed a big difference

360
00:22:02,970 --> 00:22:07,500
districts and noticed a big difference you get from the Laplace of rocks which

361
00:22:07,500 --> 00:22:12,900
you get from the Laplace of rocks which significant difference okay so not only

362
00:22:12,900 --> 00:22:16,740
significant difference okay so not only basically there is a difference in the

363
00:22:16,740 --> 00:22:18,720
basically there is a difference in the peak of the distribution but also you

364
00:22:18,720 --> 00:22:20,430
peak of the distribution but also you can see the support of the distribution

365
00:22:20,430 --> 00:22:22,440
can see the support of the distribution is very much different from the support

366
00:22:22,440 --> 00:22:24,630
is very much different from the support of the Laplace props Meister but you

367
00:22:24,630 --> 00:22:27,510
of the Laplace props Meister but you know what indulgence okay so Laplace

368
00:22:27,510 --> 00:22:29,730
know what indulgence okay so Laplace approximation and here is the various

369
00:22:29,730 --> 00:22:34,320
approximation and here is the various approaches and this I think the plot on

370
00:22:34,320 --> 00:22:35,970
approaches and this I think the plot on the right shows what is that the

371
00:22:35,970 --> 00:22:40,539
the right shows what is that the curvature

372
00:22:40,539 --> 00:22:40,549


373
00:22:40,549 --> 00:22:45,469
okay so this is I think is just the temperature of the two distributions

374
00:22:45,469 --> 00:22:47,629
temperature of the two distributions basically showing if this is the Laplace

375
00:22:47,629 --> 00:22:49,819
basically showing if this is the Laplace approximation you know this

376
00:22:49,819 --> 00:22:54,919
approximation you know this it matches basically the the curvature

377
00:22:54,919 --> 00:22:58,099
it matches basically the the curvature of the exact distribution okay and this

378
00:22:58,099 --> 00:23:01,729
of the exact distribution okay and this is this is the variation all I'm

379
00:23:01,729 --> 00:23:03,949
is this is the variation all I'm somebody with color skin oh sorry need

380
00:23:03,949 --> 00:23:06,019
somebody with color skin oh sorry need to okay so this is the polyester

381
00:23:06,019 --> 00:23:11,479
to okay so this is the polyester approximation again this is a bigger

382
00:23:11,479 --> 00:23:13,129
approximation again this is a bigger problem right it gives you a sense of

383
00:23:13,129 --> 00:23:14,690
problem right it gives you a sense of what the variational metals will do it

384
00:23:14,690 --> 00:23:18,079
what the variational metals will do it in this case there's no point of forming

385
00:23:18,079 --> 00:23:22,609
in this case there's no point of forming the you know lower bound because we only

386
00:23:22,609 --> 00:23:25,279
the you know lower bound because we only have one variable basically and there's

387
00:23:25,279 --> 00:23:30,289
have one variable basically and there's no need to go to join the minimization

388
00:23:30,289 --> 00:23:33,079
no need to go to join the minimization of the curve of the distance between two

389
00:23:33,079 --> 00:23:38,560
of the curve of the distance between two and three

390
00:23:38,560 --> 00:23:38,570


391
00:23:38,570 --> 00:23:44,490
so I stood there saying this without realizing that it was a few slides down

392
00:23:44,490 --> 00:23:47,470
realizing that it was a few slides down let's take the real problem now and say

393
00:23:47,470 --> 00:23:52,360
let's take the real problem now and say that G is high dimensional so we need to

394
00:23:52,360 --> 00:23:54,040
that G is high dimensional so we need to approximate this so the first thing we

395
00:23:54,040 --> 00:23:56,710
approximate this so the first thing we do in these variational methods we

396
00:23:56,710 --> 00:24:00,850
do in these variational methods we factorize the distribution Q of Z as a

397
00:24:00,850 --> 00:24:05,260
factorize the distribution Q of Z as a product of the distributions of Qi so

398
00:24:05,260 --> 00:24:09,910
product of the distributions of Qi so each of the states so so if you think of

399
00:24:09,910 --> 00:24:11,740
each of the states so so if you think of this you know if you have two variables

400
00:24:11,740 --> 00:24:16,270
this you know if you have two variables Q 1 of Z 1 P 2 of Z 2 okay so if somehow

401
00:24:16,270 --> 00:24:18,460
Q 1 of Z 1 P 2 of Z 2 okay so if somehow your punch made it to them to be imagine

402
00:24:18,460 --> 00:24:21,730
your punch made it to them to be imagine you even have a product of gaussians in

403
00:24:21,730 --> 00:24:25,180
you even have a product of gaussians in each variable right but it's a factorize

404
00:24:25,180 --> 00:24:31,240
each variable right but it's a factorize product okay so this is the essential

405
00:24:31,240 --> 00:24:35,080
product okay so this is the essential part to start with alright so we're

406
00:24:35,080 --> 00:24:37,480
part to start with alright so we're going to use this limitation this

407
00:24:37,480 --> 00:24:40,210
going to use this limitation this approximation on define Q of Z and then

408
00:24:40,210 --> 00:24:43,110
approximation on define Q of Z and then somehow we're going to use the

409
00:24:43,110 --> 00:24:46,240
somehow we're going to use the maximization of the lower bound to try

410
00:24:46,240 --> 00:24:50,080
maximization of the lower bound to try to compute what is the form of this two

411
00:24:50,080 --> 00:24:54,490
to compute what is the form of this two eyes and we're going to assume that

412
00:24:54,490 --> 00:24:56,320
eyes and we're going to assume that there are generational about to start

413
00:24:56,320 --> 00:24:58,720
there are generational about to start with so we will do this as general as

414
00:24:58,720 --> 00:25:03,010
with so we will do this as general as possible and and after that we which

415
00:25:03,010 --> 00:25:06,580
possible and and after that we which exactly with applications on if we need

416
00:25:06,580 --> 00:25:10,180
exactly with applications on if we need any additional approximations so I can

417
00:25:10,180 --> 00:25:10,930
any additional approximations so I can tell you again

418
00:25:10,930 --> 00:25:13,230
tell you again this is nothing new in machine learning

419
00:25:13,230 --> 00:25:15,580
this is nothing new in machine learning the machine learning people basically

420
00:25:15,580 --> 00:25:17,230
the machine learning people basically stole the idea from statistical

421
00:25:17,230 --> 00:25:19,930
stole the idea from statistical mechanics and the derivation we will do

422
00:25:19,930 --> 00:25:22,660
mechanics and the derivation we will do basically leads to the classical mean

423
00:25:22,660 --> 00:25:25,300
basically leads to the classical mean field theory from quantum mechanics okay

424
00:25:25,300 --> 00:25:27,490
field theory from quantum mechanics okay so there's nothing you and actually

425
00:25:27,490 --> 00:25:30,280
so there's nothing you and actually that's why a number of very smart

426
00:25:30,280 --> 00:25:32,980
that's why a number of very smart machine learning people especially in

427
00:25:32,980 --> 00:25:35,740
machine learning people especially in Europe actually they used to be forward

428
00:25:35,740 --> 00:25:37,420
Europe actually they used to be forward physicists who switch to machine

429
00:25:37,420 --> 00:25:39,450
physicists who switch to machine learning so they can make some money

430
00:25:39,450 --> 00:25:42,520
learning so they can make some money okay and they are very successful

431
00:25:42,520 --> 00:25:45,820
okay and they are very successful because their intuition pays off

432
00:25:45,820 --> 00:25:47,769
because their intuition pays off alright if you work just with the

433
00:25:47,769 --> 00:25:50,529
alright if you work just with the Christiane's you're missing reality and

434
00:25:50,529 --> 00:25:52,749
Christiane's you're missing reality and these people know what reality is and

435
00:25:52,749 --> 00:25:56,409
these people know what reality is and they have done some significant advances

436
00:25:56,409 --> 00:26:01,119
they have done some significant advances in this area okay all right so this is

437
00:26:01,119 --> 00:26:04,779
in this area okay all right so this is our ization all right so we plug in this

438
00:26:04,779 --> 00:26:09,399
our ization all right so we plug in this to learn about and remember the lower

439
00:26:09,399 --> 00:26:12,070
to learn about and remember the lower bound it was an expectation with respect

440
00:26:12,070 --> 00:26:14,979
bound it was an expectation with respect with you there is Q and inside here we

441
00:26:14,979 --> 00:26:16,919
with you there is Q and inside here we had the longer the joint likelihood

442
00:26:16,919 --> 00:26:21,789
had the longer the joint likelihood divided by Q which Q was this product of

443
00:26:21,789 --> 00:26:24,249
divided by Q which Q was this product of all of this and because I have a long

444
00:26:24,249 --> 00:26:29,320
all of this and because I have a long the long the product the this product

445
00:26:29,320 --> 00:26:32,529
the long the product the this product here becomes summations of the logs okay

446
00:26:32,529 --> 00:26:38,299
here becomes summations of the logs okay you see

447
00:26:38,299 --> 00:26:38,309


448
00:26:38,309 --> 00:26:47,450
again the actual Miliband had few longer fear affects command C divided by Q of Z

449
00:26:47,450 --> 00:26:50,239
fear affects command C divided by Q of Z cubes is that so you have longer bad so

450
00:26:50,239 --> 00:26:58,969
cubes is that so you have longer bad so you get summation of the logs okay we're

451
00:26:58,969 --> 00:27:01,249
you get summation of the logs okay we're going to try to do is we're going to try

452
00:27:01,249 --> 00:27:03,859
going to try to do is we're going to try to approximate its time where

453
00:27:03,859 --> 00:27:07,549
to approximate its time where fertilization one at a time right so to

454
00:27:07,549 --> 00:27:14,049
fertilization one at a time right so to see if you know if that is possible and

455
00:27:14,049 --> 00:27:16,669
see if you know if that is possible and foremost we have a physical intuition I

456
00:27:16,669 --> 00:27:20,389
foremost we have a physical intuition I have taken a course in statistical

457
00:27:20,389 --> 00:27:22,669
have taken a course in statistical mechanics or in quantum mechanics this

458
00:27:22,669 --> 00:27:26,419
mechanics or in quantum mechanics this problem is going to be sort of solved to

459
00:27:26,419 --> 00:27:29,180
problem is going to be sort of solved to such consistency now what that means is

460
00:27:29,180 --> 00:27:31,159
such consistency now what that means is we're going to compute each of the

461
00:27:31,159 --> 00:27:33,369
we're going to compute each of the factors that say the factor to j then

462
00:27:33,369 --> 00:27:36,769
factors that say the factor to j then keeping the rest of the factors to some

463
00:27:36,769 --> 00:27:39,560
keeping the rest of the factors to some values and then we will then update the

464
00:27:39,560 --> 00:27:41,570
values and then we will then update the next factor keeping the rest constant

465
00:27:41,570 --> 00:27:45,320
next factor keeping the rest constant until effectively as we play we iterate

466
00:27:45,320 --> 00:27:47,060
until effectively as we play we iterate through the factors no further updates

467
00:27:47,060 --> 00:27:49,219
through the factors no further updates are needed this is what's called

468
00:27:49,219 --> 00:27:50,269
are needed this is what's called self-consistency

469
00:27:50,269 --> 00:27:52,729
self-consistency the way we saw basically the Threadgill

470
00:27:52,729 --> 00:27:57,469
the way we saw basically the Threadgill equations okay so i'm going to insulate

471
00:27:57,469 --> 00:28:00,619
equations okay so i'm going to insulate the tactical jet so we're gonna compute

472
00:28:00,619 --> 00:28:03,680
the tactical jet so we're gonna compute we're going to try to maximize this

473
00:28:03,680 --> 00:28:07,129
we're going to try to maximize this lower bound with respect to QJ okay

474
00:28:07,129 --> 00:28:14,839
lower bound with respect to QJ okay that's our objective because you know

475
00:28:14,839 --> 00:28:18,739
that's our objective because you know it's from my direction here so we want

476
00:28:18,739 --> 00:28:21,950
it's from my direction here so we want on this thing to have oniy QJ so let's

477
00:28:21,950 --> 00:28:26,180
on this thing to have oniy QJ so let's look at this term when I have done

478
00:28:26,180 --> 00:28:28,459
look at this term when I have done cherries there is a product of for the

479
00:28:28,459 --> 00:28:30,979
cherries there is a product of for the two eyes so that I did is I put this

480
00:28:30,979 --> 00:28:36,349
two eyes so that I did is I put this product inside the integral will include

481
00:28:36,349 --> 00:28:38,749
product inside the integral will include all the terms modes a and then I do the

482
00:28:38,749 --> 00:28:40,549
all the terms modes a and then I do the integration of juice apple tree

483
00:28:40,549 --> 00:28:45,139
integration of juice apple tree nothing first you see that right I have

484
00:28:45,139 --> 00:28:46,090
nothing first you see that right I have an integration

485
00:28:46,090 --> 00:28:49,270
an integration disease but now I integrate in overseas

486
00:28:49,270 --> 00:28:51,940
disease but now I integrate in overseas but today and then the integrations a

487
00:28:51,940 --> 00:28:56,050
but today and then the integrations a separately so this then you get it like

488
00:28:56,050 --> 00:28:59,170
separately so this then you get it like that right is basically the average of

489
00:28:59,170 --> 00:29:01,960
that right is basically the average of the Journal of likelihood with respect

490
00:29:01,960 --> 00:29:03,760
the Journal of likelihood with respect to the distribution of home of the other

491
00:29:03,760 --> 00:29:08,830
to the distribution of home of the other variables but J okay all right

492
00:29:08,830 --> 00:29:14,530
variables but J okay all right place this thing coming so I have some

493
00:29:14,530 --> 00:29:16,510
place this thing coming so I have some looking on this there on everything

494
00:29:16,510 --> 00:29:18,550
looking on this there on everything except J alright

495
00:29:18,550 --> 00:29:21,850
except J alright everything except J and will you agree

496
00:29:21,850 --> 00:29:26,050
everything except J and will you agree with me but here I'm only gonna have the

497
00:29:26,050 --> 00:29:29,110
with me but here I'm only gonna have the product of the Q eyes at the Thames a

498
00:29:29,110 --> 00:29:31,210
product of the Q eyes at the Thames a because the funds are you would be

499
00:29:31,210 --> 00:29:34,290
because the funds are you would be integrated with 0 will give me loud

500
00:29:34,290 --> 00:29:40,930
integrated with 0 will give me loud because there's no J there ZJ okay and

501
00:29:40,930 --> 00:29:42,970
because there's no J there ZJ okay and then I'm gonna have an expert then when

502
00:29:42,970 --> 00:29:46,510
then I'm gonna have an expert then when I pick up here the log of Q J and I'm

503
00:29:46,510 --> 00:29:50,080
I pick up here the log of Q J and I'm going to keep here the longer the QJ

504
00:29:50,080 --> 00:29:54,250
going to keep here the longer the QJ curve as well all the other terms will

505
00:29:54,250 --> 00:29:57,930
curve as well all the other terms will be degraded to one well as a tory

506
00:29:57,930 --> 00:30:01,920
be degraded to one well as a tory because the church shall have normalized

507
00:30:01,920 --> 00:30:09,440
because the church shall have normalized okay

508
00:30:09,440 --> 00:30:09,450


509
00:30:09,450 --> 00:30:12,550
all right

510
00:30:12,550 --> 00:30:12,560


511
00:30:12,560 --> 00:30:20,500
so I am let me call this whole thing this whole parenthesis because is going

512
00:30:20,500 --> 00:30:23,620
this whole parenthesis because is going to be a function of Z J because all the

513
00:30:23,620 --> 00:30:25,720
to be a function of Z J because all the others this would be integrate now right

514
00:30:25,720 --> 00:30:29,170
others this would be integrate now right some what I call this peak with a care

515
00:30:29,170 --> 00:30:32,920
some what I call this peak with a care of X comma CJ okay and I'm claiming this

516
00:30:32,920 --> 00:30:37,360
of X comma CJ okay and I'm claiming this longer this the wholly integral okay we

517
00:30:37,360 --> 00:30:40,150
longer this the wholly integral okay we will see why it doesn't matter right now

518
00:30:40,150 --> 00:30:43,660
will see why it doesn't matter right now just the I define this to be log of P P

519
00:30:43,660 --> 00:30:47,940
just the I define this to be log of P P curve alright so this is Q J times that

520
00:30:47,940 --> 00:30:52,330
curve alright so this is Q J times that okay and remember I only want to keep

521
00:30:52,330 --> 00:30:54,670
okay and remember I only want to keep the times in Z J because I want to do a

522
00:30:54,670 --> 00:30:58,030
the times in Z J because I want to do a maximization with respect to Q J okay so

523
00:30:58,030 --> 00:30:59,980
maximization with respect to Q J okay so the only thing that I don't see any

524
00:30:59,980 --> 00:31:02,170
the only thing that I don't see any future here do you see linkage a of the

525
00:31:02,170 --> 00:31:04,600
future here do you see linkage a of the second time there is nothing there also

526
00:31:04,600 --> 00:31:05,830
second time there is nothing there also we don't really care

527
00:31:05,830 --> 00:31:09,880
we don't really care we call it constant and then we have the

528
00:31:09,880 --> 00:31:12,610
we call it constant and then we have the entropy basically of DJ which is this

529
00:31:12,610 --> 00:31:13,980
entropy basically of DJ which is this okay

530
00:31:13,980 --> 00:31:18,280
okay the rest is basically functional

531
00:31:18,280 --> 00:31:20,200
the rest is basically functional optimization and that's why these verbs

532
00:31:20,200 --> 00:31:22,300
optimization and that's why these verbs are called variational you're going to

533
00:31:22,300 --> 00:31:24,610
are called variational you're going to have to take derivatives of this with

534
00:31:24,610 --> 00:31:31,450
have to take derivatives of this with respect to Q J alright and set this

535
00:31:31,450 --> 00:31:36,340
respect to Q J alright and set this equal to zero but you know we can

536
00:31:36,340 --> 00:31:39,160
equal to zero but you know we can immediately see also from this is the

537
00:31:39,160 --> 00:31:42,660
immediately see also from this is the lower bound right we can see that

538
00:31:42,660 --> 00:31:45,040
lower bound right we can see that without any calculations actually

539
00:31:45,040 --> 00:31:47,530
without any calculations actually without any derivatives that this thing

540
00:31:47,530 --> 00:31:49,990
without any derivatives that this thing here is minus the cool but liberal

541
00:31:49,990 --> 00:31:54,400
here is minus the cool but liberal distance of Q J from this P curve we see

542
00:31:54,400 --> 00:32:01,030
distance of Q J from this P curve we see that

543
00:32:01,030 --> 00:32:01,040


544
00:32:01,040 --> 00:32:05,799
you remember the courtly distance without the minus signs here right it

545
00:32:05,799 --> 00:32:08,950
without the minus signs here right it would be the minus the integral of kids

546
00:32:08,950 --> 00:32:15,150
would be the minus the integral of kids a this / cute a so you're going to get

547
00:32:15,150 --> 00:32:18,520
a this / cute a so you're going to get there was a madness and this is in the

548
00:32:18,520 --> 00:32:20,890
there was a madness and this is in the denominator another - disappearance and

549
00:32:20,890 --> 00:32:23,140
denominator another - disappearance and with this thing we get here - which is

550
00:32:23,140 --> 00:32:30,640
with this thing we get here - which is this okay so you can see what's the

551
00:32:30,640 --> 00:32:33,490
this okay so you can see what's the optimal solution but maximize what is

552
00:32:33,490 --> 00:32:36,039
optimal solution but maximize what is the Q J that much lies the lower bound L

553
00:32:36,039 --> 00:32:39,490
the Q J that much lies the lower bound L of Q where we are scum again we do the

554
00:32:39,490 --> 00:32:41,950
of Q where we are scum again we do the optimization one time at the time that

555
00:32:41,950 --> 00:32:45,070
optimization one time at the time that the rest of the terms have to be left

556
00:32:45,070 --> 00:32:54,159
the rest of the terms have to be left alone okay so that is the optimal QJ we

557
00:32:54,159 --> 00:32:58,990
alone okay so that is the optimal QJ we want to maximize this with the - time we

558
00:32:58,990 --> 00:33:00,970
want to maximize this with the - time we want to minimize the KL distance what's

559
00:33:00,970 --> 00:33:08,290
want to minimize the KL distance what's the minimum Athenian distance

560
00:33:08,290 --> 00:33:08,300


561
00:33:08,300 --> 00:33:14,580
you have an evidence because I haven't

562
00:33:14,580 --> 00:33:14,590


563
00:33:14,590 --> 00:33:19,810
zero thank you

564
00:33:19,810 --> 00:33:19,820


565
00:33:19,820 --> 00:33:25,360
that's the minuend wins the minimum company for what values of QJ linkages

566
00:33:25,360 --> 00:33:27,120
company for what values of QJ linkages become equal to what

567
00:33:27,120 --> 00:33:32,860
become equal to what take care and so all right all right so

568
00:33:32,860 --> 00:33:39,480
take care and so all right all right so k QJ will become equal to this okay and

569
00:33:39,480 --> 00:33:48,100
k QJ will become equal to this okay and all right and and let me just write down

570
00:33:48,100 --> 00:33:52,000
all right and and let me just write down so we defined us to be this picture to

571
00:33:52,000 --> 00:33:54,100
so we defined us to be this picture to be I'm sorry little picture right so

572
00:33:54,100 --> 00:33:56,830
be I'm sorry little picture right so what I'm writing here the longer picture

573
00:33:56,830 --> 00:33:59,470
what I'm writing here the longer picture is this is my definition right this is

574
00:33:59,470 --> 00:34:01,810
is this is my definition right this is what they have so when you agree with me

575
00:34:01,810 --> 00:34:04,590
what they have so when you agree with me that the longer picture is the

576
00:34:04,590 --> 00:34:09,940
that the longer picture is the expectation with respect to other

577
00:34:09,940 --> 00:34:13,630
expectation with respect to other distributions except say of the joint

578
00:34:13,630 --> 00:34:18,130
distributions except say of the joint ability could you say this right it's an

579
00:34:18,130 --> 00:34:21,190
ability could you say this right it's an expectation with respect to all the kids

580
00:34:21,190 --> 00:34:24,250
expectation with respect to all the kids but J all right this is the expectation

581
00:34:24,250 --> 00:34:28,510
but J all right this is the expectation of you know with respect to all the Q

582
00:34:28,510 --> 00:34:33,220
of you know with respect to all the Q eyes but J okay and you told me that QJ

583
00:34:33,220 --> 00:34:35,530
eyes but J okay and you told me that QJ this optimal going to becomes equal to

584
00:34:35,530 --> 00:34:38,830
this optimal going to becomes equal to PQ all right come on I'm looking for

585
00:34:38,830 --> 00:34:41,710
PQ all right come on I'm looking for that equation okay so what did you tell

586
00:34:41,710 --> 00:34:43,810
that equation okay so what did you tell me that this is optimal when this

587
00:34:43,810 --> 00:34:48,010
me that this is optimal when this becomes equal to that so which means my

588
00:34:48,010 --> 00:34:51,250
becomes equal to that so which means my take the logs the run up to j is equal

589
00:34:51,250 --> 00:34:53,560
take the logs the run up to j is equal to log of p KL and lower peak oil was

590
00:34:53,560 --> 00:34:57,250
to log of p KL and lower peak oil was this alright and if I write this if I

591
00:34:57,250 --> 00:34:59,440
this alright and if I write this if I normalize it this is the optimal

592
00:34:59,440 --> 00:35:03,940
normalize it this is the optimal distribution project project

593
00:35:03,940 --> 00:35:08,760
distribution project project look at it very carefully okay all right

594
00:35:08,760 --> 00:35:11,250
look at it very carefully okay all right usually an easier way to remember is

595
00:35:11,250 --> 00:35:14,740
usually an easier way to remember is this equation here okay

596
00:35:14,740 --> 00:35:17,320
this equation here okay rather than this so the reason I should

597
00:35:17,320 --> 00:35:22,420
rather than this so the reason I should say is these words with respect to the

598
00:35:22,420 --> 00:35:25,510
say is these words with respect to the other this that the longer joint of

599
00:35:25,510 --> 00:35:29,170
other this that the longer joint of likelihood okay

600
00:35:29,170 --> 00:35:31,210
likelihood okay now obviously if you take an average

601
00:35:31,210 --> 00:35:34,030
now obviously if you take an average with respect to all the other cues right

602
00:35:34,030 --> 00:35:36,490
with respect to all the other cues right you have to iterate this that's why I

603
00:35:36,490 --> 00:35:37,690
you have to iterate this that's why I said you have to assume this to

604
00:35:37,690 --> 00:35:40,420
said you have to assume this to self-consistency so if you start with

605
00:35:40,420 --> 00:35:43,870
self-consistency so if you start with say and you approximate you define the

606
00:35:43,870 --> 00:35:46,720
say and you approximate you define the optimal for q1 keeping all the other

607
00:35:46,720 --> 00:35:49,240
optimal for q1 keeping all the other cues fixed then you go to optimal of

608
00:35:49,240 --> 00:35:51,190
cues fixed then you go to optimal of people to think you won and the rest are

609
00:35:51,190 --> 00:35:53,340
people to think you won and the rest are fixed and you keep iterating until

610
00:35:53,340 --> 00:35:55,720
fixed and you keep iterating until basically you're happy that they are not

611
00:35:55,720 --> 00:35:58,690
basically you're happy that they are not changes that's what's that consistency

612
00:35:58,690 --> 00:36:00,940
changes that's what's that consistency means and now why do they call this the

613
00:36:00,940 --> 00:36:04,780
means and now why do they call this the Menken approximation what is the method

614
00:36:04,780 --> 00:36:07,510
Menken approximation what is the method that makes this to sort of be a mean

615
00:36:07,510 --> 00:36:11,620
that makes this to sort of be a mean field of main field I mean what you know

616
00:36:11,620 --> 00:36:14,710
field of main field I mean what you know you can extrapolate from the research

617
00:36:14,710 --> 00:36:22,780
you can extrapolate from the research what is main main field

618
00:36:22,780 --> 00:36:22,790


619
00:36:22,790 --> 00:36:29,660
expectation right so to come you know to calculate this right you are using the

620
00:36:29,660 --> 00:36:33,260
calculate this right you are using the expectation of the log likelihood of the

621
00:36:33,260 --> 00:36:39,470
expectation of the log likelihood of the join of likelihood where you are with

622
00:36:39,470 --> 00:36:44,210
join of likelihood where you are with respect to the distributions so this is

623
00:36:44,210 --> 00:36:46,490
respect to the distributions so this is sort of you know if you say have many

624
00:36:46,490 --> 00:36:48,230
sort of you know if you say have many electrons moving around and you want

625
00:36:48,230 --> 00:36:50,900
electrons moving around and you want light in a question for a one-electron

626
00:36:50,900 --> 00:36:54,770
light in a question for a one-electron you know we are doing the computation in

627
00:36:54,770 --> 00:36:57,980
you know we are doing the computation in the average speed of electrons and then

628
00:36:57,980 --> 00:36:59,600
the average speed of electrons and then we get there was a political sort of the

629
00:36:59,600 --> 00:37:01,760
we get there was a political sort of the question in this case we get an equation

630
00:37:01,760 --> 00:37:04,700
question in this case we get an equation for the distribution through J in the

631
00:37:04,700 --> 00:37:10,640
for the distribution through J in the average distribution of the other of all

632
00:37:10,640 --> 00:37:19,220
average distribution of the other of all the other guys all right so that's where

633
00:37:19,220 --> 00:37:21,800
the other guys all right so that's where the mean field approximation yeah so you

634
00:37:21,800 --> 00:37:26,450
the mean field approximation yeah so you can feel for example I mean and we may

635
00:37:26,450 --> 00:37:30,200
can feel for example I mean and we may see an example later on let's say this

636
00:37:30,200 --> 00:37:31,910
see an example later on let's say this factorized distributions come to be a

637
00:37:31,910 --> 00:37:36,140
factorized distributions come to be a Gaussian in essence in this expectation

638
00:37:36,140 --> 00:37:39,590
Gaussian in essence in this expectation here you can think possibly that they

639
00:37:39,590 --> 00:37:42,680
here you can think possibly that they may come up that in certain cases what

640
00:37:42,680 --> 00:37:44,450
may come up that in certain cases what you do is you may have to fix the

641
00:37:44,450 --> 00:37:48,770
you do is you may have to fix the results to their mean values right to

642
00:37:48,770 --> 00:38:02,260
results to their mean values right to their mean values

643
00:38:02,260 --> 00:38:02,270


644
00:38:02,270 --> 00:38:08,200
I don't think so I don't think so

645
00:38:08,200 --> 00:38:08,210


646
00:38:08,210 --> 00:38:15,040
but I can I know one case that there is a relation between what you see here

647
00:38:15,040 --> 00:38:17,580
a relation between what you see here when you use particle approximations but

648
00:38:17,580 --> 00:38:19,630
when you use particle approximations but installation of the car love this you

649
00:38:19,630 --> 00:38:20,740
installation of the car love this you know it's very different

650
00:38:20,740 --> 00:38:30,500
know it's very different okay it's very different okay

651
00:38:30,500 --> 00:38:30,510


652
00:38:30,510 --> 00:38:35,750
now remember so we went we did this right with tick all the late and by the

653
00:38:35,750 --> 00:38:39,680
right with tick all the late and by the most pure of Z and we wrote them as a

654
00:38:39,680 --> 00:38:42,950
most pure of Z and we wrote them as a product of tax rise you know we

655
00:38:42,950 --> 00:38:44,450
product of tax rise you know we factorize the Montero at the time

656
00:38:44,450 --> 00:38:48,020
factorize the Montero at the time Kinnock you one of Z 1 Z 2 Z 2 etc so

657
00:38:48,020 --> 00:38:53,240
Kinnock you one of Z 1 Z 2 Z 2 etc so it's now this mission cubes this serves

658
00:38:53,240 --> 00:39:35,660
it's now this mission cubes this serves again you know the distribution tables

659
00:39:35,660 --> 00:39:39,470
again you know the distribution tables on the market blanket so in this case

660
00:39:39,470 --> 00:39:40,730
on the market blanket so in this case for example when you apply these

661
00:39:40,730 --> 00:39:43,400
for example when you apply these programs this is the summation Bishop

662
00:39:43,400 --> 00:39:45,620
programs this is the summation Bishop with the market blanket and updates one

663
00:39:45,620 --> 00:39:47,540
with the market blanket and updates one variable at the time one distribution at

664
00:39:47,540 --> 00:39:51,770
variable at the time one distribution at a time keeping the rest basically to

665
00:39:51,770 --> 00:39:54,980
a time keeping the rest basically to convey our okay so this is sort of our

666
00:39:54,980 --> 00:39:57,290
convey our okay so this is sort of our you can see how this may be applied to

667
00:39:57,290 --> 00:40:05,300
you can see how this may be applied to you in the context of classical modules

668
00:40:05,300 --> 00:40:05,310


669
00:40:05,310 --> 00:40:13,900
right

670
00:40:13,900 --> 00:40:13,910


671
00:40:13,910 --> 00:40:21,880
so let's do an approximation of this okay a very easy example then selects

672
00:40:21,880 --> 00:40:24,520
okay a very easy example then selects the slides contain a lot of details so

673
00:40:24,520 --> 00:40:27,520
the slides contain a lot of details so they were written by one of my master's

674
00:40:27,520 --> 00:40:30,190
they were written by one of my master's students okay who was willing and

675
00:40:30,190 --> 00:40:32,680
students okay who was willing and capable to do all these deviations and

676
00:40:32,680 --> 00:40:34,720
capable to do all these deviations and provide all the computer programs I'm

677
00:40:34,720 --> 00:40:38,080
provide all the computer programs I'm going with this lecture okay so what she

678
00:40:38,080 --> 00:40:42,700
going with this lecture okay so what she did is she yeah she started with a

679
00:40:42,700 --> 00:40:44,680
did is she yeah she started with a multivariate Gaussian in this case we

680
00:40:44,680 --> 00:40:47,500
multivariate Gaussian in this case we have two variables only with this mean

681
00:40:47,500 --> 00:40:50,740
have two variables only with this mean and quiet looking like that and he says

682
00:40:50,740 --> 00:40:52,510
and quiet looking like that and he says you know what I am going to use this

683
00:40:52,510 --> 00:40:55,150
you know what I am going to use this mini field approximation to approximate

684
00:40:55,150 --> 00:40:58,210
mini field approximation to approximate this Gaussian over two variables as a

685
00:40:58,210 --> 00:41:01,510
this Gaussian over two variables as a product of gaussians over its body more

686
00:41:01,510 --> 00:41:03,600
product of gaussians over its body more separate so it's a vectorized proper

687
00:41:03,600 --> 00:41:08,590
separate so it's a vectorized proper size of two univariate gaussians okay

688
00:41:08,590 --> 00:41:11,650
size of two univariate gaussians okay you may say let me destroy a bubble to

689
00:41:11,650 --> 00:41:12,850
you may say let me destroy a bubble to do this well you know it's interesting

690
00:41:12,850 --> 00:41:16,060
do this well you know it's interesting right I mean can you actually take you

691
00:41:16,060 --> 00:41:18,220
right I mean can you actually take you know this guy's on two variables and

692
00:41:18,220 --> 00:41:21,340
know this guy's on two variables and write is the product of thousands in in

693
00:41:21,340 --> 00:41:23,050
write is the product of thousands in in one dimension for each of the variables

694
00:41:23,050 --> 00:41:24,400
one dimension for each of the variables and it'll be interesting

695
00:41:24,400 --> 00:41:30,670
and it'll be interesting okay so no donation but actually it's a

696
00:41:30,670 --> 00:41:33,550
okay so no donation but actually it's a very good way for you to learn exactly

697
00:41:33,550 --> 00:41:39,460
very good way for you to learn exactly how this variation all right so remember

698
00:41:39,460 --> 00:41:42,280
how this variation all right so remember in space the optimal Q line the normal

699
00:41:42,280 --> 00:41:46,030
in space the optimal Q line the normal qrz one so this now means optimal it is

700
00:41:46,030 --> 00:41:50,830
qrz one so this now means optimal it is the expectation whatever value was

701
00:41:50,830 --> 00:41:57,100
the expectation whatever value was higher here Z 2 there is no hidden body

702
00:41:57,100 --> 00:41:59,410
higher here Z 2 there is no hidden body or anything that zone is zip here so

703
00:41:59,410 --> 00:42:02,020
or anything that zone is zip here so number P of Z so what I'm doing is I'm

704
00:42:02,020 --> 00:42:04,780
number P of Z so what I'm doing is I'm expanding the love of pure silver by

705
00:42:04,780 --> 00:42:07,180
expanding the love of pure silver by body of Gaussian all right clipping

706
00:42:07,180 --> 00:42:10,380
body of Gaussian all right clipping anything that doesn't depend on

707
00:42:10,380 --> 00:42:13,810
anything that doesn't depend on basically on on the Z one out of the

708
00:42:13,810 --> 00:42:16,510
basically on on the Z one out of the picture okay because we're in interest

709
00:42:16,510 --> 00:42:20,690
picture okay because we're in interest on the distribution of Z 1

710
00:42:20,690 --> 00:42:20,700


711
00:42:20,700 --> 00:42:24,460
whatever result we did we're gonna have to normalize it to be sure that this is

712
00:42:24,460 --> 00:42:28,250
to normalize it to be sure that this is integrates to on so keep that in mind so

713
00:42:28,250 --> 00:42:30,620
integrates to on so keep that in mind so you expand this write this term

714
00:42:30,620 --> 00:42:34,550
you expand this write this term ultraviolet Gaussian you expand and and

715
00:42:34,550 --> 00:42:36,079
ultraviolet Gaussian you expand and and then you have to take its protections

716
00:42:36,079 --> 00:42:38,390
then you have to take its protections with respect to Z 2 so every time that

717
00:42:38,390 --> 00:42:40,849
with respect to Z 2 so every time that has C - where is it - you can have the

718
00:42:40,849 --> 00:42:46,480
has C - where is it - you can have the expectation of Z 2 respectively - okay

719
00:42:46,480 --> 00:42:49,760
expectation of Z 2 respectively - okay there is nothing else and how do we

720
00:42:49,760 --> 00:42:51,380
there is nothing else and how do we extract this distribution from this

721
00:42:51,380 --> 00:42:54,230
extract this distribution from this expression what is the most rules that

722
00:42:54,230 --> 00:42:56,540
expression what is the most rules that we have to do this is distribution in g1

723
00:42:56,540 --> 00:43:00,650
we have to do this is distribution in g1 I see g1 here I see C 1 versus u 1

724
00:43:00,650 --> 00:43:03,260
I see g1 here I see C 1 versus u 1 square so what do we need to do to come

725
00:43:03,260 --> 00:43:05,839
square so what do we need to do to come up with the a nice expression what's the

726
00:43:05,839 --> 00:43:14,060
up with the a nice expression what's the password

727
00:43:14,060 --> 00:43:14,070


728
00:43:14,070 --> 00:43:19,030
no I'm like I said Foursquare and somebody sent to complete the square I

729
00:43:19,030 --> 00:43:26,660
somebody sent to complete the square I and I you know square square I left the

730
00:43:26,660 --> 00:43:28,640
and I you know square square I left the square I was going to say closing the

731
00:43:28,640 --> 00:43:31,280
square I was going to say closing the square or completing the square or you

732
00:43:31,280 --> 00:43:33,410
square or completing the square or you know whatever so you basically have to

733
00:43:33,410 --> 00:43:39,470
know whatever so you basically have to write this you know the three - some

734
00:43:39,470 --> 00:43:45,260
write this you know the three - some mean right mr. matrix times D - Ameen

735
00:43:45,260 --> 00:43:47,210
mean right mr. matrix times D - Ameen all right so you can get the form of a

736
00:43:47,210 --> 00:43:48,890
all right so you can get the form of a Gaussian distribution you have to close

737
00:43:48,890 --> 00:43:52,400
Gaussian distribution you have to close the square on c1 okay so you can be

738
00:43:52,400 --> 00:43:54,530
the square on c1 okay so you can be missed in a Trinity of reading and if

739
00:43:54,530 --> 00:43:56,960
missed in a Trinity of reading and if you do this in a trivial way it comes

740
00:43:56,960 --> 00:43:59,480
you do this in a trivial way it comes that the optimal distribution for zero

741
00:43:59,480 --> 00:44:04,100
that the optimal distribution for zero is and again the Vice here so you don't

742
00:44:04,100 --> 00:44:06,830
is and again the Vice here so you don't have to move a finger the moon comes to

743
00:44:06,830 --> 00:44:14,270
have to move a finger the moon comes to be what you see here okay and the

744
00:44:14,270 --> 00:44:16,130
be what you see here okay and the optimal t2 by symmetry comes to look

745
00:44:16,130 --> 00:44:18,680
optimal t2 by symmetry comes to look like that so I remind you me I am you

746
00:44:18,680 --> 00:44:20,930
like that so I remind you me I am you two are the means but they're coming

747
00:44:20,930 --> 00:44:26,810
two are the means but they're coming from the bivariate Gaussian and so if

748
00:44:26,810 --> 00:44:33,050
from the bivariate Gaussian and so if security of z1 the means of the

749
00:44:33,050 --> 00:44:34,700
security of z1 the means of the univariate gaussians in the

750
00:44:34,700 --> 00:44:37,460
univariate gaussians in the factorization alright so there is an

751
00:44:37,460 --> 00:44:39,440
factorization alright so there is an issue here you can nothing issue it this

752
00:44:39,440 --> 00:44:41,770
issue here you can nothing issue it this is exactly what we discussed about

753
00:44:41,770 --> 00:44:45,890
is exactly what we discussed about self-consistency to find kill one of z1

754
00:44:45,890 --> 00:44:49,760
self-consistency to find kill one of z1 you need to know lipids to remain as

755
00:44:49,760 --> 00:44:52,730
you need to know lipids to remain as they keep we have z2 and to find you 12

756
00:44:52,730 --> 00:44:54,800
they keep we have z2 and to find you 12 G - you need to know what is the mean of

757
00:44:54,800 --> 00:45:02,500
G - you need to know what is the mean of q1 of z1 thanks mate let's say this main

758
00:45:02,500 --> 00:45:05,990
q1 of z1 thanks mate let's say this main and you complete this and then you

759
00:45:05,990 --> 00:45:08,060
and you complete this and then you complete its meaning you are you planet

760
00:45:08,060 --> 00:45:10,310
complete its meaning you are you planet in here calculate this you can't make a

761
00:45:10,310 --> 00:45:12,530
in here calculate this you can't make a new mean of this you plug it in and get

762
00:45:12,530 --> 00:45:17,000
new mean of this you plug it in and get irate and feel convergence okay it's

763
00:45:17,000 --> 00:45:19,930
irate and feel convergence okay it's actually contact but this problem has

764
00:45:19,930 --> 00:45:26,729
actually contact but this problem has I really like it is the fix point

765
00:45:26,729 --> 00:45:31,900
I really like it is the fix point solution okay and so the fix point

766
00:45:31,900 --> 00:45:34,749
solution okay and so the fix point solution basically at the end of the day

767
00:45:34,749 --> 00:45:37,499
solution basically at the end of the day gives you the chances of you secure all

768
00:45:37,499 --> 00:45:40,870
gives you the chances of you secure all right so when you iterate this you can

769
00:45:40,870 --> 00:45:43,569
right so when you iterate this you can do it but you know gives the answer okay

770
00:45:43,569 --> 00:45:47,680
do it but you know gives the answer okay so the mean of q1 z1 is actually new one

771
00:45:47,680 --> 00:45:56,620
so the mean of q1 z1 is actually new one and the you know the precision is lambda

772
00:45:56,620 --> 00:45:58,809
and the you know the precision is lambda 1 1 and here the Precision's love with

773
00:45:58,809 --> 00:46:03,069
1 1 and here the Precision's love with you too and I mean me too alright so

774
00:46:03,069 --> 00:46:07,120
you too and I mean me too alright so let's I do not have time to check I'm

775
00:46:07,120 --> 00:46:09,209
let's I do not have time to check I'm gonna post this right but there is a

776
00:46:09,209 --> 00:46:12,599
gonna post this right but there is a product on problems actually the plan

777
00:46:12,599 --> 00:46:15,729
product on problems actually the plan and get you the pixels of your seat here

778
00:46:15,729 --> 00:46:19,569
and get you the pixels of your seat here so I'm gonna show you how the results

779
00:46:19,569 --> 00:46:24,519
so I'm gonna show you how the results look like so this is this is the

780
00:46:24,519 --> 00:46:26,319
look like so this is this is the multivariate Gaussian that to bivariate

781
00:46:26,319 --> 00:46:28,479
multivariate Gaussian that to bivariate Gaussian P of Z and you try to

782
00:46:28,479 --> 00:46:31,809
Gaussian P of Z and you try to approximate it with this factorized

783
00:46:31,809 --> 00:46:35,109
approximate it with this factorized variational distribution Q of Z alright

784
00:46:35,109 --> 00:46:41,289
variational distribution Q of Z alright so so notice okay so this is what you

785
00:46:41,289 --> 00:46:45,160
so so notice okay so this is what you get all right so this is the product of

786
00:46:45,160 --> 00:46:48,549
get all right so this is the product of two univariate gaussians and you notice

787
00:46:48,549 --> 00:46:52,390
two univariate gaussians and you notice that in in the direction of the maximal

788
00:46:52,390 --> 00:46:54,130
that in in the direction of the maximal variance of P of Z basically your

789
00:46:54,130 --> 00:47:01,150
variance of P of Z basically your underestimate X okay so basically here

790
00:47:01,150 --> 00:47:02,949
underestimate X okay so basically here you're doing good but you know you're

791
00:47:02,949 --> 00:47:06,280
you're doing good but you know you're missing what's going on in there

792
00:47:06,280 --> 00:47:08,800
missing what's going on in there so this is basically the support the

793
00:47:08,800 --> 00:47:10,450
so this is basically the support the solution that you get this multiple path

794
00:47:10,450 --> 00:47:12,910
solution that you get this multiple path than the actual P of Z so basically your

795
00:47:12,910 --> 00:47:15,760
than the actual P of Z so basically your cube Z is over confident approximation

796
00:47:15,760 --> 00:47:27,460
cube Z is over confident approximation of the procedure

797
00:47:27,460 --> 00:47:27,470


798
00:47:27,470 --> 00:47:32,790
okay alternative approximate interesting we don't need to know this right

799
00:47:32,790 --> 00:47:35,980
we don't need to know this right basically the same derivation I gave you

800
00:47:35,980 --> 00:47:39,220
basically the same derivation I gave you I see here when we did the optimization

801
00:47:39,220 --> 00:47:43,510
I see here when we did the optimization problem and crossing the constrain but

802
00:47:43,510 --> 00:47:45,910
problem and crossing the constrain but the distribution future is normalized so

803
00:47:45,910 --> 00:47:47,079
the distribution future is normalized so if you do absolutely this you get the

804
00:47:47,079 --> 00:47:49,420
if you do absolutely this you get the right solution right away so there's

805
00:47:49,420 --> 00:47:51,250
right solution right away so there's nothing you really on this think I know

806
00:47:51,250 --> 00:47:56,550
nothing you really on this think I know actually Wireless came out of sequence

807
00:47:56,550 --> 00:47:56,560


808
00:47:56,560 --> 00:48:01,390
okay it doesn't matter so let me continue this approximation of the

809
00:48:01,390 --> 00:48:04,030
continue this approximation of the Gaussian think this is what Winston's

810
00:48:04,030 --> 00:48:09,870
Gaussian think this is what Winston's we're getting okay so we mentioned that

811
00:48:09,870 --> 00:48:12,370
we're getting okay so we mentioned that maximizing the lower bound right is the

812
00:48:12,370 --> 00:48:14,260
maximizing the lower bound right is the same as minimizing the Cooper clip or

813
00:48:14,260 --> 00:48:17,890
same as minimizing the Cooper clip or distance between Q and P so it just to

814
00:48:17,890 --> 00:48:21,490
distance between Q and P so it just to make things interesting you know what we

815
00:48:21,490 --> 00:48:23,530
make things interesting you know what we do here is promote instead of minimizing

816
00:48:23,530 --> 00:48:26,470
do here is promote instead of minimizing risk QM between Q and P that comes

817
00:48:26,470 --> 00:48:28,270
risk QM between Q and P that comes directly from the is a variation of

818
00:48:28,270 --> 00:48:31,660
directly from the is a variation of Principles if we try to minimize instead

819
00:48:31,660 --> 00:48:35,800
Principles if we try to minimize instead the distance between P and Q for I can

820
00:48:35,800 --> 00:48:37,359
the distance between P and Q for I can remember the cubot liberal distance is

821
00:48:37,359 --> 00:48:39,430
remember the cubot liberal distance is not symmetric so you know anticipate

822
00:48:39,430 --> 00:48:40,930
not symmetric so you know anticipate that this will give you the same results

823
00:48:40,930 --> 00:48:42,490
that this will give you the same results as this so this is the valuation

824
00:48:42,490 --> 00:48:44,380
as this so this is the valuation approximation this is what were

825
00:48:44,380 --> 00:48:47,410
approximation this is what were discussing in this Q lectures this is

826
00:48:47,410 --> 00:48:49,030
discussing in this Q lectures this is not a variation approximation we

827
00:48:49,030 --> 00:48:51,579
not a variation approximation we minimize the KL distance between P and Q

828
00:48:51,579 --> 00:48:54,099
minimize the KL distance between P and Q all right big difference so this is what

829
00:48:54,099 --> 00:48:55,410
all right big difference so this is what you get

830
00:48:55,410 --> 00:48:58,420
you get so K here look at the so the Q is all

831
00:48:58,420 --> 00:49:01,720
so K here look at the so the Q is all over the space this is P of Z so P of Z

832
00:49:01,720 --> 00:49:05,200
over the space this is P of Z so P of Z is 0 there right so in cubes if you can

833
00:49:05,200 --> 00:49:06,550
is 0 there right so in cubes if you can see when you minimize the distance

834
00:49:06,550 --> 00:49:09,059
see when you minimize the distance between P and Q it fits for community

835
00:49:09,059 --> 00:49:12,579
between P and Q it fits for community mass where the section values of P of Z

836
00:49:12,579 --> 00:49:16,440
mass where the section values of P of Z are very small okay so before we had

837
00:49:16,440 --> 00:49:19,359
are very small okay so before we had another confident solution that

838
00:49:19,359 --> 00:49:22,780
another confident solution that underestimates the variance in this

839
00:49:22,780 --> 00:49:25,240
underestimates the variance in this direction here we have a solution has

840
00:49:25,240 --> 00:49:28,150
direction here we have a solution has very high support and non zero mass in

841
00:49:28,150 --> 00:49:30,550
very high support and non zero mass in areas for P of Z is equal to zero

842
00:49:30,550 --> 00:49:34,570
areas for P of Z is equal to zero so again be sure to write analytical

843
00:49:34,570 --> 00:49:36,520
so again be sure to write analytical problems you minimizing Cal differences

844
00:49:36,520 --> 00:49:40,720
problems you minimizing Cal differences yes this to be conserve how big

845
00:49:40,720 --> 00:49:42,730
yes this to be conserve how big differences you get when you memorize Q

846
00:49:42,730 --> 00:49:46,090
differences you get when you memorize Q and Q the distance of U from P versus

847
00:49:46,090 --> 00:49:48,640
and Q the distance of U from P versus the distance of P from Q and you can

848
00:49:48,640 --> 00:49:56,350
the distance of P from Q and you can actually sort of you know you can expect

849
00:49:56,350 --> 00:49:58,750
actually sort of you know you can expect many of this results right so you know

850
00:49:58,750 --> 00:50:04,900
many of this results right so you know so let's do one of these cases just for

851
00:50:04,900 --> 00:50:07,120
so let's do one of these cases just for demonstration so in this case we

852
00:50:07,120 --> 00:50:08,770
demonstration so in this case we minimize the distance between Q and P

853
00:50:08,770 --> 00:50:12,210
minimize the distance between Q and P and this is what this distance is right

854
00:50:12,210 --> 00:50:16,870
and this is what this distance is right so let's consider regions of the actual

855
00:50:16,870 --> 00:50:19,420
so let's consider regions of the actual distribution that P of Z given X is zero

856
00:50:19,420 --> 00:50:25,210
distribution that P of Z given X is zero all right so it's observers cannot what

857
00:50:25,210 --> 00:50:30,490
all right so it's observers cannot what this is insane it's me

858
00:50:30,490 --> 00:50:34,840
this is insane it's me okay very big it's really crucial big we

859
00:50:34,840 --> 00:50:38,350
okay very big it's really crucial big we actually are going to member of 50s need

860
00:50:38,350 --> 00:50:39,340
actually are going to member of 50s need that most probably

861
00:50:39,340 --> 00:50:43,420
that most probably so this is big Lee okay TMZ given X is

862
00:50:43,420 --> 00:50:46,420
so this is big Lee okay TMZ given X is big okay

863
00:50:46,420 --> 00:50:49,900
big okay if you get basically you get an infinite

864
00:50:49,900 --> 00:50:54,910
if you get basically you get an infinite contribution assuming what that cube Z

865
00:50:54,910 --> 00:51:00,250
contribution assuming what that cube Z is not 0 all right assuming a huge list

866
00:51:00,250 --> 00:51:06,370
is not 0 all right assuming a huge list of 0 7 doesn't mean that in regions

867
00:51:06,370 --> 00:51:13,080
of 0 7 doesn't mean that in regions where remember to minimize this distance

868
00:51:13,080 --> 00:51:13,090


869
00:51:13,090 --> 00:51:19,600
dispersed awareness Israel you can confusion over here so in regions that P

870
00:51:19,600 --> 00:51:28,250
confusion over here so in regions that P of Z is 0 she seems to be what

871
00:51:28,250 --> 00:51:28,260


872
00:51:28,260 --> 00:51:33,089
look at this expression right and considering regions were fields is zero

873
00:51:33,089 --> 00:51:34,290
considering regions were fields is zero alright

874
00:51:34,290 --> 00:51:37,530
alright to avoid getting the above zero what is

875
00:51:37,530 --> 00:51:40,710
to avoid getting the above zero what is the only option that will lead to an

876
00:51:40,710 --> 00:51:52,920
the only option that will lead to an envelope for the distance of 20

877
00:51:52,920 --> 00:51:52,930


878
00:51:52,930 --> 00:52:00,540
the presses do have the only way that you get not infinite is when when cubes

879
00:52:00,540 --> 00:52:02,490
you get not infinite is when when cubes that have become zero and this is what's

880
00:52:02,490 --> 00:52:06,480
that have become zero and this is what's called zero forcing for Q of Z and you

881
00:52:06,480 --> 00:52:10,800
called zero forcing for Q of Z and you can take care P of Z is zero right and

882
00:52:10,800 --> 00:52:15,540
can take care P of Z is zero right and curves is supposed to go to zero okay in

883
00:52:15,540 --> 00:52:17,880
curves is supposed to go to zero okay in this case and the discussions of the

884
00:52:17,880 --> 00:52:19,980
this case and the discussions of the next slide that you can read you

885
00:52:19,980 --> 00:52:24,510
next slide that you can read you basically get the opposite effectively

886
00:52:24,510 --> 00:52:27,630
basically get the opposite effectively what you get in regions where P of Z is

887
00:52:27,630 --> 00:52:32,640
what you get in regions where P of Z is 0 Q has positive master Fisto okay right

888
00:52:32,640 --> 00:52:36,900
0 Q has positive master Fisto okay right the opposite effect you can see the

889
00:52:36,900 --> 00:52:38,940
the opposite effect you can see the actual distribution is a becauseum right

890
00:52:38,940 --> 00:52:40,950
actual distribution is a becauseum right and believe this means in the prop swish

891
00:52:40,950 --> 00:52:42,930
and believe this means in the prop swish right that's a kind of an interesting

892
00:52:42,930 --> 00:52:49,440
right that's a kind of an interesting problem so this is this let me see what

893
00:52:49,440 --> 00:52:52,350
problem so this is this let me see what is the actual distribution so the video

894
00:52:52,350 --> 00:52:55,080
is the actual distribution so the video I the motion of the you say is the

895
00:52:55,080 --> 00:52:57,300
I the motion of the you say is the mixture of gaussians and we try to

896
00:52:57,300 --> 00:53:01,430
mixture of gaussians and we try to compute factorize approximation of mm

897
00:53:01,430 --> 00:53:03,230
compute factorize approximation of mm factorized

898
00:53:03,230 --> 00:53:05,460
factorized product for basically of univariate

899
00:53:05,460 --> 00:53:08,400
product for basically of univariate gaussians okay so with your situation

900
00:53:08,400 --> 00:53:12,930
gaussians okay so with your situation starting from the minimization of the

901
00:53:12,930 --> 00:53:16,500
starting from the minimization of the total distance between P and Q again I'm

902
00:53:16,500 --> 00:53:19,620
total distance between P and Q again I'm going to this is nationals

903
00:53:19,620 --> 00:53:23,340
going to this is nationals okay again this okay this is not

904
00:53:23,340 --> 00:53:25,920
okay again this okay this is not variational methods is not maximizing

905
00:53:25,920 --> 00:53:29,100
variational methods is not maximizing the lower bound and but so this is how

906
00:53:29,100 --> 00:53:31,440
the lower bound and but so this is how the approximation of loops you get

907
00:53:31,440 --> 00:53:33,660
the approximation of loops you get basically the right approximation here

908
00:53:33,660 --> 00:53:38,270
basically the right approximation here is is univariate is the product of this

909
00:53:38,270 --> 00:53:41,010
is is univariate is the product of this you know you get basically this Gaussian

910
00:53:41,010 --> 00:53:47,610
you know you get basically this Gaussian what you see here okay and you know it

911
00:53:47,610 --> 00:53:50,670
what you see here okay and you know it basically contains the point of the two

912
00:53:50,670 --> 00:53:54,720
basically contains the point of the two modes in the Gaussian mixture if you go

913
00:53:54,720 --> 00:53:57,200
modes in the Gaussian mixture if you go however and you

914
00:53:57,200 --> 00:54:00,050
however and you made you do the very National methyl and

915
00:54:00,050 --> 00:54:02,810
made you do the very National methyl and you approximate the you minimize the

916
00:54:02,810 --> 00:54:04,610
you approximate the you minimize the care of distance between Q and P you

917
00:54:04,610 --> 00:54:09,050
care of distance between Q and P you will get either a list or either that so

918
00:54:09,050 --> 00:54:10,940
will get either a list or either that so basically the variational method web

919
00:54:10,940 --> 00:54:13,580
basically the variational method web browsers it collapses to all of the

920
00:54:13,580 --> 00:54:18,500
browsers it collapses to all of the modes of the Gaussian Mexico okay so

921
00:54:18,500 --> 00:54:20,930
modes of the Gaussian Mexico okay so this is your standard mode collapse

922
00:54:20,930 --> 00:54:24,350
this is your standard mode collapse problem and of course whether Sigma

923
00:54:24,350 --> 00:54:28,100
problem and of course whether Sigma necrosis I mean is this number it you

924
00:54:28,100 --> 00:54:29,630
necrosis I mean is this number it you know how is it possible to give you two

925
00:54:29,630 --> 00:54:31,690
know how is it possible to give you two solutions well you know you start with

926
00:54:31,690 --> 00:54:40,340
solutions well you know you start with something about your so for some initial

927
00:54:40,340 --> 00:54:42,050
something about your so for some initial guess here would converge there for

928
00:54:42,050 --> 00:54:44,540
guess here would converge there for something else will converge there now

929
00:54:44,540 --> 00:54:52,010
something else will converge there now if you know the initial basis you can

930
00:54:52,010 --> 00:54:54,800
if you know the initial basis you can immediately say fifty percent of the

931
00:54:54,800 --> 00:54:56,300
immediately say fifty percent of the time you may get this and fifty percent

932
00:54:56,300 --> 00:54:58,550
time you may get this and fifty percent of the time you may get done but to the

933
00:54:58,550 --> 00:55:05,240
of the time you may get done but to the right so this is the you know minimizing

934
00:55:05,240 --> 00:55:07,160
right so this is the you know minimizing the distance between P and Q this

935
00:55:07,160 --> 00:55:09,170
the distance between P and Q this corresponds the minimization of Q and P

936
00:55:09,170 --> 00:55:11,360
corresponds the minimization of Q and P which is our variational techniques

937
00:55:11,360 --> 00:55:15,230
which is our variational techniques discussed today okay if we have time

938
00:55:15,230 --> 00:55:19,700
discussed today okay if we have time maybe in a week or two we will discuss

939
00:55:19,700 --> 00:55:21,950
maybe in a week or two we will discuss basically the whole huge set of

940
00:55:21,950 --> 00:55:24,860
basically the whole huge set of methodologies include the expectation

941
00:55:24,860 --> 00:55:27,290
methodologies include the expectation propagation basically does lead to this

942
00:55:27,290 --> 00:55:29,620
propagation basically does lead to this type of approximations with which we

943
00:55:29,620 --> 00:55:33,680
type of approximations with which we obscure but the variational methods they

944
00:55:33,680 --> 00:55:38,010
obscure but the variational methods they have this problem with multiple lost

945
00:55:38,010 --> 00:55:38,020


946
00:55:38,020 --> 00:55:43,450
okay

947
00:55:43,450 --> 00:55:43,460


948
00:55:43,460 --> 00:55:53,170
I can utilize this actually this problem with you know well there's a lot of

949
00:55:53,170 --> 00:55:54,579
with you know well there's a lot of things that can be asking it here but

950
00:55:54,579 --> 00:55:57,280
things that can be asking it here but you know in this slide basically and I'm

951
00:55:57,280 --> 00:55:58,480
you know in this slide basically and I'm rubber and walk through it because it's

952
00:55:58,480 --> 00:56:02,319
rubber and walk through it because it's elementary if somebody gives you an

953
00:56:02,319 --> 00:56:05,559
elementary if somebody gives you an arbitrary distribution P of X in high

954
00:56:05,559 --> 00:56:08,440
arbitrary distribution P of X in high dimensions and says find using

955
00:56:08,440 --> 00:56:10,150
dimensions and says find using variational methods a Gaussian

956
00:56:10,150 --> 00:56:12,700
variational methods a Gaussian approximation you know the simple

957
00:56:12,700 --> 00:56:15,430
approximation you know the simple example to demonstrate the difference

958
00:56:15,430 --> 00:56:17,470
example to demonstrate the difference from the Laplace approximation but you

959
00:56:17,470 --> 00:56:20,589
from the Laplace approximation but you may wonder is there any action the

960
00:56:20,589 --> 00:56:21,940
may wonder is there any action the organization of this to multivariate

961
00:56:21,940 --> 00:56:25,450
organization of this to multivariate distribution can I get the variation of

962
00:56:25,450 --> 00:56:27,880
distribution can I get the variation of expression of the Gaussian and it comes

963
00:56:27,880 --> 00:56:30,309
expression of the Gaussian and it comes apart so if you do the algebra and I

964
00:56:30,309 --> 00:56:33,880
apart so if you do the algebra and I have everything here and I know if you

965
00:56:33,880 --> 00:56:35,530
have everything here and I know if you are not gonna bother to look at it but

966
00:56:35,530 --> 00:56:38,500
are not gonna bother to look at it but somebody bother to type it so out of

967
00:56:38,500 --> 00:56:40,089
somebody bother to type it so out of respect for him you may want to actually

968
00:56:40,089 --> 00:56:43,150
respect for him you may want to actually read it all right because there is

969
00:56:43,150 --> 00:56:46,690
read it all right because there is pretty much to learn you know there is a

970
00:56:46,690 --> 00:56:49,480
pretty much to learn you know there is a little trick that you do suspender trick

971
00:56:49,480 --> 00:56:53,380
little trick that you do suspender trick in machine learning with with this term

972
00:56:53,380 --> 00:56:56,500
in machine learning with with this term here I am NOT going to discuss it since

973
00:56:56,500 --> 00:56:58,180
here I am NOT going to discuss it since you need to take derivatives with

974
00:56:58,180 --> 00:57:00,280
you need to take derivatives with respect to me here in Sigma to find the

975
00:57:00,280 --> 00:57:02,829
respect to me here in Sigma to find the optimal Q of X you need to know how to

976
00:57:02,829 --> 00:57:04,809
optimal Q of X you need to know how to do this derivatives but here's the

977
00:57:04,809 --> 00:57:08,589
do this derivatives but here's the answer the answer comes to be that the

978
00:57:08,589 --> 00:57:10,750
answer the answer comes to be that the mean in the variation force ratio is

979
00:57:10,750 --> 00:57:14,650
mean in the variation force ratio is naturally the expectation of X and the

980
00:57:14,650 --> 00:57:17,049
naturally the expectation of X and the violence is actually the covariance of X

981
00:57:17,049 --> 00:57:21,190
violence is actually the covariance of X okay so Sigma is exactly the covariance

982
00:57:21,190 --> 00:57:24,460
okay so Sigma is exactly the covariance of X so really there is no factorization

983
00:57:24,460 --> 00:57:26,770
of X so really there is no factorization here right so this is a multivariate

984
00:57:26,770 --> 00:57:30,670
here right so this is a multivariate Gaussian okay so if you want to do this

985
00:57:30,670 --> 00:57:33,609
Gaussian okay so if you want to do this you can use variational methods this is

986
00:57:33,609 --> 00:57:36,099
you can use variational methods this is not the result is very much different

987
00:57:36,099 --> 00:57:37,809
not the result is very much different from the Laplace approximation

988
00:57:37,809 --> 00:57:43,069
from the Laplace approximation okay very much different

989
00:57:43,069 --> 00:57:43,079


990
00:57:43,079 --> 00:57:50,329
can you generalize this metal to sort of different type of distributions because

991
00:57:50,329 --> 00:57:54,750
different type of distributions because kill distance you know not typically

992
00:57:54,750 --> 00:57:58,349
kill distance you know not typically decides it I remember you know I once

993
00:57:58,349 --> 00:58:00,540
decides it I remember you know I once upon a time I gave a seminar in a

994
00:58:00,540 --> 00:58:02,069
upon a time I gave a seminar in a British University and one of my

995
00:58:02,069 --> 00:58:03,660
British University and one of my colleagues decided you know it's very

996
00:58:03,660 --> 00:58:08,579
colleagues decided you know it's very bad and not a good distance and since

997
00:58:08,579 --> 00:58:10,859
bad and not a good distance and since then she owned 25 papers using the km

998
00:58:10,859 --> 00:58:13,050
then she owned 25 papers using the km distance for some trivial 1d problems

999
00:58:13,050 --> 00:58:16,740
distance for some trivial 1d problems hopefully and so lots of people

1000
00:58:16,740 --> 00:58:19,260
hopefully and so lots of people including ourselves right we use the km

1001
00:58:19,260 --> 00:58:22,710
including ourselves right we use the km distance because convenient we know

1002
00:58:22,710 --> 00:58:25,310
distance because convenient we know everybody knows the deficient this ok

1003
00:58:25,310 --> 00:58:31,680
everybody knows the deficient this ok but it's all about you know mathematical

1004
00:58:31,680 --> 00:58:36,000
but it's all about you know mathematical simplicity and knowing exactly when

1005
00:58:36,000 --> 00:58:37,380
simplicity and knowing exactly when things work and when things don't work

1006
00:58:37,380 --> 00:58:40,500
things work and when things don't work ok so you can internalize this K of

1007
00:58:40,500 --> 00:58:44,910
ok so you can internalize this K of distance to what's called the family of

1008
00:58:44,910 --> 00:58:46,349
distance to what's called the family of divergence okay

1009
00:58:46,349 --> 00:58:50,700
divergence okay and this is the definition of this

1010
00:58:50,700 --> 00:58:55,500
and this is the definition of this distance with parameter right it looks

1011
00:58:55,500 --> 00:58:57,900
distance with parameter right it looks very much different from the KL distant

1012
00:58:57,900 --> 00:59:00,990
very much different from the KL distant okay so this problem is alpha is 2 minus

1013
00:59:00,990 --> 00:59:05,730
okay so this problem is alpha is 2 minus R into infinity and it is obviously

1014
00:59:05,730 --> 00:59:09,030
R into infinity and it is obviously greater or equal to 0 equal to 0 when P

1015
00:59:09,030 --> 00:59:11,910
greater or equal to 0 equal to 0 when P and Q are equal if you want to actually

1016
00:59:11,910 --> 00:59:16,380
and Q are equal if you want to actually read everything about calculations with

1017
00:59:16,380 --> 00:59:19,980
read everything about calculations with this type of alpha distances memcache is

1018
00:59:19,980 --> 00:59:22,890
this type of alpha distances memcache is a very nice report okay that you can

1019
00:59:22,890 --> 00:59:25,349
a very nice report okay that you can download from Microsoft website with all

1020
00:59:25,349 --> 00:59:27,270
download from Microsoft website with all the derivations are in the appendices so

1021
00:59:27,270 --> 00:59:31,220
the derivations are in the appendices so you can still have a very interesting

1022
00:59:31,220 --> 00:59:35,430
you can still have a very interesting type of extensions of the K of distance

1023
00:59:35,430 --> 00:59:39,930
type of extensions of the K of distance so the let me see if I need to tell you

1024
00:59:39,930 --> 00:59:49,010
so the let me see if I need to tell you anything here

1025
00:59:49,010 --> 00:59:49,020


1026
00:59:49,020 --> 00:59:57,350
so basically we're having the next two slides is to show you some limiting

1027
00:59:57,350 --> 01:00:02,840
slides is to show you some limiting values of this alpha distances when

1028
01:00:02,840 --> 01:00:06,200
values of this alpha distances when alpha takes various doors to one

1029
01:00:06,200 --> 01:00:08,060
alpha takes various doors to one basically what exactly you are getting

1030
01:00:08,060 --> 01:00:13,910
basically what exactly you are getting and so if you define this coefficient 1

1031
01:00:13,910 --> 01:00:16,370
and so if you define this coefficient 1 plus alpha over the fewest gamma P and

1032
01:00:16,370 --> 01:00:19,520
plus alpha over the fewest gamma P and this coefficient as gamma Q arise the

1033
01:00:19,520 --> 01:00:21,260
this coefficient as gamma Q arise the exponent of the Judis to be the Q

1034
01:00:21,260 --> 01:00:25,040
exponent of the Judis to be the Q distribution obviously gamma P a gamma Q

1035
01:00:25,040 --> 01:00:28,540
distribution obviously gamma P a gamma Q equal to 1 so if you take the limit at

1036
01:00:28,540 --> 01:00:32,450
equal to 1 so if you take the limit at this gamez when alpha goes to 1 you can

1037
01:00:32,450 --> 01:00:36,290
this gamez when alpha goes to 1 you can see this goes to zero this goes to 1 and

1038
01:00:36,290 --> 01:00:40,070
see this goes to zero this goes to 1 and what you can do is actually you can take

1039
01:00:40,070 --> 01:00:42,230
what you can do is actually you can take these powers in the definition of this

1040
01:00:42,230 --> 01:00:45,530
these powers in the definition of this alpha distances and use first of all the

1041
01:00:45,530 --> 01:00:48,859
alpha distances and use first of all the Taylor expansions the results are

1042
01:00:48,859 --> 01:00:50,330
Taylor expansions the results are actually previous I'm not gonna go

1043
01:00:50,330 --> 01:00:53,420
actually previous I'm not gonna go through everything here okay so for

1044
01:00:53,420 --> 01:00:57,890
through everything here okay so for example if you take the distribution Q

1045
01:00:57,890 --> 01:01:03,200
example if you take the distribution Q to gamma Q you can write this as the

1046
01:01:03,200 --> 01:01:06,530
to gamma Q you can write this as the exponential of gamma Q log of Q and then

1047
01:01:06,530 --> 01:01:10,609
exponential of gamma Q log of Q and then you can use the turtle data since its

1048
01:01:10,609 --> 01:01:13,099
you can use the turtle data since its function in gamma cubed because gamma Q

1049
01:01:13,099 --> 01:01:16,370
function in gamma cubed because gamma Q goes to 0 as alpha goes to 1 so you can

1050
01:01:16,370 --> 01:01:19,790
goes to 0 as alpha goes to 1 so you can write this as exponential is 1 plus X

1051
01:01:19,790 --> 01:01:22,130
write this as exponential is 1 plus X plus x squared over 2 so the first term

1052
01:01:22,130 --> 01:01:25,520
plus x squared over 2 so the first term is 1 plus gamma log of Q this is to

1053
01:01:25,520 --> 01:01:27,440
is 1 plus gamma log of Q this is to limit what Ricky but it's nothing major

1054
01:01:27,440 --> 01:01:29,960
limit what Ricky but it's nothing major you can approximate this B minus gamma Q

1055
01:01:29,960 --> 01:01:32,870
you can approximate this B minus gamma Q P of log of P and it comes out that this

1056
01:01:32,870 --> 01:01:38,150
P of log of P and it comes out that this integral it gives you a very nice form

1057
01:01:38,150 --> 01:01:41,349
integral it gives you a very nice form that maybe some of you have seen it in

1058
01:01:41,349 --> 01:01:44,090
that maybe some of you have seen it in different contexts in machine learning

1059
01:01:44,090 --> 01:01:49,250
different contexts in machine learning so if you take this asymptotic value and

1060
01:01:49,250 --> 01:01:52,430
so if you take this asymptotic value and we plug them in there if you get

1061
01:01:52,430 --> 01:01:54,800
we plug them in there if you get basically you recover from the self of

1062
01:01:54,800 --> 01:01:56,840
basically you recover from the self of distances the KL Vista

1063
01:01:56,840 --> 01:02:00,800
distances the KL Vista okay so as a matter fact when alpha we

1064
01:02:00,800 --> 01:02:02,390
okay so as a matter fact when alpha we set out from goes to z/2 on right

1065
01:02:02,390 --> 01:02:05,090
set out from goes to z/2 on right without reverse to 1 this becomes equal

1066
01:02:05,090 --> 01:02:07,460
without reverse to 1 this becomes equal to answer you get the K of distance okay

1067
01:02:07,460 --> 01:02:11,480
to answer you get the K of distance okay and so this has some limiting values

1068
01:02:11,480 --> 01:02:14,030
and so this has some limiting values okay so when alpha worst one you get the

1069
01:02:14,030 --> 01:02:17,210
okay so when alpha worst one you get the KL distance and actually here yeah so

1070
01:02:17,210 --> 01:02:19,250
KL distance and actually here yeah so this correct and you can get the reverse

1071
01:02:19,250 --> 01:02:20,570
this correct and you can get the reverse key and distance when alpha goes to

1072
01:02:20,570 --> 01:02:24,950
key and distance when alpha goes to minus 1 in even further you can get out

1073
01:02:24,950 --> 01:02:25,790
minus 1 in even further you can get out of this

1074
01:02:25,790 --> 01:02:28,970
of this even the Shannon give distance but some

1075
01:02:28,970 --> 01:02:31,610
even the Shannon give distance but some people in network theory goes right so

1076
01:02:31,610 --> 01:02:34,790
people in network theory goes right so this is recovered by taking after equal

1077
01:02:34,790 --> 01:02:44,690
this is recovered by taking after equal to 0

1078
01:02:44,690 --> 01:02:44,700


1079
01:02:44,700 --> 01:02:55,370
let me see if I am going to tell you brothers

1080
01:02:55,370 --> 01:02:55,380


1081
01:02:55,380 --> 01:03:03,080
right so let's say okay let me see how we're going to define this problem so in

1082
01:03:03,080 --> 01:03:05,000
we're going to define this problem so in everything we did like one will apply

1083
01:03:05,000 --> 01:03:07,430
everything we did like one will apply expectation maximization right do you

1084
01:03:07,430 --> 01:03:12,260
expectation maximization right do you remember we had a set of latent variable

1085
01:03:12,260 --> 01:03:14,380
remember we had a set of latent variable Z but also we have a set of parameters

1086
01:03:14,380 --> 01:03:18,380
Z but also we have a set of parameters okay and in the expectation step what we

1087
01:03:18,380 --> 01:03:22,220
okay and in the expectation step what we did is we computed the efficient

1088
01:03:22,220 --> 01:03:24,730
did is we computed the efficient statistics that they were effectively

1089
01:03:24,730 --> 01:03:26,690
statistics that they were effectively expectations with respect to the Z

1090
01:03:26,690 --> 01:03:29,090
expectations with respect to the Z variable for fixed parameters and then

1091
01:03:29,090 --> 01:03:31,460
variable for fixed parameters and then we since we completed the sufficient

1092
01:03:31,460 --> 01:03:41,360
we since we completed the sufficient statistics then we much mize all right

1093
01:03:41,360 --> 01:03:44,360
statistics then we much mize all right so here can I do a body astral

1094
01:03:44,360 --> 01:03:46,490
so here can I do a body astral approximation what is not going to give

1095
01:03:46,490 --> 01:03:49,370
approximation what is not going to give me when the Joint Distribution of the

1096
01:03:49,370 --> 01:03:51,020
me when the Joint Distribution of the unknown Nathan variables Z and the

1097
01:03:51,020 --> 01:03:53,870
unknown Nathan variables Z and the parameters theta fertilizes has what you

1098
01:03:53,870 --> 01:03:56,660
parameters theta fertilizes has what you see here I should have put there cure

1099
01:03:56,660 --> 01:04:01,070
see here I should have put there cure see that comma theta all right now if

1100
01:04:01,070 --> 01:04:03,350
see that comma theta all right now if you want to keep the the parameters

1101
01:04:03,350 --> 01:04:06,530
you want to keep the the parameters theta to a fixed value all right and you

1102
01:04:06,530 --> 01:04:08,630
theta to a fixed value all right and you don't want to bother basically bistro3

1103
01:04:08,630 --> 01:04:12,520
don't want to bother basically bistro3 that becomes a delta function okay and

1104
01:04:12,520 --> 01:04:16,490
that becomes a delta function okay and so let's see what case this is

1105
01:04:16,490 --> 01:04:22,400
so let's see what case this is considered here so the maximization of

1106
01:04:22,400 --> 01:04:24,980
considered here so the maximization of the lower bound is the same as the

1107
01:04:24,980 --> 01:04:27,020
the lower bound is the same as the minimization of the distance between qlp

1108
01:04:27,020 --> 01:04:29,960
minimization of the distance between qlp so the distance between Q and P is

1109
01:04:29,960 --> 01:04:34,070
so the distance between Q and P is written minus Q of theta Q of Z this the

1110
01:04:34,070 --> 01:04:35,870
written minus Q of theta Q of Z this the authorization I use this is the log of

1111
01:04:35,870 --> 01:04:39,200
authorization I use this is the log of joint log likelihood you have Z Q theta

1112
01:04:39,200 --> 01:04:50,000
joint log likelihood you have Z Q theta just the definition okay so anyways if I

1113
01:04:50,000 --> 01:04:52,100
just the definition okay so anyways if I say let's fix the parameters the value

1114
01:04:52,100 --> 01:04:57,250
say let's fix the parameters the value theta 0 expectation step all right so

1115
01:04:57,250 --> 01:04:59,080
theta 0 expectation step all right so actually does the expectations that has

1116
01:04:59,080 --> 01:05:01,360
actually does the expectations that has anything to do with wanna show that

1117
01:05:01,360 --> 01:05:04,300
anything to do with wanna show that right so we're going to keep paper

1118
01:05:04,300 --> 01:05:17,200
right so we're going to keep paper tricks to 0 so the calculation that

1119
01:05:17,200 --> 01:05:19,330
tricks to 0 so the calculation that involves basically the numerator there

1120
01:05:19,330 --> 01:05:22,960
involves basically the numerator there all right theta is the Delta function so

1121
01:05:22,960 --> 01:05:25,800
all right theta is the Delta function so you believe that I'm going to get their

1122
01:05:25,800 --> 01:05:29,070
you believe that I'm going to get their number physique of P of Z comma theta 0

1123
01:05:29,070 --> 01:05:33,340
number physique of P of Z comma theta 0 all right but basically we think that we

1124
01:05:33,340 --> 01:05:35,020
all right but basically we think that we don't really care because I am looking

1125
01:05:35,020 --> 01:05:38,560
don't really care because I am looking down on 0 date 3 so out here minus Q of

1126
01:05:38,560 --> 01:05:45,550
down on 0 date 3 so out here minus Q of Z log of p 0 0 so minus P of Z ok some

1127
01:05:45,550 --> 01:05:47,290
Z log of p 0 0 so minus P of Z ok some thinking

1128
01:05:47,290 --> 01:05:50,590
thinking here is the join little Hydra put this

1129
01:05:50,590 --> 01:05:52,960
here is the join little Hydra put this is what we use in expectation

1130
01:05:52,960 --> 01:05:55,270
is what we use in expectation maximization and I like this as the

1131
01:05:55,270 --> 01:05:58,270
maximization and I like this as the posterior of Z given the parameters

1132
01:05:58,270 --> 01:06:01,750
posterior of Z given the parameters times the probability of theta 0 the

1133
01:06:01,750 --> 01:06:04,500
times the probability of theta 0 the posterior of theta 0 basically

1134
01:06:04,500 --> 01:06:09,340
posterior of theta 0 basically okay so he just doesn't matter for this

1135
01:06:09,340 --> 01:06:12,430
okay so he just doesn't matter for this I get rid of it so minus Q of Z log of

1136
01:06:12,430 --> 01:06:18,340
I get rid of it so minus Q of Z log of this divided by Q C ok and which is the

1137
01:06:18,340 --> 01:06:21,970
this divided by Q C ok and which is the the minimization so we need to minimize

1138
01:06:21,970 --> 01:06:24,310
the minimization so we need to minimize K this K of distance all right

1139
01:06:24,310 --> 01:06:26,620
K this K of distance all right all right so when is the minimization

1140
01:06:26,620 --> 01:06:30,610
all right so when is the minimization happening when Q of Z equal to P of Z

1141
01:06:30,610 --> 01:06:34,240
happening when Q of Z equal to P of Z given theta 0 comma X this is what

1142
01:06:34,240 --> 01:06:36,130
given theta 0 comma X this is what actually we did from the expectation

1143
01:06:36,130 --> 01:06:38,740
actually we did from the expectation step right we fix the parameters all

1144
01:06:38,740 --> 01:06:40,900
step right we fix the parameters all right and you remember what we did this

1145
01:06:40,900 --> 01:06:43,450
right and you remember what we did this we computed the average of the join

1146
01:06:43,450 --> 01:06:47,730
we computed the average of the join locally with respect to the distribution

1147
01:06:47,730 --> 01:06:47,740


1148
01:06:47,740 --> 01:06:53,230
what was the distribution and then step that we averaged with because the

1149
01:06:53,230 --> 01:07:00,850
that we averaged with because the custodian of Z given the parameter ETA

1150
01:07:00,850 --> 01:07:01,849
custodian of Z given the parameter ETA is the first

1151
01:07:01,849 --> 01:07:05,809
is the first she given because 0 so this step

1152
01:07:05,809 --> 01:07:08,359
she given because 0 so this step actually does come from it's the small

1153
01:07:08,359 --> 01:07:11,890
actually does come from it's the small part of the variational methods okay

1154
01:07:11,890 --> 01:07:15,440
part of the variational methods okay then you may ask I have a maximization

1155
01:07:15,440 --> 01:07:21,920
then you may ask I have a maximization step well so so maybe now Q of Z equal

1156
01:07:21,920 --> 01:07:23,599
step well so so maybe now Q of Z equal to that and we want to compute Q theta

1157
01:07:23,599 --> 01:07:28,059
to that and we want to compute Q theta shall we go back to again to the right

1158
01:07:28,059 --> 01:07:31,279
shall we go back to again to the right okay so we remember if this Q theta Q of

1159
01:07:31,279 --> 01:07:36,130
okay so we remember if this Q theta Q of Z the giant everything data and announce

1160
01:07:36,130 --> 01:07:41,209
Z the giant everything data and announce okay and in this case remember the this

1161
01:07:41,209 --> 01:07:45,229
okay and in this case remember the this fixed now okay so I put cubes inside and

1162
01:07:45,229 --> 01:07:50,979
fixed now okay so I put cubes inside and with a latest this 10 times that is the

1163
01:07:50,979 --> 01:07:53,900
with a latest this 10 times that is the expectation with respect to Q of Zeta of

1164
01:07:53,900 --> 01:07:58,029
expectation with respect to Q of Zeta of the joining of livelihood all right okay

1165
01:07:58,029 --> 01:08:01,309
the joining of livelihood all right okay and I have Q then I have an average with

1166
01:08:01,309 --> 01:08:03,259
and I have Q then I have an average with respect to Q theta and then of the

1167
01:08:03,259 --> 01:08:04,699
respect to Q theta and then of the denominator basically I'm going to get

1168
01:08:04,699 --> 01:08:07,609
denominator basically I'm going to get the entropy of the distribution theta

1169
01:08:07,609 --> 01:08:13,849
the entropy of the distribution theta and how do you find how do you update to

1170
01:08:13,849 --> 01:08:18,070
and how do you find how do you update to theta you basically need to maximize

1171
01:08:18,070 --> 01:08:21,740
theta you basically need to maximize look at this right how are you going to

1172
01:08:21,740 --> 01:08:28,709
look at this right how are you going to maximize this

1173
01:08:28,709 --> 01:08:28,719


1174
01:08:28,719 --> 01:08:34,280
you're verifying the thickness no overmuch Mises the joined up likelihood

1175
01:08:34,280 --> 01:08:38,070
overmuch Mises the joined up likelihood and actually yeah I was going to

1176
01:08:38,070 --> 01:08:40,979
and actually yeah I was going to thinking they're just having a

1177
01:08:40,979 --> 01:08:43,229
thinking they're just having a genetically Delavan theta zero for the

1178
01:08:43,229 --> 01:08:45,630
genetically Delavan theta zero for the suppose we have to take we update theta

1179
01:08:45,630 --> 01:08:48,329
suppose we have to take we update theta 0 here so this is their step

1180
01:08:48,329 --> 01:08:51,030
0 here so this is their step alright you only have computing the

1181
01:08:51,030 --> 01:08:53,430
alright you only have computing the sufficient statistics with respect to

1182
01:08:53,430 --> 01:08:55,769
sufficient statistics with respect to the distribution cubes if we complete it

1183
01:08:55,769 --> 01:08:57,720
the distribution cubes if we complete it and then you maximize the joint of

1184
01:08:57,720 --> 01:08:59,280
and then you maximize the joint of likelihood with respect to the

1185
01:08:59,280 --> 01:09:02,820
likelihood with respect to the parameters bottom line is you know when

1186
01:09:02,820 --> 01:09:06,720
parameters bottom line is you know when you applied this term this variation

1187
01:09:06,720 --> 01:09:09,390
you applied this term this variation electron from the distribution that

1188
01:09:09,390 --> 01:09:11,400
electron from the distribution that factorizes the latent variables and the

1189
01:09:11,400 --> 01:09:13,740
factorizes the latent variables and the parameters like that basically you

1190
01:09:13,740 --> 01:09:17,160
parameters like that basically you discover as you have a Z first and then

1191
01:09:17,160 --> 01:09:19,890
discover as you have a Z first and then you update theta you discover the E&amp;M

1192
01:09:19,890 --> 01:09:21,990
you update theta you discover the E&amp;M steps of the expectation maximization

1193
01:09:21,990 --> 01:09:26,039
steps of the expectation maximization algorithm as a matter of fact they am

1194
01:09:26,039 --> 01:09:27,930
algorithm as a matter of fact they am aware if I remember historically was

1195
01:09:27,930 --> 01:09:31,289
aware if I remember historically was developed way because people must in

1196
01:09:31,289 --> 01:09:33,599
developed way because people must in have Inc have it using the variational

1197
01:09:33,599 --> 01:09:36,170
have Inc have it using the variational methods but now you can see sort of the

1198
01:09:36,170 --> 01:09:39,660
methods but now you can see sort of the TM algorithm is really the variational

1199
01:09:39,660 --> 01:09:55,500
TM algorithm is really the variational approximation substance okay you missed

1200
01:09:55,500 --> 01:09:58,110
approximation substance okay you missed and maybe some of you have not seen this

1201
01:09:58,110 --> 01:10:02,640
and maybe some of you have not seen this example but please and I think we gave

1202
01:10:02,640 --> 01:10:04,290
example but please and I think we gave you did we give them a homework on this

1203
01:10:04,290 --> 01:10:07,350
you did we give them a homework on this this semester all right so you have seen

1204
01:10:07,350 --> 01:10:09,630
this semester all right so you have seen it okay so this is the denoising of an

1205
01:10:09,630 --> 01:10:12,270
it okay so this is the denoising of an image problem well you can think of this

1206
01:10:12,270 --> 01:10:15,090
image problem well you can think of this being the pixels in an image this of the

1207
01:10:15,090 --> 01:10:18,180
being the pixels in an image this of the noisy bodies an extra you know the

1208
01:10:18,180 --> 01:10:20,280
noisy bodies an extra you know the original image but here we take it to be

1209
01:10:20,280 --> 01:10:23,040
original image but here we take it to be a binary black or white okay plus one or

1210
01:10:23,040 --> 01:10:27,870
a binary black or white okay plus one or minus one okay

1211
01:10:27,870 --> 01:10:27,880


1212
01:10:27,880 --> 01:10:32,430
the joint distributions of X Y is protects

1213
01:10:32,430 --> 01:10:35,670
protects times the likelihood of

1214
01:10:35,670 --> 01:10:35,680


1215
01:10:35,680 --> 01:10:41,910
P of Y given X okay so I am going to do some manipulation of the algebra okay

1216
01:10:41,910 --> 01:10:44,730
some manipulation of the algebra okay just to make things to generalize things

1217
01:10:44,730 --> 01:10:46,890
just to make things to generalize things so the first thing I'm going to assume

1218
01:10:46,890 --> 01:10:49,800
so the first thing I'm going to assume the prior so we're going to compute X

1219
01:10:49,800 --> 01:10:51,270
the prior so we're going to compute X right we want to compute what the

1220
01:10:51,270 --> 01:10:53,520
right we want to compute what the original image this if you have some

1221
01:10:53,520 --> 01:10:56,850
original image this if you have some noisy red surveillance most web

1222
01:10:56,850 --> 01:10:59,670
noisy red surveillance most web denoising is so the primary are X I'm

1223
01:10:59,670 --> 01:11:03,290
denoising is so the primary are X I'm going to assume it will have an

1224
01:11:03,290 --> 01:11:05,430
going to assume it will have an exponential prominent distribution would

1225
01:11:05,430 --> 01:11:09,570
exponential prominent distribution would be e to the minus some energy and this

1226
01:11:09,570 --> 01:11:11,930
be e to the minus some energy and this energy functional I write it like that

1227
01:11:11,930 --> 01:11:15,720
energy functional I write it like that where I account

1228
01:11:15,720 --> 01:11:18,750
where I account interactions between neighbors so this

1229
01:11:18,750 --> 01:11:21,000
interactions between neighbors so this is sort of a journal is a nesting model

1230
01:11:21,000 --> 01:11:24,530
is sort of a journal is a nesting model okay so I put some weights between

1231
01:11:24,530 --> 01:11:27,990
okay so I put some weights between pixels I and J but the only account for

1232
01:11:27,990 --> 01:11:30,540
pixels I and J but the only account for this interaction science a if I and J is

1233
01:11:30,540 --> 01:11:36,690
this interaction science a if I and J is unable okay so a business my so this is

1234
01:11:36,690 --> 01:11:40,700
unable okay so a business my so this is a very standard way you know to

1235
01:11:40,700 --> 01:11:45,750
a very standard way you know to introduce basically the dependencies

1236
01:11:45,750 --> 01:11:50,160
introduce basically the dependencies between the X pixels okay so again there

1237
01:11:50,160 --> 01:11:52,740
between the X pixels okay so again there is a regional writing uses into the - II

1238
01:11:52,740 --> 01:11:55,680
is a regional writing uses into the - II like it gives distribution and this is

1239
01:11:55,680 --> 01:11:58,230
like it gives distribution and this is my energy and let's see what the

1240
01:11:58,230 --> 01:12:00,210
my energy and let's see what the likelihood is going to be like so the

1241
01:12:00,210 --> 01:12:02,280
likelihood is going to be like so the likelihood is a progress of x for its I

1242
01:12:02,280 --> 01:12:04,200
likelihood is a progress of x for its I right so it is the probability of Y

1243
01:12:04,200 --> 01:12:07,080
right so it is the probability of Y given X I so you can think of this sort

1244
01:12:07,080 --> 01:12:10,110
given X I so you can think of this sort of being a Gaussian with some describe

1245
01:12:10,110 --> 01:12:14,340
of being a Gaussian with some describe noise so I'm gonna do is I'm gonna write

1246
01:12:14,340 --> 01:12:17,940
noise so I'm gonna do is I'm gonna write each of these terms to follow this

1247
01:12:17,940 --> 01:12:22,320
each of these terms to follow this format is into some energy life all

1248
01:12:22,320 --> 01:12:22,500
format is into some energy life all right

1249
01:12:22,500 --> 01:12:24,390
right so because it's products of

1250
01:12:24,390 --> 01:12:26,660
so because it's products of Exponential's then I'm going to get the

1251
01:12:26,660 --> 01:12:28,770
Exponential's then I'm going to get the expectation of the summation of all the

1252
01:12:28,770 --> 01:12:32,970
expectation of the summation of all the alarms and this allows can be Gaussian

1253
01:12:32,970 --> 01:12:37,470
alarms and this allows can be Gaussian right with some noise step okay all

1254
01:12:37,470 --> 01:12:38,550
right with some noise step okay all right

1255
01:12:38,550 --> 01:12:40,200
right easy

1256
01:12:40,200 --> 01:12:44,820
easy if I compute the joint you know if you

1257
01:12:44,820 --> 01:12:47,940
if I compute the joint you know if you forgive the organization or the the

1258
01:12:47,940 --> 01:12:51,450
forgive the organization or the the posterior of X given Y I get e to the

1259
01:12:51,450 --> 01:12:55,680
posterior of X given Y I get e to the minus the energy equal the energy is e 0

1260
01:12:55,680 --> 01:13:00,900
minus the energy equal the energy is e 0 minus restoration of audiologic ok so my

1261
01:13:00,900 --> 01:13:05,310
minus restoration of audiologic ok so my energy in my giant log likelihood this

1262
01:13:05,310 --> 01:13:06,750
energy in my giant log likelihood this is what we need from the variational

1263
01:13:06,750 --> 01:13:18,560
is what we need from the variational methods is e to the minus this energy so

1264
01:13:18,560 --> 01:13:21,360
methods is e to the minus this energy so multinational approximation now because

1265
01:13:21,360 --> 01:13:57,320
multinational approximation now because this looks complicated ok in this is the

1266
01:13:57,320 --> 01:13:57,330


1267
01:13:57,330 --> 01:14:37,080
because to simplify this calculation because an exponential I am going to

1268
01:14:37,080 --> 01:14:41,140
because an exponential I am going to define

1269
01:14:41,140 --> 01:14:41,150


1270
01:14:41,150 --> 01:14:47,200
so I do not put the rice here right but this is you can think of this is the

1271
01:14:47,200 --> 01:14:50,830
this is you can think of this is the giant okay this is what it is okay so

1272
01:14:50,830 --> 01:14:56,770
giant okay this is what it is okay so the giant under that liquid is so the

1273
01:14:56,770 --> 01:14:58,780
the giant under that liquid is so the Journal article will disconnect this

1274
01:14:58,780 --> 01:15:03,330
Journal article will disconnect this exponential to disappear and it will be

1275
01:15:03,330 --> 01:15:08,399
exponential to disappear and it will be there's a minus sign there so it will be

1276
01:15:08,399 --> 01:15:12,160
there's a minus sign there so it will be X I times the summation over the

1277
01:15:12,160 --> 01:15:15,820
X I times the summation over the neighborhoods of Na VI j xj all right

1278
01:15:15,820 --> 01:15:20,410
neighborhoods of Na VI j xj all right plus Li of X I this person's so ravenous

1279
01:15:20,410 --> 01:15:23,830
plus Li of X I this person's so ravenous I am writing the joint of liquid but

1280
01:15:23,830 --> 01:15:25,919
I am writing the joint of liquid but because I'm interested only calculate

1281
01:15:25,919 --> 01:15:31,270
because I'm interested only calculate one factor at the time I am making the

1282
01:15:31,270 --> 01:15:34,919
one factor at the time I am making the terms that contain X I nothing else

1283
01:15:34,919 --> 01:15:37,959
terms that contain X I nothing else so there's many other terms like there

1284
01:15:37,959 --> 01:15:39,970
so there's many other terms like there is actual on here x all the neighbor

1285
01:15:39,970 --> 01:15:43,859
is actual on here x all the neighbor selector I only want to see X I because

1286
01:15:43,859 --> 01:15:45,820
selector I only want to see X I because effectively this is what I'm interested

1287
01:15:45,820 --> 01:15:50,050
effectively this is what I'm interested to to compute in in some iterative

1288
01:15:50,050 --> 01:15:52,660
to to compute in in some iterative manner so the jury will be happy who is

1289
01:15:52,660 --> 01:16:01,300
manner so the jury will be happy who is this okay so all right nice distribution

1290
01:16:01,300 --> 01:16:05,229
this okay so all right nice distribution is you know that the log of the optimal

1291
01:16:05,229 --> 01:16:11,950
is you know that the log of the optimal right is remember it is the expectation

1292
01:16:11,950 --> 01:16:16,810
right is remember it is the expectation with the cures and here is not all right

1293
01:16:16,810 --> 01:16:22,600
with the cures and here is not all right so the Qi is the expectation okay of X I

1294
01:16:22,600 --> 01:16:32,070
so the Qi is the expectation okay of X I times this where I substance that

1295
01:16:32,070 --> 01:16:32,080


1296
01:16:32,080 --> 01:16:39,459
because Qi is expectation of this disguise the exponential of the

1297
01:16:39,459 --> 01:16:42,339
disguise the exponential of the expectation of this and what is the

1298
01:16:42,339 --> 01:16:44,290
expectation of this and what is the expectation of X J because the binary

1299
01:16:44,290 --> 01:16:46,189
expectation of X J because the binary variable

1300
01:16:46,189 --> 01:16:49,219
variable it's men so actively in this trivial

1301
01:16:49,219 --> 01:16:55,729
it's men so actively in this trivial model - you can try to access to that

1302
01:16:55,729 --> 01:16:59,209
model - you can try to access to that average value very nice

1303
01:16:59,209 --> 01:17:02,509
average value very nice all right you know extremely is the

1304
01:17:02,509 --> 01:17:05,149
all right you know extremely is the completion so you don't really have to

1305
01:17:05,149 --> 01:17:07,489
completion so you don't really have to calculate any sufficient statistics and

1306
01:17:07,489 --> 01:17:12,529
calculate any sufficient statistics and complicated expectations you just fix XA

1307
01:17:12,529 --> 01:17:14,449
complicated expectations you just fix XA with the mean right because you need to

1308
01:17:14,449 --> 01:17:16,399
with the mean right because you need to break expectations okay

1309
01:17:16,399 --> 01:17:23,180
break expectations okay and I want you to interests right but we

1310
01:17:23,180 --> 01:17:25,129
and I want you to interests right but we are you know we're calling the stems

1311
01:17:25,129 --> 01:17:30,529
are you know we're calling the stems here Mui okay and we define the values

1312
01:17:30,529 --> 01:17:35,629
here Mui okay and we define the values of shall I and I plus an ally - okay I'm

1313
01:17:35,629 --> 01:17:36,979
of shall I and I plus an ally - okay I'm gonna finish all right so I'm going to

1314
01:17:36,979 --> 01:17:38,239
gonna finish all right so I'm going to come back to this example

1315
01:17:38,239 --> 01:17:40,549
come back to this example so maybe sakali at the end of the day

1316
01:17:40,549 --> 01:17:44,270
so maybe sakali at the end of the day you can derive the UI to get a very nice

1317
01:17:44,270 --> 01:17:48,140
you can derive the UI to get a very nice cloth solution okay - solution so mr.

1318
01:17:48,140 --> 01:17:51,680
cloth solution okay - solution so mr. right around would be one line and if

1319
01:17:51,680 --> 01:17:53,739
right around would be one line and if you apply the mean field approximation

1320
01:17:53,739 --> 01:17:57,699
you apply the mean field approximation in a self consistent manner few images

1321
01:17:57,699 --> 01:18:02,059
in a self consistent manner few images you can see you know with 

